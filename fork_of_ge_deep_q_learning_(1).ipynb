{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "fork-of-ge-deep-q-learning (1).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nicolas1805961/GE_reinforcement_learning/blob/master/fork_of_ge_deep_q_learning_(1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "93r3A2Gi_IAT"
      },
      "source": [
        "#import glob\n",
        "#import pydicom\n",
        "#import numpy as np\n",
        "#import matplotlib.pyplot as plt\n",
        "\n",
        "#patient_path = glob.glob('/kaggle/input/ge-dqn/Patients/*')[0]\n",
        "#files = []\n",
        "#for slice_path in glob.glob(patient_path + '/*'):\n",
        "#    files.append(pydicom.dcmread(slice_path))\n",
        "#files = sorted(files, key=lambda s: s.SliceLocation)\n",
        "#shape = files[0].pixel_array.shape\n",
        "#shape = list(shape)\n",
        "#shape.insert(0, len(files))\n",
        "#img3d = np.zeros(shape)\n",
        "#for i, s in enumerate(files):\n",
        "#    img2d = s.pixel_array\n",
        "#    img3d[i, :, :] = img2d\n",
        "#print(img3d.shape)\n",
        "#img = np.max(img3d, axis=0).T\n",
        "#img = np.rot90(img, 2)\n",
        "        \n",
        "#plt.imshow(img3d[:, shape[1] // 2, :], cmap='bone')"
      ],
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gd2rLckR_h0r",
        "outputId": "cb510983-db90-4e25-99b0-8a93a2d7f872",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "trusted": true,
        "id": "lAdezC5A_IAh"
      },
      "source": [
        "import math\n",
        "import random\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import namedtuple\n",
        "from itertools import count\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as T"
      ],
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "-nqa2nah_IAy",
        "outputId": "fa9b9bf0-1c0b-444a-c624-d6380995f525",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ],
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "oSLL4mnx_IBB"
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import pandas as pd\n",
        "from numpy.random import randint, choice\n",
        "from torchvision.transforms.functional import affine\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "data_train = pd.read_csv(\"/content/drive/My Drive/fashion-mnist_train.csv\")\n",
        "data_train = data_train.iloc[:50, 1:].values\n",
        "data_train = data_train.reshape((data_train.shape[0], 28, 28))\n",
        "p = (50 - 28) // 2\n",
        "data_train = np.pad(data_train, ((0, 0), (p, p), (p, p)), 'constant', constant_values=0)"
      ],
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "WeJP_C1f_IBN"
      },
      "source": [
        "data_test = pd.read_csv(\"/content/drive/My Drive/fashion-mnist_test.csv\")\n",
        "data_test = data_test.iloc[:50, 1:].values\n",
        "data_test = data_test.reshape((data_test.shape[0], 28, 28))\n",
        "p = (50 - 28) // 2\n",
        "data_test = np.pad(data_test, ((0, 0), (p, p), (p, p)), 'constant', constant_values=0)"
      ],
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "euDRlNVS_IBa"
      },
      "source": [
        "data = np.concatenate((data_train, data_test))"
      ],
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "pgKVUEzx_IBl"
      },
      "source": [
        "del data_train\n",
        "del data_test"
      ],
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "9PuC6eU0_IB-",
        "outputId": "9a058829-7731-4dce-adea-697d3ef0f0ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "data = data.astype(np.uint8)\n",
        "print(data.shape)"
      ],
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(100, 50, 50)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dpa81UQpbYQU",
        "outputId": "0146db89-df31-4aea-e237-383005fe7ae0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from skimage.measure import regionprops, label\n",
        "from skimage.filters import threshold_otsu\n",
        "\n",
        "image_center = np.array([25, 25])\n",
        "offset = np.zeros((data.shape[0], 2))\n",
        "test = data.copy()\n",
        "for i in range(data.shape[0]):\n",
        "  binary = (test[i, ...] > 20).astype(np.uint8)\n",
        "  centroid = np.array(regionprops(binary)[0].centroid)\n",
        "  offset[i] = np.rint(image_center - centroid)[::-1]\n",
        "  im = Image.fromarray(test[i, ...].astype(np.uint8))\n",
        "  data[i, ...] = np.array(affine(im, angle=0, translate=list(offset[i]), scale=1, shear=0))\n",
        "\n",
        "fig, ax = plt.subplots(4, 2, figsize=(24, 9*4))\n",
        "for i in range(4):\n",
        "    n = randint(0, data.shape[0])\n",
        "    print(offset[n])\n",
        "    ax[i, 0].imshow(test[n,...], cmap='gray')\n",
        "    ax[i, 1].imshow(data[n,...], cmap='gray')\n",
        "  \n",
        "del offset, test, image_center"
      ],
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 0. -0.]\n",
            "[ 0. -0.]\n",
            "[0. 0.]\n",
            "[ 1. -0.]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1728x2592 with 8 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "LPwSbs8C_ICU"
      },
      "source": [
        "from skimage.measure import label, regionprops, approximate_polygon, find_contours\n",
        "\n",
        "def get_perimeter(image):\n",
        "    img = image.copy()\n",
        "    img[img > 0] = 255\n",
        "    perimeter = 0\n",
        "    contour = sorted(find_contours(img, 0), key=lambda x: x.shape[0])[-1]\n",
        "    perimeter = approximate_polygon(contour, tolerance=0)\n",
        "    return perimeter"
      ],
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "O_UpBhs4_ICg"
      },
      "source": [
        "def rotate_image(image, angle, image_center):\n",
        "  rot_mat = cv2.getRotationMatrix2D(image_center, angle, 1.0)\n",
        "  result = cv2.warpAffine(image, rot_mat, image.shape[1::-1], flags=cv2.INTER_LINEAR)\n",
        "  return result"
      ],
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "-883XOG4_ICw"
      },
      "source": [
        "def translate_image(image, x, y):\n",
        "    translation_matrix = np.float32([ [1,0,x], [0,1,y] ])\n",
        "    img_translation = cv2.warpAffine(image, translation_matrix, image.shape[1::-1])\n",
        "    return img_translation"
      ],
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "kaddl-OB_IC7"
      },
      "source": [
        "from numpy.random import choice\n",
        "\n",
        "def get_rotation_number(r1=None):\n",
        "    r = 0\n",
        "    if r1 is not None:\n",
        "      while r == 0 or abs(r1 - r) > 50 or abs(r1 - r) == 0:\n",
        "        r = choice(np.arange(start=-50, stop=51, step=10))\n",
        "    else:\n",
        "      while r == 0:\n",
        "        r = choice(np.arange(start=-50, stop=51, step=10))\n",
        "    return r"
      ],
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pilT14IQEdw9"
      },
      "source": [
        "from numpy.random import choice\n",
        "\n",
        "def get_rotation_number2(t=None, r1=None):\n",
        "    r = 0\n",
        "    if r1 is not None and t is not None:\n",
        "      r = r1 - t\n",
        "    else:\n",
        "      while abs(r) < 10:\n",
        "        r = np.random.randint(-50, 50)\n",
        "    return r"
      ],
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "pwM6Qpcj_IDG",
        "outputId": "a53abce1-51a4-415c-da73-6d8a0fc98c15",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 154
        }
      },
      "source": [
        "from numpy.random import choice\n",
        "'''\n",
        "repeat = choice([1, 2], size=data.shape[0], p=[1, 0])\n",
        "number_of_images = repeat.sum()\n",
        "print('number of images = {}'.format(number_of_images))\n",
        "root_images = np.zeros((number_of_images, data.shape[1], data.shape[2]))\n",
        "first_images = np.zeros((number_of_images, data.shape[1], data.shape[2]))\n",
        "second_images = np.zeros((number_of_images, data.shape[1], data.shape[2]))\n",
        "Y = torch.zeros((number_of_images, 3), dtype=torch.int16)\n",
        "second_translation = torch.zeros((number_of_images, 2), dtype=torch.int16)\n",
        "count = 0\n",
        "for i, j in zip(range(data.shape[0]), repeat):\n",
        "    im = Image.fromarray(data[i, ...].astype(np.uint8))\n",
        "    for t in range(j):\n",
        "        #numbers = [randint(-10, 10), randint(-10, 10)]\n",
        "        numbers = [0, 0]\n",
        "        r1 = get_rotation_number()\n",
        "        one = affine(im, angle=r1, translate=numbers, scale=1, shear=0)\n",
        "        #one = affine(im, angle=0, translate=numbers, scale=1, shear=0)\n",
        "        #rotated = rotate_image(data[i, ...], numbers[0], (25, 25))\n",
        "        #translated = translate_image(rotated, numbers[1], numbers[2])\n",
        "\n",
        "        #numbers2 = [randint(-10, 10), randint(-10, 10)]\n",
        "        numbers2 = [0, 0]\n",
        "        r2 = get_rotation_number(r1)\n",
        "        two = affine(im, angle=r2, translate=numbers2, scale=1, shear=0)\n",
        "        #two = affine(im, angle=0, translate=numbers2, scale=1, shear=0)\n",
        "        #rotated2 = rotate_image(data[i, ...], numbers2[0], (25, 25))\n",
        "        #translated2 = translate_image(rotated2, numbers2[1], numbers2[2])\n",
        "        \n",
        "        second_images[count, ...] = two\n",
        "        second_translation[count] = torch.tensor([numbers2[0], numbers2[1]])\n",
        "        first_images[count, ...] = one\n",
        "        root_images[count, ...] = data[i, ...]\n",
        "        Y[count] = torch.tensor([r1 - r2, numbers[0] - numbers2[0], numbers[1] - numbers2[1]])\n",
        "        #Y[count] = torch.tensor([0, numbers[0] - numbers2[0], numbers[1] - numbers2[1]])\n",
        "        count += 1\n",
        "        \n",
        "del data\n",
        "\n",
        "fig, ax = plt.subplots(4, 2, figsize=(24, 9*4))\n",
        "for i in range(4):\n",
        "    n = randint(0, number_of_images)\n",
        "    print(Y[n])\n",
        "    ax[i, 0].imshow(first_images[n,...], cmap='gray')\n",
        "    ax[i, 1].imshow(second_images[n,...], cmap='gray')'''"
      ],
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\nrepeat = choice([1, 2], size=data.shape[0], p=[1, 0])\\nnumber_of_images = repeat.sum()\\nprint('number of images = {}'.format(number_of_images))\\nroot_images = np.zeros((number_of_images, data.shape[1], data.shape[2]))\\nfirst_images = np.zeros((number_of_images, data.shape[1], data.shape[2]))\\nsecond_images = np.zeros((number_of_images, data.shape[1], data.shape[2]))\\nY = torch.zeros((number_of_images, 3), dtype=torch.int16)\\nsecond_translation = torch.zeros((number_of_images, 2), dtype=torch.int16)\\ncount = 0\\nfor i, j in zip(range(data.shape[0]), repeat):\\n    im = Image.fromarray(data[i, ...].astype(np.uint8))\\n    for t in range(j):\\n        #numbers = [randint(-10, 10), randint(-10, 10)]\\n        numbers = [0, 0]\\n        r1 = get_rotation_number()\\n        one = affine(im, angle=r1, translate=numbers, scale=1, shear=0)\\n        #one = affine(im, angle=0, translate=numbers, scale=1, shear=0)\\n        #rotated = rotate_image(data[i, ...], numbers[0], (25, 25))\\n        #translated = translate_image(rotated, numbers[1], numbers[2])\\n\\n        #numbers2 = [randint(-10, 10), randint(-10, 10)]\\n        numbers2 = [0, 0]\\n        r2 = get_rotation_number(r1)\\n        two = affine(im, angle=r2, translate=numbers2, scale=1, shear=0)\\n        #two = affine(im, angle=0, translate=numbers2, scale=1, shear=0)\\n        #rotated2 = rotate_image(data[i, ...], numbers2[0], (25, 25))\\n        #translated2 = translate_image(rotated2, numbers2[1], numbers2[2])\\n        \\n        second_images[count, ...] = two\\n        second_translation[count] = torch.tensor([numbers2[0], numbers2[1]])\\n        first_images[count, ...] = one\\n        root_images[count, ...] = data[i, ...]\\n        Y[count] = torch.tensor([r1 - r2, numbers[0] - numbers2[0], numbers[1] - numbers2[1]])\\n        #Y[count] = torch.tensor([0, numbers[0] - numbers2[0], numbers[1] - numbers2[1]])\\n        count += 1\\n        \\ndel data\\n\\nfig, ax = plt.subplots(4, 2, figsize=(24, 9*4))\\nfor i in range(4):\\n    n = randint(0, number_of_images)\\n    print(Y[n])\\n    ax[i, 0].imshow(first_images[n,...], cmap='gray')\\n    ax[i, 1].imshow(second_images[n,...], cmap='gray')\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 131
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MP5kyIV9DLcv",
        "outputId": "9e39e9ea-3db8-413f-d015-80441a39aaf9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#repeat = choice([1, 2], size=data.shape[0], p=[1, 0])\n",
        "number_of_images = 1000\n",
        "print('number of images = {}'.format(number_of_images))\n",
        "root_images = np.zeros((number_of_images, data.shape[1], data.shape[2]))\n",
        "first_images = np.zeros((number_of_images, data.shape[1], data.shape[2]))\n",
        "second_images = np.zeros((number_of_images, data.shape[1], data.shape[2]))\n",
        "Y = torch.zeros((number_of_images, 3), dtype=torch.int16)\n",
        "second_translation = torch.zeros((number_of_images, 2), dtype=torch.int16)\n",
        "count = 0\n",
        "for i in range(data.shape[0]):\n",
        "    im = Image.fromarray(data[i, ...].astype(np.uint8))\n",
        "    for t in range(-50, 51, 10):\n",
        "      if t == 0:\n",
        "        continue\n",
        "      #numbers = [randint(-10, 10), randint(-10, 10)]\n",
        "      numbers = [0, 0]\n",
        "      r1 = get_rotation_number2()\n",
        "      one = affine(im, angle=r1, translate=numbers, scale=1, shear=0)\n",
        "      #one = affine(im, angle=0, translate=numbers, scale=1, shear=0)\n",
        "      #rotated = rotate_image(data[i, ...], numbers[0], (25, 25))\n",
        "      #translated = translate_image(rotated, numbers[1], numbers[2])\n",
        "\n",
        "      #numbers2 = [randint(-10, 10), randint(-10, 10)]\n",
        "      numbers2 = [0, 0]\n",
        "      r2 = get_rotation_number2(t, r1)\n",
        "      two = affine(im, angle=r2, translate=numbers2, scale=1, shear=0)\n",
        "      #two = affine(im, angle=0, translate=numbers2, scale=1, shear=0)\n",
        "      #rotated2 = rotate_image(data[i, ...], numbers2[0], (25, 25))\n",
        "      #translated2 = translate_image(rotated2, numbers2[1], numbers2[2])\n",
        "      \n",
        "      second_images[count, ...] = two\n",
        "      second_translation[count] = torch.tensor([numbers2[0], numbers2[1]])\n",
        "      first_images[count, ...] = one\n",
        "      root_images[count, ...] = data[i, ...]\n",
        "      Y[count] = torch.tensor([r1 - r2, numbers[0] - numbers2[0], numbers[1] - numbers2[1]])\n",
        "      #Y[count] = torch.tensor([0, numbers[0] - numbers2[0], numbers[1] - numbers2[1]])\n",
        "      count += 1\n",
        "        \n",
        "del data\n",
        "print(count)\n",
        "\n",
        "fig, ax = plt.subplots(4, 2, figsize=(24, 9*4))\n",
        "for i in range(4):\n",
        "    n = randint(0, number_of_images)\n",
        "    print(Y[n])\n",
        "    ax[i, 0].imshow(first_images[n,...], cmap='gray')\n",
        "    ax[i, 1].imshow(second_images[n,...], cmap='gray')"
      ],
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of images = 1000\n",
            "1000\n",
            "tensor([40,  0,  0], dtype=torch.int16)\n",
            "tensor([50,  0,  0], dtype=torch.int16)\n",
            "tensor([-30,   0,   0], dtype=torch.int16)\n",
            "tensor([10,  0,  0], dtype=torch.int16)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1728x2592 with 8 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "qU1ElsFF_IDR"
      },
      "source": [
        "from math import cos, sin\n",
        "\n",
        "class Q_value():\n",
        "    def __init__(self, gamma, epsilon, R, number_of_outputs):\n",
        "        self.Tg = None\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.R = R\n",
        "        self.number_of_outputs = number_of_outputs\n",
        "\n",
        "    def get_matrix_from_tensor(self, Tt):\n",
        "        if self.number_of_outputs == 12:\n",
        "            A = torch.tensor([[1, 0, 0, Tt[0]],\n",
        "                            [0, cos(Tt[3]), -sin(Tt[3]), Tt[1]],\n",
        "                            [0, sin(Tt[3]), cos(Tt[3]), Tt[2]],\n",
        "                            [0, 0, 0, 1]])\n",
        "\n",
        "            B = torch.tensor([[cos(Tt[4]), 0, sin(Tt[4]), 0],\n",
        "                            [0, 0, 0, 0],\n",
        "                            [-sin(Tt[4]), 0, cos(Tt[4]), 0],\n",
        "                            [0, 0, 0, 1]])\n",
        "\n",
        "            C = torch.tensor([[cos(Tt[5]), -sin(Tt[5]), 0, 0],\n",
        "                            [sin(Tt[5]), cos(Tt[5]), 0, 0],\n",
        "                            [0, 0, 1, 0],\n",
        "                            [0, 0, 0, 1]])\n",
        "\n",
        "            D = torch.matmul(A, B)\n",
        "            return torch.matmul(D, C)\n",
        "        \n",
        "        elif self.number_of_outputs == 6:\n",
        "\n",
        "            A = torch.tensor([[1, 0, 0, Tt[1]],\n",
        "                            [0, cos(Tt[0]), -sin(Tt[0]), Tt[2]],\n",
        "                            [0, sin(Tt[0]), cos(Tt[0]), 0],\n",
        "                            [0, 0, 0, 1]])\n",
        "            \n",
        "            return A\n",
        "            \n",
        "\n",
        "    def get_optimal_action(self, Tt):\n",
        "        at = torch.cat((torch.eye(self.number_of_outputs // 2), torch.eye(self.number_of_outputs // 2) * -1))\n",
        "        at = at\n",
        "        at_o_Tt = at + Tt.view((1, 3))\n",
        "        norms = [self.get_distance(at_o_Tt[i, :]) for i in range(self.number_of_outputs)]\n",
        "        return np.argmin(np.asarray(norms))\n",
        "    \n",
        "    def get_distance(self, x):\n",
        "        if self.number_of_outputs == 4 or self.number_of_outputs == 6:\n",
        "            inversed = x * -1\n",
        "            composed = self.Tg + inversed\n",
        "        else:\n",
        "            matrice = self.get_matrix_from_tensor(x)\n",
        "            inversed = torch.inverse(matrice)\n",
        "            Tg_matrix = self.get_matrix_from_tensor(self.Tg)\n",
        "            composed = torch.matmul(Tg_matrix, inversed)\n",
        "            print(composed)\n",
        "        composed = composed.float()\n",
        "        norm = torch.norm(composed).item()\n",
        "        return norm\n",
        "    \n",
        "    def set_Tg(self, Tg):\n",
        "        self.Tg = Tg\n",
        "\n",
        "    def take_action(self, action, Tt):\n",
        "        current_Tt = Tt.clone()\n",
        "        if action < self.number_of_outputs // 2:\n",
        "            current_Tt[action] +=1\n",
        "            return current_Tt\n",
        "        else:\n",
        "            current_Tt[action - (self.number_of_outputs // 2)] -=1\n",
        "            return current_Tt\n",
        "    \n",
        "    def get_reward(self, Tt, Tt_next):\n",
        "        return self.get_distance(Tt) - self.get_distance(Tt_next)\n",
        "\n",
        "    def get_q_value(self, Tt, action):\n",
        "        Tt_next = self.take_action(action, Tt)\n",
        "        reward = self.get_reward(Tt, Tt_next)\n",
        "        #print(self.get_distance(Tt_next))\n",
        "        if self.get_distance(Tt_next) < self.epsilon:\n",
        "            return reward + self.R\n",
        "        optimal_action_next = self.get_optimal_action(Tt_next)\n",
        "        return reward + self.gamma * self.get_q_value(Tt_next, optimal_action_next)"
      ],
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "sDj5rjIk_IDb"
      },
      "source": [
        "class Dataset_train(torch.utils.data.Dataset):\n",
        "  'Characterizes a dataset for PyTorch'\n",
        "  def __init__(self, X, Q, transform):\n",
        "        'Initialization'\n",
        "        self.X = X\n",
        "        self.Q = Q\n",
        "        self.transform = transform\n",
        "\n",
        "  def __len__(self):\n",
        "        'Denotes the total number of samples'\n",
        "        return self.X.shape[0]\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "        'Generates one sample of data'\n",
        "        # Select sample\n",
        "        X = self.X[index]\n",
        "        #X = self.transform(X)\n",
        "\n",
        "        # Load data and get label\n",
        "        Q = self.Q[index]\n",
        "\n",
        "        return X, Q"
      ],
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "Wngxnj77_IDm"
      },
      "source": [
        "class Dataset_test(torch.utils.data.Dataset):\n",
        "  'Characterizes a dataset for PyTorch'\n",
        "  def __init__(self, X, Y, first_images, root_image, second_translation, transform):\n",
        "        'Initialization'\n",
        "        self.X = X\n",
        "        self.Y = Y\n",
        "        self.first_images = first_images\n",
        "        #self.second_images = second_images\n",
        "        self.root = root_image\n",
        "        self.second_translation = second_translation\n",
        "        self.transform = transform\n",
        "\n",
        "  def __len__(self):\n",
        "        'Denotes the total number of samples'\n",
        "        return self.X.shape[0]\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "        'Generates one sample of data'\n",
        "        # Select sample\n",
        "        X = self.X[index]\n",
        "        #X = self.transform(X)\n",
        "\n",
        "        # Load data and get label\n",
        "        Y = self.Y[index]\n",
        "        \n",
        "        one = self.first_images[index]\n",
        "        #two = self.second_images[index]\n",
        "        \n",
        "        root_image = self.root[index]\n",
        "        second_translation = self.second_translation[index]\n",
        "\n",
        "        return X, Y, one, root_image, second_translation, index"
      ],
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "j0mM0eU1_IDy",
        "outputId": "0010f82e-10ac-411d-ae56-80a8e8f01cbe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "import torchvision.transforms as transforms\n",
        "\n",
        "X = first_images - second_images\n",
        "del second_images\n",
        "X = np.expand_dims(X, axis=(1))\n",
        "X /= 255\n",
        "#mean = X.mean()\n",
        "#std = X.std()\n",
        "\n",
        "transform = transforms.Compose(\n",
        "    [transforms.Normalize((0.5), (0.5))])\n",
        "\n",
        "all_indices = list(range(X.shape[0]))\n",
        "print('ok1')\n",
        "\n",
        "test_size_percent = 0.00001\n",
        "test_size = test_size_percent * len(all_indices)\n",
        "train_indices, test_indices = train_test_split(all_indices, test_size=test_size_percent, random_state=42)\n",
        "del all_indices\n",
        "X_train = torch.from_numpy(X[train_indices, ...])\n",
        "X_test = torch.from_numpy(X[test_indices, ...])\n",
        "del X\n",
        "Y_train = Y[train_indices, ...]\n",
        "Y_test = Y[test_indices, ...]\n",
        "del Y\n",
        "print('ok2')\n",
        "\n",
        "dataset_test = Dataset_test(X_test, Y_test, first_images[test_indices, ...], root_images[test_indices, ...], second_translation[test_indices, ...], transform)\n",
        "dataset_overfit = Dataset_test(X_train, Y_train, first_images[train_indices, ...], root_images[train_indices, ...], second_translation[train_indices, ...], transform)\n",
        "\n",
        "del X_test, Y_test, test_indices, train_indices, first_images, root_images, second_translation\n",
        "\n",
        "print('ok3')\n",
        "\n",
        "subset_indices = list(range(20))\n",
        "subset_overfit = torch.utils.data.Subset(dataset_overfit, subset_indices)\n",
        "del subset_indices\n",
        "\n",
        "testing_generator = torch.utils.data.DataLoader(dataset_test)\n",
        "overfit_generator = torch.utils.data.DataLoader(subset_overfit)\n",
        "del subset_overfit, dataset_test"
      ],
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ok1\n",
            "ok2\n",
            "ok3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "EPfZi-7K_IEB",
        "outputId": "e75a5565-cf2f-49c6-8947-1ad0e9acc3a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "q_value = Q_value(0.9, 0.5, 10, 6)\n",
        "Tt = torch.zeros(3)\n",
        "q_values = torch.zeros((X_train.shape[0], 6)).to(device)\n",
        "for i in range(X_train.shape[0]):\n",
        "    print(i)\n",
        "    q_value.set_Tg(Y_train[i])\n",
        "    row = []\n",
        "    for t in range(6):\n",
        "        row.append(q_value.get_q_value(Tt, t))\n",
        "    q_values[i, :] = torch.tensor(row).reshape((1, -1))"
      ],
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "27\n",
            "28\n",
            "29\n",
            "30\n",
            "31\n",
            "32\n",
            "33\n",
            "34\n",
            "35\n",
            "36\n",
            "37\n",
            "38\n",
            "39\n",
            "40\n",
            "41\n",
            "42\n",
            "43\n",
            "44\n",
            "45\n",
            "46\n",
            "47\n",
            "48\n",
            "49\n",
            "50\n",
            "51\n",
            "52\n",
            "53\n",
            "54\n",
            "55\n",
            "56\n",
            "57\n",
            "58\n",
            "59\n",
            "60\n",
            "61\n",
            "62\n",
            "63\n",
            "64\n",
            "65\n",
            "66\n",
            "67\n",
            "68\n",
            "69\n",
            "70\n",
            "71\n",
            "72\n",
            "73\n",
            "74\n",
            "75\n",
            "76\n",
            "77\n",
            "78\n",
            "79\n",
            "80\n",
            "81\n",
            "82\n",
            "83\n",
            "84\n",
            "85\n",
            "86\n",
            "87\n",
            "88\n",
            "89\n",
            "90\n",
            "91\n",
            "92\n",
            "93\n",
            "94\n",
            "95\n",
            "96\n",
            "97\n",
            "98\n",
            "99\n",
            "100\n",
            "101\n",
            "102\n",
            "103\n",
            "104\n",
            "105\n",
            "106\n",
            "107\n",
            "108\n",
            "109\n",
            "110\n",
            "111\n",
            "112\n",
            "113\n",
            "114\n",
            "115\n",
            "116\n",
            "117\n",
            "118\n",
            "119\n",
            "120\n",
            "121\n",
            "122\n",
            "123\n",
            "124\n",
            "125\n",
            "126\n",
            "127\n",
            "128\n",
            "129\n",
            "130\n",
            "131\n",
            "132\n",
            "133\n",
            "134\n",
            "135\n",
            "136\n",
            "137\n",
            "138\n",
            "139\n",
            "140\n",
            "141\n",
            "142\n",
            "143\n",
            "144\n",
            "145\n",
            "146\n",
            "147\n",
            "148\n",
            "149\n",
            "150\n",
            "151\n",
            "152\n",
            "153\n",
            "154\n",
            "155\n",
            "156\n",
            "157\n",
            "158\n",
            "159\n",
            "160\n",
            "161\n",
            "162\n",
            "163\n",
            "164\n",
            "165\n",
            "166\n",
            "167\n",
            "168\n",
            "169\n",
            "170\n",
            "171\n",
            "172\n",
            "173\n",
            "174\n",
            "175\n",
            "176\n",
            "177\n",
            "178\n",
            "179\n",
            "180\n",
            "181\n",
            "182\n",
            "183\n",
            "184\n",
            "185\n",
            "186\n",
            "187\n",
            "188\n",
            "189\n",
            "190\n",
            "191\n",
            "192\n",
            "193\n",
            "194\n",
            "195\n",
            "196\n",
            "197\n",
            "198\n",
            "199\n",
            "200\n",
            "201\n",
            "202\n",
            "203\n",
            "204\n",
            "205\n",
            "206\n",
            "207\n",
            "208\n",
            "209\n",
            "210\n",
            "211\n",
            "212\n",
            "213\n",
            "214\n",
            "215\n",
            "216\n",
            "217\n",
            "218\n",
            "219\n",
            "220\n",
            "221\n",
            "222\n",
            "223\n",
            "224\n",
            "225\n",
            "226\n",
            "227\n",
            "228\n",
            "229\n",
            "230\n",
            "231\n",
            "232\n",
            "233\n",
            "234\n",
            "235\n",
            "236\n",
            "237\n",
            "238\n",
            "239\n",
            "240\n",
            "241\n",
            "242\n",
            "243\n",
            "244\n",
            "245\n",
            "246\n",
            "247\n",
            "248\n",
            "249\n",
            "250\n",
            "251\n",
            "252\n",
            "253\n",
            "254\n",
            "255\n",
            "256\n",
            "257\n",
            "258\n",
            "259\n",
            "260\n",
            "261\n",
            "262\n",
            "263\n",
            "264\n",
            "265\n",
            "266\n",
            "267\n",
            "268\n",
            "269\n",
            "270\n",
            "271\n",
            "272\n",
            "273\n",
            "274\n",
            "275\n",
            "276\n",
            "277\n",
            "278\n",
            "279\n",
            "280\n",
            "281\n",
            "282\n",
            "283\n",
            "284\n",
            "285\n",
            "286\n",
            "287\n",
            "288\n",
            "289\n",
            "290\n",
            "291\n",
            "292\n",
            "293\n",
            "294\n",
            "295\n",
            "296\n",
            "297\n",
            "298\n",
            "299\n",
            "300\n",
            "301\n",
            "302\n",
            "303\n",
            "304\n",
            "305\n",
            "306\n",
            "307\n",
            "308\n",
            "309\n",
            "310\n",
            "311\n",
            "312\n",
            "313\n",
            "314\n",
            "315\n",
            "316\n",
            "317\n",
            "318\n",
            "319\n",
            "320\n",
            "321\n",
            "322\n",
            "323\n",
            "324\n",
            "325\n",
            "326\n",
            "327\n",
            "328\n",
            "329\n",
            "330\n",
            "331\n",
            "332\n",
            "333\n",
            "334\n",
            "335\n",
            "336\n",
            "337\n",
            "338\n",
            "339\n",
            "340\n",
            "341\n",
            "342\n",
            "343\n",
            "344\n",
            "345\n",
            "346\n",
            "347\n",
            "348\n",
            "349\n",
            "350\n",
            "351\n",
            "352\n",
            "353\n",
            "354\n",
            "355\n",
            "356\n",
            "357\n",
            "358\n",
            "359\n",
            "360\n",
            "361\n",
            "362\n",
            "363\n",
            "364\n",
            "365\n",
            "366\n",
            "367\n",
            "368\n",
            "369\n",
            "370\n",
            "371\n",
            "372\n",
            "373\n",
            "374\n",
            "375\n",
            "376\n",
            "377\n",
            "378\n",
            "379\n",
            "380\n",
            "381\n",
            "382\n",
            "383\n",
            "384\n",
            "385\n",
            "386\n",
            "387\n",
            "388\n",
            "389\n",
            "390\n",
            "391\n",
            "392\n",
            "393\n",
            "394\n",
            "395\n",
            "396\n",
            "397\n",
            "398\n",
            "399\n",
            "400\n",
            "401\n",
            "402\n",
            "403\n",
            "404\n",
            "405\n",
            "406\n",
            "407\n",
            "408\n",
            "409\n",
            "410\n",
            "411\n",
            "412\n",
            "413\n",
            "414\n",
            "415\n",
            "416\n",
            "417\n",
            "418\n",
            "419\n",
            "420\n",
            "421\n",
            "422\n",
            "423\n",
            "424\n",
            "425\n",
            "426\n",
            "427\n",
            "428\n",
            "429\n",
            "430\n",
            "431\n",
            "432\n",
            "433\n",
            "434\n",
            "435\n",
            "436\n",
            "437\n",
            "438\n",
            "439\n",
            "440\n",
            "441\n",
            "442\n",
            "443\n",
            "444\n",
            "445\n",
            "446\n",
            "447\n",
            "448\n",
            "449\n",
            "450\n",
            "451\n",
            "452\n",
            "453\n",
            "454\n",
            "455\n",
            "456\n",
            "457\n",
            "458\n",
            "459\n",
            "460\n",
            "461\n",
            "462\n",
            "463\n",
            "464\n",
            "465\n",
            "466\n",
            "467\n",
            "468\n",
            "469\n",
            "470\n",
            "471\n",
            "472\n",
            "473\n",
            "474\n",
            "475\n",
            "476\n",
            "477\n",
            "478\n",
            "479\n",
            "480\n",
            "481\n",
            "482\n",
            "483\n",
            "484\n",
            "485\n",
            "486\n",
            "487\n",
            "488\n",
            "489\n",
            "490\n",
            "491\n",
            "492\n",
            "493\n",
            "494\n",
            "495\n",
            "496\n",
            "497\n",
            "498\n",
            "499\n",
            "500\n",
            "501\n",
            "502\n",
            "503\n",
            "504\n",
            "505\n",
            "506\n",
            "507\n",
            "508\n",
            "509\n",
            "510\n",
            "511\n",
            "512\n",
            "513\n",
            "514\n",
            "515\n",
            "516\n",
            "517\n",
            "518\n",
            "519\n",
            "520\n",
            "521\n",
            "522\n",
            "523\n",
            "524\n",
            "525\n",
            "526\n",
            "527\n",
            "528\n",
            "529\n",
            "530\n",
            "531\n",
            "532\n",
            "533\n",
            "534\n",
            "535\n",
            "536\n",
            "537\n",
            "538\n",
            "539\n",
            "540\n",
            "541\n",
            "542\n",
            "543\n",
            "544\n",
            "545\n",
            "546\n",
            "547\n",
            "548\n",
            "549\n",
            "550\n",
            "551\n",
            "552\n",
            "553\n",
            "554\n",
            "555\n",
            "556\n",
            "557\n",
            "558\n",
            "559\n",
            "560\n",
            "561\n",
            "562\n",
            "563\n",
            "564\n",
            "565\n",
            "566\n",
            "567\n",
            "568\n",
            "569\n",
            "570\n",
            "571\n",
            "572\n",
            "573\n",
            "574\n",
            "575\n",
            "576\n",
            "577\n",
            "578\n",
            "579\n",
            "580\n",
            "581\n",
            "582\n",
            "583\n",
            "584\n",
            "585\n",
            "586\n",
            "587\n",
            "588\n",
            "589\n",
            "590\n",
            "591\n",
            "592\n",
            "593\n",
            "594\n",
            "595\n",
            "596\n",
            "597\n",
            "598\n",
            "599\n",
            "600\n",
            "601\n",
            "602\n",
            "603\n",
            "604\n",
            "605\n",
            "606\n",
            "607\n",
            "608\n",
            "609\n",
            "610\n",
            "611\n",
            "612\n",
            "613\n",
            "614\n",
            "615\n",
            "616\n",
            "617\n",
            "618\n",
            "619\n",
            "620\n",
            "621\n",
            "622\n",
            "623\n",
            "624\n",
            "625\n",
            "626\n",
            "627\n",
            "628\n",
            "629\n",
            "630\n",
            "631\n",
            "632\n",
            "633\n",
            "634\n",
            "635\n",
            "636\n",
            "637\n",
            "638\n",
            "639\n",
            "640\n",
            "641\n",
            "642\n",
            "643\n",
            "644\n",
            "645\n",
            "646\n",
            "647\n",
            "648\n",
            "649\n",
            "650\n",
            "651\n",
            "652\n",
            "653\n",
            "654\n",
            "655\n",
            "656\n",
            "657\n",
            "658\n",
            "659\n",
            "660\n",
            "661\n",
            "662\n",
            "663\n",
            "664\n",
            "665\n",
            "666\n",
            "667\n",
            "668\n",
            "669\n",
            "670\n",
            "671\n",
            "672\n",
            "673\n",
            "674\n",
            "675\n",
            "676\n",
            "677\n",
            "678\n",
            "679\n",
            "680\n",
            "681\n",
            "682\n",
            "683\n",
            "684\n",
            "685\n",
            "686\n",
            "687\n",
            "688\n",
            "689\n",
            "690\n",
            "691\n",
            "692\n",
            "693\n",
            "694\n",
            "695\n",
            "696\n",
            "697\n",
            "698\n",
            "699\n",
            "700\n",
            "701\n",
            "702\n",
            "703\n",
            "704\n",
            "705\n",
            "706\n",
            "707\n",
            "708\n",
            "709\n",
            "710\n",
            "711\n",
            "712\n",
            "713\n",
            "714\n",
            "715\n",
            "716\n",
            "717\n",
            "718\n",
            "719\n",
            "720\n",
            "721\n",
            "722\n",
            "723\n",
            "724\n",
            "725\n",
            "726\n",
            "727\n",
            "728\n",
            "729\n",
            "730\n",
            "731\n",
            "732\n",
            "733\n",
            "734\n",
            "735\n",
            "736\n",
            "737\n",
            "738\n",
            "739\n",
            "740\n",
            "741\n",
            "742\n",
            "743\n",
            "744\n",
            "745\n",
            "746\n",
            "747\n",
            "748\n",
            "749\n",
            "750\n",
            "751\n",
            "752\n",
            "753\n",
            "754\n",
            "755\n",
            "756\n",
            "757\n",
            "758\n",
            "759\n",
            "760\n",
            "761\n",
            "762\n",
            "763\n",
            "764\n",
            "765\n",
            "766\n",
            "767\n",
            "768\n",
            "769\n",
            "770\n",
            "771\n",
            "772\n",
            "773\n",
            "774\n",
            "775\n",
            "776\n",
            "777\n",
            "778\n",
            "779\n",
            "780\n",
            "781\n",
            "782\n",
            "783\n",
            "784\n",
            "785\n",
            "786\n",
            "787\n",
            "788\n",
            "789\n",
            "790\n",
            "791\n",
            "792\n",
            "793\n",
            "794\n",
            "795\n",
            "796\n",
            "797\n",
            "798\n",
            "799\n",
            "800\n",
            "801\n",
            "802\n",
            "803\n",
            "804\n",
            "805\n",
            "806\n",
            "807\n",
            "808\n",
            "809\n",
            "810\n",
            "811\n",
            "812\n",
            "813\n",
            "814\n",
            "815\n",
            "816\n",
            "817\n",
            "818\n",
            "819\n",
            "820\n",
            "821\n",
            "822\n",
            "823\n",
            "824\n",
            "825\n",
            "826\n",
            "827\n",
            "828\n",
            "829\n",
            "830\n",
            "831\n",
            "832\n",
            "833\n",
            "834\n",
            "835\n",
            "836\n",
            "837\n",
            "838\n",
            "839\n",
            "840\n",
            "841\n",
            "842\n",
            "843\n",
            "844\n",
            "845\n",
            "846\n",
            "847\n",
            "848\n",
            "849\n",
            "850\n",
            "851\n",
            "852\n",
            "853\n",
            "854\n",
            "855\n",
            "856\n",
            "857\n",
            "858\n",
            "859\n",
            "860\n",
            "861\n",
            "862\n",
            "863\n",
            "864\n",
            "865\n",
            "866\n",
            "867\n",
            "868\n",
            "869\n",
            "870\n",
            "871\n",
            "872\n",
            "873\n",
            "874\n",
            "875\n",
            "876\n",
            "877\n",
            "878\n",
            "879\n",
            "880\n",
            "881\n",
            "882\n",
            "883\n",
            "884\n",
            "885\n",
            "886\n",
            "887\n",
            "888\n",
            "889\n",
            "890\n",
            "891\n",
            "892\n",
            "893\n",
            "894\n",
            "895\n",
            "896\n",
            "897\n",
            "898\n",
            "899\n",
            "900\n",
            "901\n",
            "902\n",
            "903\n",
            "904\n",
            "905\n",
            "906\n",
            "907\n",
            "908\n",
            "909\n",
            "910\n",
            "911\n",
            "912\n",
            "913\n",
            "914\n",
            "915\n",
            "916\n",
            "917\n",
            "918\n",
            "919\n",
            "920\n",
            "921\n",
            "922\n",
            "923\n",
            "924\n",
            "925\n",
            "926\n",
            "927\n",
            "928\n",
            "929\n",
            "930\n",
            "931\n",
            "932\n",
            "933\n",
            "934\n",
            "935\n",
            "936\n",
            "937\n",
            "938\n",
            "939\n",
            "940\n",
            "941\n",
            "942\n",
            "943\n",
            "944\n",
            "945\n",
            "946\n",
            "947\n",
            "948\n",
            "949\n",
            "950\n",
            "951\n",
            "952\n",
            "953\n",
            "954\n",
            "955\n",
            "956\n",
            "957\n",
            "958\n",
            "959\n",
            "960\n",
            "961\n",
            "962\n",
            "963\n",
            "964\n",
            "965\n",
            "966\n",
            "967\n",
            "968\n",
            "969\n",
            "970\n",
            "971\n",
            "972\n",
            "973\n",
            "974\n",
            "975\n",
            "976\n",
            "977\n",
            "978\n",
            "979\n",
            "980\n",
            "981\n",
            "982\n",
            "983\n",
            "984\n",
            "985\n",
            "986\n",
            "987\n",
            "988\n",
            "989\n",
            "990\n",
            "991\n",
            "992\n",
            "993\n",
            "994\n",
            "995\n",
            "996\n",
            "997\n",
            "998\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "FdXE3Qe-_IEU",
        "outputId": "4017b32b-fc38-4966-809a-cf17e364ec3b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "for i in range(30):\n",
        "    print(Y_train[i])\n",
        "    print(q_values[i, :])"
      ],
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([30,  0,  0], dtype=torch.int16)\n",
            "tensor([10.0471,  8.9673,  8.9673,  8.0382,  8.9673,  8.9673], device='cuda:0')\n",
            "tensor([-50,   0,   0], dtype=torch.int16)\n",
            "tensor([ 8.0046,  8.9858,  8.9858, 10.0057,  8.9858,  8.9858], device='cuda:0')\n",
            "tensor([-50,   0,   0], dtype=torch.int16)\n",
            "tensor([ 8.0046,  8.9858,  8.9858, 10.0057,  8.9858,  8.9858], device='cuda:0')\n",
            "tensor([-40,   0,   0], dtype=torch.int16)\n",
            "tensor([ 8.0133,  8.9796,  8.9796, 10.0164,  8.9796,  8.9796], device='cuda:0')\n",
            "tensor([40,  0,  0], dtype=torch.int16)\n",
            "tensor([10.0164,  8.9796,  8.9796,  8.0133,  8.9796,  8.9796], device='cuda:0')\n",
            "tensor([20,  0,  0], dtype=torch.int16)\n",
            "tensor([10.1351,  8.9422,  8.9422,  8.1094,  8.9422,  8.9422], device='cuda:0')\n",
            "tensor([-20,   0,   0], dtype=torch.int16)\n",
            "tensor([ 8.1094,  8.9422,  8.9422, 10.1351,  8.9422,  8.9422], device='cuda:0')\n",
            "tensor([50,  0,  0], dtype=torch.int16)\n",
            "tensor([10.0057,  8.9858,  8.9858,  8.0046,  8.9858,  8.9858], device='cuda:0')\n",
            "tensor([20,  0,  0], dtype=torch.int16)\n",
            "tensor([10.1351,  8.9422,  8.9422,  8.1094,  8.9422,  8.9422], device='cuda:0')\n",
            "tensor([-40,   0,   0], dtype=torch.int16)\n",
            "tensor([ 8.0133,  8.9796,  8.9796, 10.0164,  8.9796,  8.9796], device='cuda:0')\n",
            "tensor([20,  0,  0], dtype=torch.int16)\n",
            "tensor([10.1351,  8.9422,  8.9422,  8.1094,  8.9422,  8.9422], device='cuda:0')\n",
            "tensor([20,  0,  0], dtype=torch.int16)\n",
            "tensor([10.1351,  8.9422,  8.9422,  8.1094,  8.9422,  8.9422], device='cuda:0')\n",
            "tensor([-20,   0,   0], dtype=torch.int16)\n",
            "tensor([ 8.1094,  8.9422,  8.9422, 10.1351,  8.9422,  8.9422], device='cuda:0')\n",
            "tensor([40,  0,  0], dtype=torch.int16)\n",
            "tensor([10.0164,  8.9796,  8.9796,  8.0133,  8.9796,  8.9796], device='cuda:0')\n",
            "tensor([50,  0,  0], dtype=torch.int16)\n",
            "tensor([10.0057,  8.9858,  8.9858,  8.0046,  8.9858,  8.9858], device='cuda:0')\n",
            "tensor([-50,   0,   0], dtype=torch.int16)\n",
            "tensor([ 8.0046,  8.9858,  8.9858, 10.0057,  8.9858,  8.9858], device='cuda:0')\n",
            "tensor([-20,   0,   0], dtype=torch.int16)\n",
            "tensor([ 8.1094,  8.9422,  8.9422, 10.1351,  8.9422,  8.9422], device='cuda:0')\n",
            "tensor([-40,   0,   0], dtype=torch.int16)\n",
            "tensor([ 8.0133,  8.9796,  8.9796, 10.0164,  8.9796,  8.9796], device='cuda:0')\n",
            "tensor([50,  0,  0], dtype=torch.int16)\n",
            "tensor([10.0057,  8.9858,  8.9858,  8.0046,  8.9858,  8.9858], device='cuda:0')\n",
            "tensor([50,  0,  0], dtype=torch.int16)\n",
            "tensor([10.0057,  8.9858,  8.9858,  8.0046,  8.9858,  8.9858], device='cuda:0')\n",
            "tensor([-10,   0,   0], dtype=torch.int16)\n",
            "tensor([ 8.3138,  8.8931,  8.8931, 10.3874,  8.8931,  8.8931], device='cuda:0')\n",
            "tensor([-40,   0,   0], dtype=torch.int16)\n",
            "tensor([ 8.0133,  8.9796,  8.9796, 10.0164,  8.9796,  8.9796], device='cuda:0')\n",
            "tensor([30,  0,  0], dtype=torch.int16)\n",
            "tensor([10.0471,  8.9673,  8.9673,  8.0382,  8.9673,  8.9673], device='cuda:0')\n",
            "tensor([-50,   0,   0], dtype=torch.int16)\n",
            "tensor([ 8.0046,  8.9858,  8.9858, 10.0057,  8.9858,  8.9858], device='cuda:0')\n",
            "tensor([10,  0,  0], dtype=torch.int16)\n",
            "tensor([10.3874,  8.8931,  8.8931,  8.3138,  8.8931,  8.8931], device='cuda:0')\n",
            "tensor([-40,   0,   0], dtype=torch.int16)\n",
            "tensor([ 8.0133,  8.9796,  8.9796, 10.0164,  8.9796,  8.9796], device='cuda:0')\n",
            "tensor([20,  0,  0], dtype=torch.int16)\n",
            "tensor([10.1351,  8.9422,  8.9422,  8.1094,  8.9422,  8.9422], device='cuda:0')\n",
            "tensor([-30,   0,   0], dtype=torch.int16)\n",
            "tensor([ 8.0382,  8.9673,  8.9673, 10.0471,  8.9673,  8.9673], device='cuda:0')\n",
            "tensor([30,  0,  0], dtype=torch.int16)\n",
            "tensor([10.0471,  8.9673,  8.9673,  8.0382,  8.9673,  8.9673], device='cuda:0')\n",
            "tensor([20,  0,  0], dtype=torch.int16)\n",
            "tensor([10.1351,  8.9422,  8.9422,  8.1094,  8.9422,  8.9422], device='cuda:0')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "TUg26Hs1_IEg"
      },
      "source": [
        "dataset_train = Dataset_train(X_train, q_values, transform)\n",
        "training_generator = torch.utils.data.DataLoader(dataset_train, 1)"
      ],
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "0zQ6Zo9l_IEp"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class DQN(nn.Module):\n",
        "\n",
        "    def __init__(self, d, h, w, outputs):\n",
        "        super(DQN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 8, kernel_size=3)\n",
        "        self.maxpool1 = nn.MaxPool2d(2)\n",
        "        self.bn1 = nn.BatchNorm2d(8)\n",
        "        self.conv2 = nn.Conv2d(8, 32, kernel_size=3)\n",
        "        self.maxpool2 = nn.MaxPool2d(2)\n",
        "        self.bn2 = nn.BatchNorm2d(32)\n",
        "        self.conv3 = nn.Conv2d(32, 32, kernel_size=3)\n",
        "        self.bn3 = nn.BatchNorm2d(32)\n",
        "        self.conv4 = nn.Conv2d(32, 128, kernel_size=3)\n",
        "        self.bn4 = nn.BatchNorm2d(128)\n",
        "        self.conv5 = nn.Conv2d(128, 128, kernel_size=3)\n",
        "        self.bn5 = nn.BatchNorm2d(128)\n",
        "\n",
        "        # Number of Linear input connections depends on output of conv2d layers\n",
        "        # and therefore the input image size, so compute it.\n",
        "        def conv2d_size_out(w, f=3, p=0, s=1):\n",
        "            return ((w - f + 2*p) // s) + 1\n",
        "        \n",
        "        def maxpool_size_out(w):\n",
        "            return w // 2\n",
        "        \n",
        "        convw = conv2d_size_out(conv2d_size_out(conv2d_size_out(maxpool_size_out(conv2d_size_out(maxpool_size_out(conv2d_size_out(w)))))))\n",
        "        linear_input_size = convw * convw * 128\n",
        "        \n",
        "        self.head1 = nn.Linear(linear_input_size, 512)\n",
        "        self.head2 = nn.Linear(512, 512)\n",
        "        self.head3 = nn.Linear(512, 64)\n",
        "        self.head4 = nn.Linear(64, outputs)\n",
        "        \n",
        "        self.linear_input_size = linear_input_size\n",
        "\n",
        "    # Called with either one element to determine next action, or a batch\n",
        "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.bn1(self.maxpool1(self.conv1(x))))\n",
        "        x = F.relu(self.bn2(self.maxpool2(self.conv2(x))))\n",
        "        x = F.relu(self.bn3(self.conv3(x)))\n",
        "        x = F.relu(self.bn4(self.conv4(x)))\n",
        "        x = F.relu(self.bn5(self.conv5(x)))\n",
        "        x = x.view(-1, self.linear_input_size)\n",
        "        x = F.relu(self.head1(x))\n",
        "        x = F.relu(self.head2(x))\n",
        "        x = F.relu(self.head3(x))\n",
        "        return self.head4(x)\n",
        "\n",
        "#for x, y in training_generator:\n",
        "#    print(x.size())\n",
        " #   net = DQN(1, x.shape[2], x.shape[3], 4).float()\n",
        "#    out = net(x.float())\n",
        "#    print(out.max(1)[1])\n",
        "#    print(torch.argmax(out, dim=1))\n",
        "#    break"
      ],
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "Sywf0uAt_IE3",
        "outputId": "dbec594d-918e-49c9-8235-32dcf8b71ef9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from itertools import count\n",
        "\n",
        "policy_net = DQN(1, 50, 50, 6).to(device)\n",
        "optimizer = optim.Adam(policy_net.parameters())\n",
        "criterion = nn.MSELoss().to(device)\n",
        "running_loss = 0.0\n",
        "number_of_epoch = 2000\n",
        "\n",
        "for i_episode in range(number_of_epoch):\n",
        "    for index, data in enumerate(training_generator):\n",
        "        x = data[0].to(device)\n",
        "        q = data[1].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        out = policy_net(x.float()).to(device)\n",
        "        \n",
        "        #loss = torch.norm(out - q, dim=1).sum()\n",
        "        loss = criterion(out, q)\n",
        "\n",
        "        loss.backward()\n",
        "        #for param in policy_net.parameters():\n",
        "         #   param.grad.data.clamp_(-1, 1)\n",
        "        optimizer.step()\n",
        "        \n",
        "        running_loss += loss.item()\n",
        "    # print statistics\n",
        "    print(\"Epoch: {} Loss: {}\".format((i_episode + 1), running_loss / (i_episode + 1)))\n"
      ],
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1 Loss: 1499.0889339828864\n",
            "Epoch: 2 Loss: 924.6355310226791\n",
            "Epoch: 3 Loss: 697.4603195271533\n",
            "Epoch: 4 Loss: 572.2800813154608\n",
            "Epoch: 5 Loss: 487.2905974496389\n",
            "Epoch: 6 Loss: 423.6491885356566\n",
            "Epoch: 7 Loss: 384.6737361576109\n",
            "Epoch: 8 Loss: 347.90005868660955\n",
            "Epoch: 9 Loss: 320.99809634480494\n",
            "Epoch: 10 Loss: 294.6219061348587\n",
            "Epoch: 11 Loss: 272.81418042492373\n",
            "Epoch: 12 Loss: 257.1614119019844\n",
            "Epoch: 13 Loss: 240.4735196209479\n",
            "Epoch: 14 Loss: 227.61340649089107\n",
            "Epoch: 15 Loss: 215.58683229340627\n",
            "Epoch: 16 Loss: 206.03527564895649\n",
            "Epoch: 17 Loss: 196.08439650738472\n",
            "Epoch: 18 Loss: 187.36741017387612\n",
            "Epoch: 19 Loss: 179.3467722209672\n",
            "Epoch: 20 Loss: 172.92078758563693\n",
            "Epoch: 21 Loss: 165.79888828823252\n",
            "Epoch: 22 Loss: 159.8534070639547\n",
            "Epoch: 23 Loss: 154.72034157653061\n",
            "Epoch: 24 Loss: 149.43029250365012\n",
            "Epoch: 25 Loss: 144.53703562955852\n",
            "Epoch: 26 Loss: 140.06690012806283\n",
            "Epoch: 27 Loss: 136.9162085953246\n",
            "Epoch: 28 Loss: 133.71592200538913\n",
            "Epoch: 29 Loss: 130.1655582486855\n",
            "Epoch: 30 Loss: 126.8227595481143\n",
            "Epoch: 31 Loss: 123.73604315372681\n",
            "Epoch: 32 Loss: 120.97440361695948\n",
            "Epoch: 33 Loss: 118.49949228950015\n",
            "Epoch: 34 Loss: 116.48541566142285\n",
            "Epoch: 35 Loss: 114.5410843507619\n",
            "Epoch: 36 Loss: 111.9279626482089\n",
            "Epoch: 37 Loss: 109.43715562126212\n",
            "Epoch: 38 Loss: 107.35249133959007\n",
            "Epoch: 39 Loss: 105.37043892186888\n",
            "Epoch: 40 Loss: 103.36352465476416\n",
            "Epoch: 41 Loss: 102.20286308242478\n",
            "Epoch: 42 Loss: 100.3390051833461\n",
            "Epoch: 43 Loss: 98.40655727807804\n",
            "Epoch: 44 Loss: 96.50272516625955\n",
            "Epoch: 45 Loss: 94.78345149009094\n",
            "Epoch: 46 Loss: 93.64784271087188\n",
            "Epoch: 47 Loss: 92.51160057280364\n",
            "Epoch: 48 Loss: 90.9444591904559\n",
            "Epoch: 49 Loss: 89.42901498833987\n",
            "Epoch: 50 Loss: 88.11909340243008\n",
            "Epoch: 51 Loss: 87.36021581555492\n",
            "Epoch: 52 Loss: 86.36654820194843\n",
            "Epoch: 53 Loss: 85.08827176926907\n",
            "Epoch: 54 Loss: 83.94011614409862\n",
            "Epoch: 55 Loss: 82.68473440371248\n",
            "Epoch: 56 Loss: 81.49898497851875\n",
            "Epoch: 57 Loss: 80.66309547252112\n",
            "Epoch: 58 Loss: 79.75640936663387\n",
            "Epoch: 59 Loss: 78.82463859308984\n",
            "Epoch: 60 Loss: 77.76273495834103\n",
            "Epoch: 61 Loss: 76.71045389446596\n",
            "Epoch: 62 Loss: 75.72808937068365\n",
            "Epoch: 63 Loss: 74.9832366639735\n",
            "Epoch: 64 Loss: 74.13456450253597\n",
            "Epoch: 65 Loss: 73.23981343532478\n",
            "Epoch: 66 Loss: 72.35985436662166\n",
            "Epoch: 67 Loss: 71.71710304704897\n",
            "Epoch: 68 Loss: 70.99995915219488\n",
            "Epoch: 69 Loss: 70.19632134685975\n",
            "Epoch: 70 Loss: 69.390920813521\n",
            "Epoch: 71 Loss: 68.64785197446588\n",
            "Epoch: 72 Loss: 68.01407986579872\n",
            "Epoch: 73 Loss: 67.45611862551411\n",
            "Epoch: 74 Loss: 66.73199447890279\n",
            "Epoch: 75 Loss: 65.9795907218764\n",
            "Epoch: 76 Loss: 65.23985222428338\n",
            "Epoch: 77 Loss: 64.54608327149614\n",
            "Epoch: 78 Loss: 63.9594588889646\n",
            "Epoch: 79 Loss: 63.52150582134141\n",
            "Epoch: 80 Loss: 63.007676994464035\n",
            "Epoch: 81 Loss: 62.41289039821499\n",
            "Epoch: 82 Loss: 61.81099492040118\n",
            "Epoch: 83 Loss: 61.23350840966329\n",
            "Epoch: 84 Loss: 60.7376161981363\n",
            "Epoch: 85 Loss: 60.367919005519944\n",
            "Epoch: 86 Loss: 59.88932122527181\n",
            "Epoch: 87 Loss: 59.35121664733941\n",
            "Epoch: 88 Loss: 58.80747986079361\n",
            "Epoch: 89 Loss: 58.27364817685653\n",
            "Epoch: 90 Loss: 57.906757186350276\n",
            "Epoch: 91 Loss: 57.51641924975137\n",
            "Epoch: 92 Loss: 57.20596531271778\n",
            "Epoch: 93 Loss: 56.82849902906187\n",
            "Epoch: 94 Loss: 56.388219379675355\n",
            "Epoch: 95 Loss: 55.961147250976786\n",
            "Epoch: 96 Loss: 55.58624593744141\n",
            "Epoch: 97 Loss: 55.28677206941796\n",
            "Epoch: 98 Loss: 54.935244892981395\n",
            "Epoch: 99 Loss: 54.55074600215136\n",
            "Epoch: 100 Loss: 54.16905071548885\n",
            "Epoch: 101 Loss: 53.75718637343895\n",
            "Epoch: 102 Loss: 53.343752936583485\n",
            "Epoch: 103 Loss: 52.94861214519933\n",
            "Epoch: 104 Loss: 52.66278168472877\n",
            "Epoch: 105 Loss: 52.54881872299103\n",
            "Epoch: 106 Loss: 52.26857114972739\n",
            "Epoch: 107 Loss: 52.031029195504814\n",
            "Epoch: 108 Loss: 51.706845941111716\n",
            "Epoch: 109 Loss: 51.343828330450606\n",
            "Epoch: 110 Loss: 50.97741749065516\n",
            "Epoch: 111 Loss: 50.621882770390684\n",
            "Epoch: 112 Loss: 50.314482671196906\n",
            "Epoch: 113 Loss: 50.18669653907928\n",
            "Epoch: 114 Loss: 49.94068261407926\n",
            "Epoch: 115 Loss: 49.605320107529664\n",
            "Epoch: 116 Loss: 49.26577916761127\n",
            "Epoch: 117 Loss: 48.93317806184677\n",
            "Epoch: 118 Loss: 48.629022732909746\n",
            "Epoch: 119 Loss: 48.45587205619612\n",
            "Epoch: 120 Loss: 48.25344298741682\n",
            "Epoch: 121 Loss: 48.0027332909498\n",
            "Epoch: 122 Loss: 47.74951899550747\n",
            "Epoch: 123 Loss: 47.52480738457872\n",
            "Epoch: 124 Loss: 47.25069184164705\n",
            "Epoch: 125 Loss: 46.95976262543047\n",
            "Epoch: 126 Loss: 46.67426959006276\n",
            "Epoch: 127 Loss: 46.42319999942827\n",
            "Epoch: 128 Loss: 46.26367367041133\n",
            "Epoch: 129 Loss: 46.033942368696856\n",
            "Epoch: 130 Loss: 45.80332939420297\n",
            "Epoch: 131 Loss: 45.57300417116092\n",
            "Epoch: 132 Loss: 45.30477016526656\n",
            "Epoch: 133 Loss: 45.041006092201755\n",
            "Epoch: 134 Loss: 44.791984897654466\n",
            "Epoch: 135 Loss: 44.59128707576853\n",
            "Epoch: 136 Loss: 44.375058454339396\n",
            "Epoch: 137 Loss: 44.15162408310519\n",
            "Epoch: 138 Loss: 43.91188063115649\n",
            "Epoch: 139 Loss: 43.664251183192754\n",
            "Epoch: 140 Loss: 43.41915951565867\n",
            "Epoch: 141 Loss: 43.18550708667599\n",
            "Epoch: 142 Loss: 43.00000348365901\n",
            "Epoch: 143 Loss: 42.83887118356087\n",
            "Epoch: 144 Loss: 42.639598541292095\n",
            "Epoch: 145 Loss: 42.43082512061919\n",
            "Epoch: 146 Loss: 42.21836960825022\n",
            "Epoch: 147 Loss: 42.01004109587451\n",
            "Epoch: 148 Loss: 41.825355368280334\n",
            "Epoch: 149 Loss: 41.720982854642564\n",
            "Epoch: 150 Loss: 41.608634501264945\n",
            "Epoch: 151 Loss: 41.44622118884287\n",
            "Epoch: 152 Loss: 41.26698250569622\n",
            "Epoch: 153 Loss: 41.11174504777282\n",
            "Epoch: 154 Loss: 40.91961375543712\n",
            "Epoch: 155 Loss: 40.71970600158886\n",
            "Epoch: 156 Loss: 40.51893450194479\n",
            "Epoch: 157 Loss: 40.321380353864285\n",
            "Epoch: 158 Loss: 40.13919958264939\n",
            "Epoch: 159 Loss: 40.01246879740413\n",
            "Epoch: 160 Loss: 39.936747596717\n",
            "Epoch: 161 Loss: 39.77325364346597\n",
            "Epoch: 162 Loss: 39.59564203904143\n",
            "Epoch: 163 Loss: 39.412162792995964\n",
            "Epoch: 164 Loss: 39.22730545243789\n",
            "Epoch: 165 Loss: 39.04568489070075\n",
            "Epoch: 166 Loss: 38.88179989711554\n",
            "Epoch: 167 Loss: 38.79201134025006\n",
            "Epoch: 168 Loss: 38.67396505883618\n",
            "Epoch: 169 Loss: 38.59228649613603\n",
            "Epoch: 170 Loss: 38.434218162864575\n",
            "Epoch: 171 Loss: 38.27156105997575\n",
            "Epoch: 172 Loss: 38.108334480788905\n",
            "Epoch: 173 Loss: 37.94654000484729\n",
            "Epoch: 174 Loss: 37.83383205294144\n",
            "Epoch: 175 Loss: 37.68543081450589\n",
            "Epoch: 176 Loss: 37.55189133518751\n",
            "Epoch: 177 Loss: 37.440437531007085\n",
            "Epoch: 178 Loss: 37.315070661015554\n",
            "Epoch: 179 Loss: 37.175213905726025\n",
            "Epoch: 180 Loss: 37.031536229145374\n",
            "Epoch: 181 Loss: 36.89119306236088\n",
            "Epoch: 182 Loss: 36.785543320718325\n",
            "Epoch: 183 Loss: 36.68017251348013\n",
            "Epoch: 184 Loss: 36.58793257619674\n",
            "Epoch: 185 Loss: 36.45035236738439\n",
            "Epoch: 186 Loss: 36.305922835962605\n",
            "Epoch: 187 Loss: 36.15988292805786\n",
            "Epoch: 188 Loss: 36.01524318481971\n",
            "Epoch: 189 Loss: 35.88843010704985\n",
            "Epoch: 190 Loss: 35.792443028849135\n",
            "Epoch: 191 Loss: 35.69891816266334\n",
            "Epoch: 192 Loss: 35.58627094002507\n",
            "Epoch: 193 Loss: 35.46272505537276\n",
            "Epoch: 194 Loss: 35.33494796742977\n",
            "Epoch: 195 Loss: 35.208828919404795\n",
            "Epoch: 196 Loss: 35.09589313703684\n",
            "Epoch: 197 Loss: 35.026073113448824\n",
            "Epoch: 198 Loss: 34.967621301733935\n",
            "Epoch: 199 Loss: 34.89053144908788\n",
            "Epoch: 200 Loss: 34.787329683714354\n",
            "Epoch: 201 Loss: 34.6567067545405\n",
            "Epoch: 202 Loss: 34.52478722290547\n",
            "Epoch: 203 Loss: 34.39463372730645\n",
            "Epoch: 204 Loss: 34.26972360630864\n",
            "Epoch: 205 Loss: 34.16695215687896\n",
            "Epoch: 206 Loss: 34.10335016637339\n",
            "Epoch: 207 Loss: 34.016609984052835\n",
            "Epoch: 208 Loss: 33.920491738493496\n",
            "Epoch: 209 Loss: 33.816916483657266\n",
            "Epoch: 210 Loss: 33.709035885690156\n",
            "Epoch: 211 Loss: 33.59185537371072\n",
            "Epoch: 212 Loss: 33.47864357093132\n",
            "Epoch: 213 Loss: 33.39208733109355\n",
            "Epoch: 214 Loss: 33.3857140314671\n",
            "Epoch: 215 Loss: 33.32817778757229\n",
            "Epoch: 216 Loss: 33.24027257151415\n",
            "Epoch: 217 Loss: 33.128231195127185\n",
            "Epoch: 218 Loss: 33.01369813025013\n",
            "Epoch: 219 Loss: 32.899132393410845\n",
            "Epoch: 220 Loss: 32.78483073715239\n",
            "Epoch: 221 Loss: 32.67261581761104\n",
            "Epoch: 222 Loss: 32.575969601906436\n",
            "Epoch: 223 Loss: 32.49090615396098\n",
            "Epoch: 224 Loss: 32.400585107711\n",
            "Epoch: 225 Loss: 32.313101706912995\n",
            "Epoch: 226 Loss: 32.20879449591526\n",
            "Epoch: 227 Loss: 32.10487163262869\n",
            "Epoch: 228 Loss: 32.00702848311985\n",
            "Epoch: 229 Loss: 31.948043077216465\n",
            "Epoch: 230 Loss: 31.927368266344843\n",
            "Epoch: 231 Loss: 31.898771711385876\n",
            "Epoch: 232 Loss: 31.81172613999033\n",
            "Epoch: 233 Loss: 31.719220169027007\n",
            "Epoch: 234 Loss: 31.67816341284437\n",
            "Epoch: 235 Loss: 31.610905838902394\n",
            "Epoch: 236 Loss: 31.522471721940857\n",
            "Epoch: 237 Loss: 31.43399460281714\n",
            "Epoch: 238 Loss: 31.34626779657091\n",
            "Epoch: 239 Loss: 31.261061539289035\n",
            "Epoch: 240 Loss: 31.18478756313713\n",
            "Epoch: 241 Loss: 31.136191723701284\n",
            "Epoch: 242 Loss: 31.081218082099323\n",
            "Epoch: 243 Loss: 31.013963969407694\n",
            "Epoch: 244 Loss: 30.955477166827993\n",
            "Epoch: 245 Loss: 30.88687879949597\n",
            "Epoch: 246 Loss: 30.80159004514249\n",
            "Epoch: 247 Loss: 30.716057696457117\n",
            "Epoch: 248 Loss: 30.633103031207565\n",
            "Epoch: 249 Loss: 30.562124168469897\n",
            "Epoch: 250 Loss: 30.523295387757138\n",
            "Epoch: 251 Loss: 30.502662421724665\n",
            "Epoch: 252 Loss: 30.45463356674236\n",
            "Epoch: 253 Loss: 30.3753244474977\n",
            "Epoch: 254 Loss: 30.293260022702857\n",
            "Epoch: 255 Loss: 30.210423092119647\n",
            "Epoch: 256 Loss: 30.15342659270938\n",
            "Epoch: 257 Loss: 30.10370512159302\n",
            "Epoch: 258 Loss: 30.022436554805093\n",
            "Epoch: 259 Loss: 29.939003207052952\n",
            "Epoch: 260 Loss: 29.854941101959252\n",
            "Epoch: 261 Loss: 29.77152053047802\n",
            "Epoch: 262 Loss: 29.693567950633266\n",
            "Epoch: 263 Loss: 29.636658410686927\n",
            "Epoch: 264 Loss: 29.588514993174325\n",
            "Epoch: 265 Loss: 29.54982841954546\n",
            "Epoch: 266 Loss: 29.488035282076467\n",
            "Epoch: 267 Loss: 29.42035418763443\n",
            "Epoch: 268 Loss: 29.346677212440756\n",
            "Epoch: 269 Loss: 29.27075227621767\n",
            "Epoch: 270 Loss: 29.193489392000846\n",
            "Epoch: 271 Loss: 29.115829937225037\n",
            "Epoch: 272 Loss: 29.038892630145455\n",
            "Epoch: 273 Loss: 28.965217641774576\n",
            "Epoch: 274 Loss: 28.910001003098536\n",
            "Epoch: 275 Loss: 28.86849034707089\n",
            "Epoch: 276 Loss: 28.82206317847773\n",
            "Epoch: 277 Loss: 28.7515480560861\n",
            "Epoch: 278 Loss: 28.67810101081794\n",
            "Epoch: 279 Loss: 28.60361297992537\n",
            "Epoch: 280 Loss: 28.528862619631045\n",
            "Epoch: 281 Loss: 28.454961744680325\n",
            "Epoch: 282 Loss: 28.41033628331162\n",
            "Epoch: 283 Loss: 28.41125736650693\n",
            "Epoch: 284 Loss: 28.37035063567236\n",
            "Epoch: 285 Loss: 28.330601431891065\n",
            "Epoch: 286 Loss: 28.287542625203606\n",
            "Epoch: 287 Loss: 28.23777994871945\n",
            "Epoch: 288 Loss: 28.185678446355457\n",
            "Epoch: 289 Loss: 28.127486120116775\n",
            "Epoch: 290 Loss: 28.067176164793562\n",
            "Epoch: 291 Loss: 28.00558184148409\n",
            "Epoch: 292 Loss: 27.94334450147134\n",
            "Epoch: 293 Loss: 27.882010193201015\n",
            "Epoch: 294 Loss: 27.82939937811213\n",
            "Epoch: 295 Loss: 27.798371542103947\n",
            "Epoch: 296 Loss: 27.74603573800579\n",
            "Epoch: 297 Loss: 27.688597628883034\n",
            "Epoch: 298 Loss: 27.627141029435503\n",
            "Epoch: 299 Loss: 27.56374343687408\n",
            "Epoch: 300 Loss: 27.506596507513507\n",
            "Epoch: 301 Loss: 27.461364635943504\n",
            "Epoch: 302 Loss: 27.43701431170933\n",
            "Epoch: 303 Loss: 27.392275265358904\n",
            "Epoch: 304 Loss: 27.33948212665821\n",
            "Epoch: 305 Loss: 27.317446870902902\n",
            "Epoch: 306 Loss: 27.26256355720986\n",
            "Epoch: 307 Loss: 27.20382463319811\n",
            "Epoch: 308 Loss: 27.144102373488106\n",
            "Epoch: 309 Loss: 27.083709789723144\n",
            "Epoch: 310 Loss: 27.0229722684132\n",
            "Epoch: 311 Loss: 26.96376671829286\n",
            "Epoch: 312 Loss: 26.912831212126513\n",
            "Epoch: 313 Loss: 26.89353070026637\n",
            "Epoch: 314 Loss: 26.91471254962464\n",
            "Epoch: 315 Loss: 26.873278508489303\n",
            "Epoch: 316 Loss: 26.8234676756776\n",
            "Epoch: 317 Loss: 26.77038054772131\n",
            "Epoch: 318 Loss: 26.715337779221805\n",
            "Epoch: 319 Loss: 26.66402625967908\n",
            "Epoch: 320 Loss: 26.60832102039251\n",
            "Epoch: 321 Loss: 26.551254385661565\n",
            "Epoch: 322 Loss: 26.49388229593487\n",
            "Epoch: 323 Loss: 26.437611034881378\n",
            "Epoch: 324 Loss: 26.387417792965667\n",
            "Epoch: 325 Loss: 26.35663882670705\n",
            "Epoch: 326 Loss: 26.354626382123865\n",
            "Epoch: 327 Loss: 26.306754568546044\n",
            "Epoch: 328 Loss: 26.25010797257168\n",
            "Epoch: 329 Loss: 26.19185595232837\n",
            "Epoch: 330 Loss: 26.133119102744136\n",
            "Epoch: 331 Loss: 26.074504293184418\n",
            "Epoch: 332 Loss: 26.016372993257555\n",
            "Epoch: 333 Loss: 25.960511924137325\n",
            "Epoch: 334 Loss: 25.917191132109735\n",
            "Epoch: 335 Loss: 25.884675791405183\n",
            "Epoch: 336 Loss: 25.850537322080733\n",
            "Epoch: 337 Loss: 25.864563802937752\n",
            "Epoch: 338 Loss: 25.833396494745866\n",
            "Epoch: 339 Loss: 25.783792366895366\n",
            "Epoch: 340 Loss: 25.73173005665904\n",
            "Epoch: 341 Loss: 25.67817319409334\n",
            "Epoch: 342 Loss: 25.623915438768496\n",
            "Epoch: 343 Loss: 25.569402269427204\n",
            "Epoch: 344 Loss: 25.54893473253377\n",
            "Epoch: 345 Loss: 25.53418264497725\n",
            "Epoch: 346 Loss: 25.500938330278245\n",
            "Epoch: 347 Loss: 25.464939187328763\n",
            "Epoch: 348 Loss: 25.426372040375075\n",
            "Epoch: 349 Loss: 25.385922093677024\n",
            "Epoch: 350 Loss: 25.344413043535237\n",
            "Epoch: 351 Loss: 25.302846161089317\n",
            "Epoch: 352 Loss: 25.26253669242881\n",
            "Epoch: 353 Loss: 25.233432635247166\n",
            "Epoch: 354 Loss: 25.20026344551653\n",
            "Epoch: 355 Loss: 25.16345435579635\n",
            "Epoch: 356 Loss: 25.119917739316048\n",
            "Epoch: 357 Loss: 25.092921799754528\n",
            "Epoch: 358 Loss: 25.061461024396852\n",
            "Epoch: 359 Loss: 25.01855613303262\n",
            "Epoch: 360 Loss: 24.972341510188457\n",
            "Epoch: 361 Loss: 24.924385753036184\n",
            "Epoch: 362 Loss: 24.875329147079523\n",
            "Epoch: 363 Loss: 24.825714884859245\n",
            "Epoch: 364 Loss: 24.775813055397112\n",
            "Epoch: 365 Loss: 24.726019192376715\n",
            "Epoch: 366 Loss: 24.677401618528553\n",
            "Epoch: 367 Loss: 24.64368192942184\n",
            "Epoch: 368 Loss: 24.624680216670765\n",
            "Epoch: 369 Loss: 24.586519256269455\n",
            "Epoch: 370 Loss: 24.545228519809083\n",
            "Epoch: 371 Loss: 24.501494841461007\n",
            "Epoch: 372 Loss: 24.456050971044256\n",
            "Epoch: 373 Loss: 24.409458130641582\n",
            "Epoch: 374 Loss: 24.362447056934233\n",
            "Epoch: 375 Loss: 24.31569464714959\n",
            "Epoch: 376 Loss: 24.270472814104618\n",
            "Epoch: 377 Loss: 24.246448891544897\n",
            "Epoch: 378 Loss: 24.25062335624875\n",
            "Epoch: 379 Loss: 24.238209344393905\n",
            "Epoch: 380 Loss: 24.202990978831238\n",
            "Epoch: 381 Loss: 24.166711359100667\n",
            "Epoch: 382 Loss: 24.136606032225227\n",
            "Epoch: 383 Loss: 24.09390344871907\n",
            "Epoch: 384 Loss: 24.050349030019216\n",
            "Epoch: 385 Loss: 24.006130566670297\n",
            "Epoch: 386 Loss: 23.961454454691737\n",
            "Epoch: 387 Loss: 23.916627048219528\n",
            "Epoch: 388 Loss: 23.872061879679116\n",
            "Epoch: 389 Loss: 23.829292569828713\n",
            "Epoch: 390 Loss: 23.79887676603299\n",
            "Epoch: 391 Loss: 23.798734352435318\n",
            "Epoch: 392 Loss: 23.789896799779914\n",
            "Epoch: 393 Loss: 23.750276953181167\n",
            "Epoch: 394 Loss: 23.708940043180093\n",
            "Epoch: 395 Loss: 23.66642618659365\n",
            "Epoch: 396 Loss: 23.623240713827112\n",
            "Epoch: 397 Loss: 23.579772102938474\n",
            "Epoch: 398 Loss: 23.53633815062844\n",
            "Epoch: 399 Loss: 23.4936207407572\n",
            "Epoch: 400 Loss: 23.45551993970069\n",
            "Epoch: 401 Loss: 23.44500517836507\n",
            "Epoch: 402 Loss: 23.439611259327577\n",
            "Epoch: 403 Loss: 23.41698942939317\n",
            "Epoch: 404 Loss: 23.386376296226086\n",
            "Epoch: 405 Loss: 23.355164838041517\n",
            "Epoch: 406 Loss: 23.395336224807345\n",
            "Epoch: 407 Loss: 23.354412148888525\n",
            "Epoch: 408 Loss: 23.313020683686965\n",
            "Epoch: 409 Loss: 23.271393575876072\n",
            "Epoch: 410 Loss: 23.229710699296774\n",
            "Epoch: 411 Loss: 23.188054428503683\n",
            "Epoch: 412 Loss: 23.146540782963456\n",
            "Epoch: 413 Loss: 23.105362455584782\n",
            "Epoch: 414 Loss: 23.065157688639054\n",
            "Epoch: 415 Loss: 23.029833682930942\n",
            "Epoch: 416 Loss: 23.002359771178856\n",
            "Epoch: 417 Loss: 22.984787951762524\n",
            "Epoch: 418 Loss: 22.959100093564192\n",
            "Epoch: 419 Loss: 22.931859091748453\n",
            "Epoch: 420 Loss: 22.89652547814164\n",
            "Epoch: 421 Loss: 22.857093959782873\n",
            "Epoch: 422 Loss: 22.81738186067236\n",
            "Epoch: 423 Loss: 22.777624431634763\n",
            "Epoch: 424 Loss: 22.738100668761426\n",
            "Epoch: 425 Loss: 22.703367819928907\n",
            "Epoch: 426 Loss: 22.727397807133205\n",
            "Epoch: 427 Loss: 22.747488612803213\n",
            "Epoch: 428 Loss: 22.745373986568232\n",
            "Epoch: 429 Loss: 22.71520512997993\n",
            "Epoch: 430 Loss: 22.689449301197413\n",
            "Epoch: 431 Loss: 22.71457279211148\n",
            "Epoch: 432 Loss: 22.678098318028376\n",
            "Epoch: 433 Loss: 22.641097963694033\n",
            "Epoch: 434 Loss: 22.603701963581745\n",
            "Epoch: 435 Loss: 22.566003953917683\n",
            "Epoch: 436 Loss: 22.528148310741116\n",
            "Epoch: 437 Loss: 22.4902215952978\n",
            "Epoch: 438 Loss: 22.45241421288792\n",
            "Epoch: 439 Loss: 22.415138134853795\n",
            "Epoch: 440 Loss: 22.38168513224523\n",
            "Epoch: 441 Loss: 22.355424910140236\n",
            "Epoch: 442 Loss: 22.335130306659927\n",
            "Epoch: 443 Loss: 22.313774000860263\n",
            "Epoch: 444 Loss: 22.291148220781245\n",
            "Epoch: 445 Loss: 22.26291669063592\n",
            "Epoch: 446 Loss: 22.24734159339922\n",
            "Epoch: 447 Loss: 22.231212423226985\n",
            "Epoch: 448 Loss: 22.206806071522045\n",
            "Epoch: 449 Loss: 22.17248846706329\n",
            "Epoch: 450 Loss: 22.14355718765018\n",
            "Epoch: 451 Loss: 22.114115656039957\n",
            "Epoch: 452 Loss: 22.083254008646193\n",
            "Epoch: 453 Loss: 22.04778643175851\n",
            "Epoch: 454 Loss: 22.01216713292505\n",
            "Epoch: 455 Loss: 21.97646424168818\n",
            "Epoch: 456 Loss: 21.940765715290123\n",
            "Epoch: 457 Loss: 21.90514085404176\n",
            "Epoch: 458 Loss: 21.869846934895413\n",
            "Epoch: 459 Loss: 21.836266596339318\n",
            "Epoch: 460 Loss: 21.806880233252492\n",
            "Epoch: 461 Loss: 21.783523674666263\n",
            "Epoch: 462 Loss: 21.76321560767559\n",
            "Epoch: 463 Loss: 21.737160596376942\n",
            "Epoch: 464 Loss: 21.739238686775142\n",
            "Epoch: 465 Loss: 21.721362581396594\n",
            "Epoch: 466 Loss: 21.70276638087252\n",
            "Epoch: 467 Loss: 21.683677890622633\n",
            "Epoch: 468 Loss: 21.66422038256666\n",
            "Epoch: 469 Loss: 21.644536317357336\n",
            "Epoch: 470 Loss: 21.624298447954555\n",
            "Epoch: 471 Loss: 21.59518375481221\n",
            "Epoch: 472 Loss: 21.562440599767875\n",
            "Epoch: 473 Loss: 21.52949903181174\n",
            "Epoch: 474 Loss: 21.496406431445166\n",
            "Epoch: 475 Loss: 21.463422689505457\n",
            "Epoch: 476 Loss: 21.430844375758035\n",
            "Epoch: 477 Loss: 21.400070421105763\n",
            "Epoch: 478 Loss: 21.382989084010468\n",
            "Epoch: 479 Loss: 21.364924807733267\n",
            "Epoch: 480 Loss: 21.33846094957886\n",
            "Epoch: 481 Loss: 21.307594303561146\n",
            "Epoch: 482 Loss: 21.27599057455978\n",
            "Epoch: 483 Loss: 21.24401684842926\n",
            "Epoch: 484 Loss: 21.211843580900997\n",
            "Epoch: 485 Loss: 21.179744410725203\n",
            "Epoch: 486 Loss: 21.151271843196497\n",
            "Epoch: 487 Loss: 21.155547674103047\n",
            "Epoch: 488 Loss: 21.138323630594286\n",
            "Epoch: 489 Loss: 21.11070287110819\n",
            "Epoch: 490 Loss: 21.082194536808007\n",
            "Epoch: 491 Loss: 21.05300518140118\n",
            "Epoch: 492 Loss: 21.023385353804393\n",
            "Epoch: 493 Loss: 20.993616633844344\n",
            "Epoch: 494 Loss: 20.9713749367258\n",
            "Epoch: 495 Loss: 20.942262698359393\n",
            "Epoch: 496 Loss: 20.912545373244146\n",
            "Epoch: 497 Loss: 20.882467960459262\n",
            "Epoch: 498 Loss: 20.852109554812323\n",
            "Epoch: 499 Loss: 20.821553741441775\n",
            "Epoch: 500 Loss: 20.79090624293065\n",
            "Epoch: 501 Loss: 20.76030120544071\n",
            "Epoch: 502 Loss: 20.73761609203606\n",
            "Epoch: 503 Loss: 20.72368803528904\n",
            "Epoch: 504 Loss: 20.69632861496449\n",
            "Epoch: 505 Loss: 20.668788486734847\n",
            "Epoch: 506 Loss: 20.64104778621718\n",
            "Epoch: 507 Loss: 20.61327817753796\n",
            "Epoch: 508 Loss: 20.585707134142883\n",
            "Epoch: 509 Loss: 20.559111143357857\n",
            "Epoch: 510 Loss: 20.534214867434937\n",
            "Epoch: 511 Loss: 20.509500615906177\n",
            "Epoch: 512 Loss: 20.494220098237122\n",
            "Epoch: 513 Loss: 20.489366255735803\n",
            "Epoch: 514 Loss: 20.46672721833634\n",
            "Epoch: 515 Loss: 20.443311010474453\n",
            "Epoch: 516 Loss: 20.414046101987655\n",
            "Epoch: 517 Loss: 20.38455176704608\n",
            "Epoch: 518 Loss: 20.354974754276395\n",
            "Epoch: 519 Loss: 20.325498078882827\n",
            "Epoch: 520 Loss: 20.296631747938836\n",
            "Epoch: 521 Loss: 20.295697294966324\n",
            "Epoch: 522 Loss: 20.28565859055873\n",
            "Epoch: 523 Loss: 20.267408411035817\n",
            "Epoch: 524 Loss: 20.246227709378385\n",
            "Epoch: 525 Loss: 20.221542934134195\n",
            "Epoch: 526 Loss: 20.19660922555066\n",
            "Epoch: 527 Loss: 20.171466977438637\n",
            "Epoch: 528 Loss: 20.1462101104799\n",
            "Epoch: 529 Loss: 20.120923437692195\n",
            "Epoch: 530 Loss: 20.09872848428761\n",
            "Epoch: 531 Loss: 20.082800768859084\n",
            "Epoch: 532 Loss: 20.059911393934545\n",
            "Epoch: 533 Loss: 20.036503959505318\n",
            "Epoch: 534 Loss: 20.012622946028554\n",
            "Epoch: 535 Loss: 19.988374675938882\n",
            "Epoch: 536 Loss: 19.96384515821461\n",
            "Epoch: 537 Loss: 19.94180635828519\n",
            "Epoch: 538 Loss: 19.922514737572737\n",
            "Epoch: 539 Loss: 19.90427840931458\n",
            "Epoch: 540 Loss: 19.881804985942203\n",
            "Epoch: 541 Loss: 19.855945215635867\n",
            "Epoch: 542 Loss: 19.829631508350808\n",
            "Epoch: 543 Loss: 19.802941284040056\n",
            "Epoch: 544 Loss: 19.776044706486363\n",
            "Epoch: 545 Loss: 19.74969950566421\n",
            "Epoch: 546 Loss: 19.749344197867103\n",
            "Epoch: 547 Loss: 19.733963698923958\n",
            "Epoch: 548 Loss: 19.71018380399994\n",
            "Epoch: 549 Loss: 19.686238498076126\n",
            "Epoch: 550 Loss: 19.662144534149945\n",
            "Epoch: 551 Loss: 19.637960295205584\n",
            "Epoch: 552 Loss: 19.6137397072244\n",
            "Epoch: 553 Loss: 19.589506577407285\n",
            "Epoch: 554 Loss: 19.565328595982987\n",
            "Epoch: 555 Loss: 19.54494688054009\n",
            "Epoch: 556 Loss: 19.546052222269708\n",
            "Epoch: 557 Loss: 19.548058045742174\n",
            "Epoch: 558 Loss: 19.54948319195469\n",
            "Epoch: 559 Loss: 19.550412390004624\n",
            "Epoch: 560 Loss: 19.55103742656131\n",
            "Epoch: 561 Loss: 19.543018815285848\n",
            "Epoch: 562 Loss: 19.538775125786565\n",
            "Epoch: 563 Loss: 19.521282731355722\n",
            "Epoch: 564 Loss: 19.498306123553746\n",
            "Epoch: 565 Loss: 19.475083919227654\n",
            "Epoch: 566 Loss: 19.45430029062668\n",
            "Epoch: 567 Loss: 19.428911853713593\n",
            "Epoch: 568 Loss: 19.40336277094833\n",
            "Epoch: 569 Loss: 19.377740976082812\n",
            "Epoch: 570 Loss: 19.352046365486544\n",
            "Epoch: 571 Loss: 19.32631266566838\n",
            "Epoch: 572 Loss: 19.300640188954247\n",
            "Epoch: 573 Loss: 19.275097847387837\n",
            "Epoch: 574 Loss: 19.249959212547214\n",
            "Epoch: 575 Loss: 19.228245929120284\n",
            "Epoch: 576 Loss: 19.204996786471472\n",
            "Epoch: 577 Loss: 19.18150269963289\n",
            "Epoch: 578 Loss: 19.16814792982233\n",
            "Epoch: 579 Loss: 19.143602330890126\n",
            "Epoch: 580 Loss: 19.11879608915261\n",
            "Epoch: 581 Loss: 19.093782333852886\n",
            "Epoch: 582 Loss: 19.0686673550914\n",
            "Epoch: 583 Loss: 19.043505781889706\n",
            "Epoch: 584 Loss: 19.018386002676724\n",
            "Epoch: 585 Loss: 18.993307271884106\n",
            "Epoch: 586 Loss: 18.971945340692198\n",
            "Epoch: 587 Loss: 18.95525301400048\n",
            "Epoch: 588 Loss: 18.933066178350668\n",
            "Epoch: 589 Loss: 18.912596640063054\n",
            "Epoch: 590 Loss: 18.889120922031708\n",
            "Epoch: 591 Loss: 18.859992285557407\n",
            "Epoch: 592 Loss: 18.829954486100984\n",
            "Epoch: 593 Loss: 18.799577796144693\n",
            "Epoch: 594 Loss: 18.768989261867727\n",
            "Epoch: 595 Loss: 18.73829774237022\n",
            "Epoch: 596 Loss: 18.70756890350872\n",
            "Epoch: 597 Loss: 18.676921251619778\n",
            "Epoch: 598 Loss: 18.646676367816582\n",
            "Epoch: 599 Loss: 18.62426921068013\n",
            "Epoch: 600 Loss: 18.600256930323685\n",
            "Epoch: 601 Loss: 18.576136699068474\n",
            "Epoch: 602 Loss: 18.55014767677331\n",
            "Epoch: 603 Loss: 18.523509450418263\n",
            "Epoch: 604 Loss: 18.496613799919402\n",
            "Epoch: 605 Loss: 18.467068956958514\n",
            "Epoch: 606 Loss: 18.43738797985083\n",
            "Epoch: 607 Loss: 18.407677292735542\n",
            "Epoch: 608 Loss: 18.377907130291394\n",
            "Epoch: 609 Loss: 18.348173136949796\n",
            "Epoch: 610 Loss: 18.318653191974143\n",
            "Epoch: 611 Loss: 18.289927053666148\n",
            "Epoch: 612 Loss: 18.264153025258334\n",
            "Epoch: 613 Loss: 18.24052980639799\n",
            "Epoch: 614 Loss: 18.213801626566223\n",
            "Epoch: 615 Loss: 18.18479503220775\n",
            "Epoch: 616 Loss: 18.161542268826427\n",
            "Epoch: 617 Loss: 18.13503755683957\n",
            "Epoch: 618 Loss: 18.10614388532464\n",
            "Epoch: 619 Loss: 18.077220298079677\n",
            "Epoch: 620 Loss: 18.04855684599303\n",
            "Epoch: 621 Loss: 18.020533339809944\n",
            "Epoch: 622 Loss: 17.99415395131025\n",
            "Epoch: 623 Loss: 17.96617036587843\n",
            "Epoch: 624 Loss: 17.938017235856556\n",
            "Epoch: 625 Loss: 17.909791756728588\n",
            "Epoch: 626 Loss: 17.881546736171313\n",
            "Epoch: 627 Loss: 17.853332774705038\n",
            "Epoch: 628 Loss: 17.82518365774536\n",
            "Epoch: 629 Loss: 17.797215685206435\n",
            "Epoch: 630 Loss: 17.775060601925972\n",
            "Epoch: 631 Loss: 17.74764360911812\n",
            "Epoch: 632 Loss: 17.720044892117787\n",
            "Epoch: 633 Loss: 17.69237710798889\n",
            "Epoch: 634 Loss: 17.664719602733452\n",
            "Epoch: 635 Loss: 17.637121989261534\n",
            "Epoch: 636 Loss: 17.609789782656076\n",
            "Epoch: 637 Loss: 17.582990115051615\n",
            "Epoch: 638 Loss: 17.55613841559594\n",
            "Epoch: 639 Loss: 17.52919935271585\n",
            "Epoch: 640 Loss: 17.502470718292948\n",
            "Epoch: 641 Loss: 17.47821372551583\n",
            "Epoch: 642 Loss: 17.46629015031777\n",
            "Epoch: 643 Loss: 17.439972548282594\n",
            "Epoch: 644 Loss: 17.413447639285312\n",
            "Epoch: 645 Loss: 17.38684363435101\n",
            "Epoch: 646 Loss: 17.360224106557403\n",
            "Epoch: 647 Loss: 17.33362650865172\n",
            "Epoch: 648 Loss: 17.3070893779312\n",
            "Epoch: 649 Loss: 17.28068304485432\n",
            "Epoch: 650 Loss: 17.25464167683954\n",
            "Epoch: 651 Loss: 17.23111883991218\n",
            "Epoch: 652 Loss: 17.2139576432718\n",
            "Epoch: 653 Loss: 17.19694894594607\n",
            "Epoch: 654 Loss: 17.175547676544962\n",
            "Epoch: 655 Loss: 17.154018707854075\n",
            "Epoch: 656 Loss: 17.137018840968555\n",
            "Epoch: 657 Loss: 17.132386022328408\n",
            "Epoch: 658 Loss: 17.113004807841044\n",
            "Epoch: 659 Loss: 17.08748730811418\n",
            "Epoch: 660 Loss: 17.06194988105391\n",
            "Epoch: 661 Loss: 17.03642935221562\n",
            "Epoch: 662 Loss: 17.01094928704742\n",
            "Epoch: 663 Loss: 16.985506872788264\n",
            "Epoch: 664 Loss: 16.960136099112827\n",
            "Epoch: 665 Loss: 16.934850207655806\n",
            "Epoch: 666 Loss: 16.90969210094733\n",
            "Epoch: 667 Loss: 16.884832178607994\n",
            "Epoch: 668 Loss: 16.86018289619226\n",
            "Epoch: 669 Loss: 16.846411516733156\n",
            "Epoch: 670 Loss: 16.83401365681628\n",
            "Epoch: 671 Loss: 16.813525592941478\n",
            "Epoch: 672 Loss: 16.79110222196265\n",
            "Epoch: 673 Loss: 16.768525261889618\n",
            "Epoch: 674 Loss: 16.744047255709827\n",
            "Epoch: 675 Loss: 16.7195382569271\n",
            "Epoch: 676 Loss: 16.69503996662464\n",
            "Epoch: 677 Loss: 16.670593448743375\n",
            "Epoch: 678 Loss: 16.646233236315798\n",
            "Epoch: 679 Loss: 16.622077934491713\n",
            "Epoch: 680 Loss: 16.59798730254635\n",
            "Epoch: 681 Loss: 16.573963340545355\n",
            "Epoch: 682 Loss: 16.55023170169196\n",
            "Epoch: 683 Loss: 16.5266592636633\n",
            "Epoch: 684 Loss: 16.503088968001965\n",
            "Epoch: 685 Loss: 16.479565026298783\n",
            "Epoch: 686 Loss: 16.456225077417475\n",
            "Epoch: 687 Loss: 16.432670828850895\n",
            "Epoch: 688 Loss: 16.4090774800914\n",
            "Epoch: 689 Loss: 16.385486510872468\n",
            "Epoch: 690 Loss: 16.3619343746765\n",
            "Epoch: 691 Loss: 16.340550728841265\n",
            "Epoch: 692 Loss: 16.321467445378456\n",
            "Epoch: 693 Loss: 16.307986059672835\n",
            "Epoch: 694 Loss: 16.29560495412786\n",
            "Epoch: 695 Loss: 16.27493285373009\n",
            "Epoch: 696 Loss: 16.254190660262122\n",
            "Epoch: 697 Loss: 16.23344284572507\n",
            "Epoch: 698 Loss: 16.212674296757754\n",
            "Epoch: 699 Loss: 16.191881277481524\n",
            "Epoch: 700 Loss: 16.168999251120923\n",
            "Epoch: 701 Loss: 16.14612814748785\n",
            "Epoch: 702 Loss: 16.12328618257722\n",
            "Epoch: 703 Loss: 16.100498876091486\n",
            "Epoch: 704 Loss: 16.077782076767367\n",
            "Epoch: 705 Loss: 16.055241853521135\n",
            "Epoch: 706 Loss: 16.04205761291619\n",
            "Epoch: 707 Loss: 16.0237955938139\n",
            "Epoch: 708 Loss: 16.00148231656202\n",
            "Epoch: 709 Loss: 15.979127890043378\n",
            "Epoch: 710 Loss: 15.956785871119576\n",
            "Epoch: 711 Loss: 15.934476958950146\n",
            "Epoch: 712 Loss: 15.912232322017255\n",
            "Epoch: 713 Loss: 15.890120432114806\n",
            "Epoch: 714 Loss: 15.88064676777143\n",
            "Epoch: 715 Loss: 15.86465773129304\n",
            "Epoch: 716 Loss: 15.845702695934236\n",
            "Epoch: 717 Loss: 15.849306983787768\n",
            "Epoch: 718 Loss: 15.827921972623747\n",
            "Epoch: 719 Loss: 15.806435358944928\n",
            "Epoch: 720 Loss: 15.78493502564802\n",
            "Epoch: 721 Loss: 15.763422218921074\n",
            "Epoch: 722 Loss: 15.74191396915477\n",
            "Epoch: 723 Loss: 15.720407879449795\n",
            "Epoch: 724 Loss: 15.698910103088037\n",
            "Epoch: 725 Loss: 15.677444204953813\n",
            "Epoch: 726 Loss: 15.656028551816904\n",
            "Epoch: 727 Loss: 15.634776591345242\n",
            "Epoch: 728 Loss: 15.613800711428569\n",
            "Epoch: 729 Loss: 15.597197698499956\n",
            "Epoch: 730 Loss: 15.586420422798037\n",
            "Epoch: 731 Loss: 15.573352545533886\n",
            "Epoch: 732 Loss: 15.556375849047745\n",
            "Epoch: 733 Loss: 15.537451897395945\n",
            "Epoch: 734 Loss: 15.518546530259131\n",
            "Epoch: 735 Loss: 15.501709007454789\n",
            "Epoch: 736 Loss: 15.484955437846088\n",
            "Epoch: 737 Loss: 15.474184669306485\n",
            "Epoch: 738 Loss: 15.463368811467664\n",
            "Epoch: 739 Loss: 15.445084902028217\n",
            "Epoch: 740 Loss: 15.426701363228096\n",
            "Epoch: 741 Loss: 15.408278984377304\n",
            "Epoch: 742 Loss: 15.387914231043343\n",
            "Epoch: 743 Loss: 15.367553922727717\n",
            "Epoch: 744 Loss: 15.34720661605952\n",
            "Epoch: 745 Loss: 15.3268772550987\n",
            "Epoch: 746 Loss: 15.306569375901095\n",
            "Epoch: 747 Loss: 15.2862957117385\n",
            "Epoch: 748 Loss: 15.266053517748203\n",
            "Epoch: 749 Loss: 15.245904145321287\n",
            "Epoch: 750 Loss: 15.225940480313161\n",
            "Epoch: 751 Loss: 15.206228770199258\n",
            "Epoch: 752 Loss: 15.188624582098395\n",
            "Epoch: 753 Loss: 15.172320907390201\n",
            "Epoch: 754 Loss: 15.153350701843758\n",
            "Epoch: 755 Loss: 15.133932896929446\n",
            "Epoch: 756 Loss: 15.114407342732958\n",
            "Epoch: 757 Loss: 15.094828644573637\n",
            "Epoch: 758 Loss: 15.075238338890033\n",
            "Epoch: 759 Loss: 15.055653270185411\n",
            "Epoch: 760 Loss: 15.036094320595057\n",
            "Epoch: 761 Loss: 15.016554346675083\n",
            "Epoch: 762 Loss: 14.997043967078072\n",
            "Epoch: 763 Loss: 14.980195458578002\n",
            "Epoch: 764 Loss: 14.972256460994938\n",
            "Epoch: 765 Loss: 14.955120035561736\n",
            "Epoch: 766 Loss: 14.937889356772816\n",
            "Epoch: 767 Loss: 14.920645637822572\n",
            "Epoch: 768 Loss: 14.90339423796479\n",
            "Epoch: 769 Loss: 14.886150091598791\n",
            "Epoch: 770 Loss: 14.86892658170477\n",
            "Epoch: 771 Loss: 14.851767886289222\n",
            "Epoch: 772 Loss: 14.834899548466824\n",
            "Epoch: 773 Loss: 14.820183593616878\n",
            "Epoch: 774 Loss: 14.803184454802938\n",
            "Epoch: 775 Loss: 14.784369308007944\n",
            "Epoch: 776 Loss: 14.76554310786097\n",
            "Epoch: 777 Loss: 14.746717189466402\n",
            "Epoch: 778 Loss: 14.727917779441917\n",
            "Epoch: 779 Loss: 14.709171971870683\n",
            "Epoch: 780 Loss: 14.690564272299234\n",
            "Epoch: 781 Loss: 14.672114699258605\n",
            "Epoch: 782 Loss: 14.657435530227513\n",
            "Epoch: 783 Loss: 14.644689196193905\n",
            "Epoch: 784 Loss: 14.647609320244527\n",
            "Epoch: 785 Loss: 14.6369807083247\n",
            "Epoch: 786 Loss: 14.620846533533813\n",
            "Epoch: 787 Loss: 14.604653772276128\n",
            "Epoch: 788 Loss: 14.588432441239224\n",
            "Epoch: 789 Loss: 14.573864305825943\n",
            "Epoch: 790 Loss: 14.559270285309495\n",
            "Epoch: 791 Loss: 14.544660333705014\n",
            "Epoch: 792 Loss: 14.53004064007868\n",
            "Epoch: 793 Loss: 14.515428621838085\n",
            "Epoch: 794 Loss: 14.500834031722038\n",
            "Epoch: 795 Loss: 14.486362927015234\n",
            "Epoch: 796 Loss: 14.478274004806964\n",
            "Epoch: 797 Loss: 14.46417008441659\n",
            "Epoch: 798 Loss: 14.450002302187771\n",
            "Epoch: 799 Loss: 14.435798689407209\n",
            "Epoch: 800 Loss: 14.420208752090055\n",
            "Epoch: 801 Loss: 14.402850612851008\n",
            "Epoch: 802 Loss: 14.385356496428376\n",
            "Epoch: 803 Loss: 14.367814715181078\n",
            "Epoch: 804 Loss: 14.350249816591203\n",
            "Epoch: 805 Loss: 14.332672502375129\n",
            "Epoch: 806 Loss: 14.315091690473901\n",
            "Epoch: 807 Loss: 14.297519328811807\n",
            "Epoch: 808 Loss: 14.301259802262404\n",
            "Epoch: 809 Loss: 14.287415935979329\n",
            "Epoch: 810 Loss: 14.27355195445642\n",
            "Epoch: 811 Loss: 14.259691414280729\n",
            "Epoch: 812 Loss: 14.245837531730317\n",
            "Epoch: 813 Loss: 14.231996407134325\n",
            "Epoch: 814 Loss: 14.218164862950626\n",
            "Epoch: 815 Loss: 14.204342316620009\n",
            "Epoch: 816 Loss: 14.190552883941379\n",
            "Epoch: 817 Loss: 14.176762122139378\n",
            "Epoch: 818 Loss: 14.164868667042672\n",
            "Epoch: 819 Loss: 14.147785231373481\n",
            "Epoch: 820 Loss: 14.130721133230926\n",
            "Epoch: 821 Loss: 14.113761775595307\n",
            "Epoch: 822 Loss: 14.096993370189612\n",
            "Epoch: 823 Loss: 14.092419029064176\n",
            "Epoch: 824 Loss: 14.077395463215163\n",
            "Epoch: 825 Loss: 14.074266792155449\n",
            "Epoch: 826 Loss: 14.059667415448201\n",
            "Epoch: 827 Loss: 14.044832280911459\n",
            "Epoch: 828 Loss: 14.029935688828195\n",
            "Epoch: 829 Loss: 14.015015834516769\n",
            "Epoch: 830 Loss: 14.000093026622482\n",
            "Epoch: 831 Loss: 13.985206018300964\n",
            "Epoch: 832 Loss: 13.968590615479311\n",
            "Epoch: 833 Loss: 13.95198577528294\n",
            "Epoch: 834 Loss: 13.935405363212423\n",
            "Epoch: 835 Loss: 13.918905613773472\n",
            "Epoch: 836 Loss: 13.903235157958227\n",
            "Epoch: 837 Loss: 13.890744641157617\n",
            "Epoch: 838 Loss: 13.874546537968783\n",
            "Epoch: 839 Loss: 13.858314941311809\n",
            "Epoch: 840 Loss: 13.842069029481692\n",
            "Epoch: 841 Loss: 13.82582043102075\n",
            "Epoch: 842 Loss: 13.811410433522774\n",
            "Epoch: 843 Loss: 13.795240088882407\n",
            "Epoch: 844 Loss: 13.780853447411985\n",
            "Epoch: 845 Loss: 13.764724829179713\n",
            "Epoch: 846 Loss: 13.750427630603074\n",
            "Epoch: 847 Loss: 13.736217649049067\n",
            "Epoch: 848 Loss: 13.72702203121576\n",
            "Epoch: 849 Loss: 13.719384167508233\n",
            "Epoch: 850 Loss: 13.714658863077391\n",
            "Epoch: 851 Loss: 13.705320948049282\n",
            "Epoch: 852 Loss: 13.689576001974238\n",
            "Epoch: 853 Loss: 13.673759569042478\n",
            "Epoch: 854 Loss: 13.657926421164571\n",
            "Epoch: 855 Loss: 13.642102898441737\n",
            "Epoch: 856 Loss: 13.626301274024152\n",
            "Epoch: 857 Loss: 13.610533650822244\n",
            "Epoch: 858 Loss: 13.596618884368546\n",
            "Epoch: 859 Loss: 13.586062178577453\n",
            "Epoch: 860 Loss: 13.581887059582762\n",
            "Epoch: 861 Loss: 13.58722766171715\n",
            "Epoch: 862 Loss: 13.581943666107188\n",
            "Epoch: 863 Loss: 13.576506987385367\n",
            "Epoch: 864 Loss: 13.571005285814838\n",
            "Epoch: 865 Loss: 13.56264492215859\n",
            "Epoch: 866 Loss: 13.554260097504292\n",
            "Epoch: 867 Loss: 13.543982550747778\n",
            "Epoch: 868 Loss: 13.537924676516429\n",
            "Epoch: 869 Loss: 13.527063088174835\n",
            "Epoch: 870 Loss: 13.515422183002519\n",
            "Epoch: 871 Loss: 13.503686443658024\n",
            "Epoch: 872 Loss: 13.491901167657085\n",
            "Epoch: 873 Loss: 13.48007065654729\n",
            "Epoch: 874 Loss: 13.468206389958807\n",
            "Epoch: 875 Loss: 13.456318826665788\n",
            "Epoch: 876 Loss: 13.446107352256668\n",
            "Epoch: 877 Loss: 13.43493401688877\n",
            "Epoch: 878 Loss: 13.42171188698479\n",
            "Epoch: 879 Loss: 13.407156385697894\n",
            "Epoch: 880 Loss: 13.392263655925008\n",
            "Epoch: 881 Loss: 13.377289299822236\n",
            "Epoch: 882 Loss: 13.362304959148007\n",
            "Epoch: 883 Loss: 13.34732727341693\n",
            "Epoch: 884 Loss: 13.332365204262892\n",
            "Epoch: 885 Loss: 13.317457860716718\n",
            "Epoch: 886 Loss: 13.30444234412694\n",
            "Epoch: 887 Loss: 13.294230023934078\n",
            "Epoch: 888 Loss: 13.280945383121315\n",
            "Epoch: 889 Loss: 13.266498685757046\n",
            "Epoch: 890 Loss: 13.251859390715019\n",
            "Epoch: 891 Loss: 13.237169018784774\n",
            "Epoch: 892 Loss: 13.22246335280415\n",
            "Epoch: 893 Loss: 13.207775746489645\n",
            "Epoch: 894 Loss: 13.19318097056509\n",
            "Epoch: 895 Loss: 13.180518464316265\n",
            "Epoch: 896 Loss: 13.167607502401577\n",
            "Epoch: 897 Loss: 13.15328781945608\n",
            "Epoch: 898 Loss: 13.13889043213482\n",
            "Epoch: 899 Loss: 13.124449003331588\n",
            "Epoch: 900 Loss: 13.109994122699796\n",
            "Epoch: 901 Loss: 13.095553033621368\n",
            "Epoch: 902 Loss: 13.081181795136114\n",
            "Epoch: 903 Loss: 13.067056452670066\n",
            "Epoch: 904 Loss: 13.05314905273861\n",
            "Epoch: 905 Loss: 13.038987751967532\n",
            "Epoch: 906 Loss: 13.024807755330462\n",
            "Epoch: 907 Loss: 13.010812104207176\n",
            "Epoch: 908 Loss: 12.996972849224333\n",
            "Epoch: 909 Loss: 12.982971178037333\n",
            "Epoch: 910 Loss: 12.968890474052472\n",
            "Epoch: 911 Loss: 12.954818500471884\n",
            "Epoch: 912 Loss: 12.940842128640776\n",
            "Epoch: 913 Loss: 12.929621806711484\n",
            "Epoch: 914 Loss: 12.916142702908388\n",
            "Epoch: 915 Loss: 12.902371762160376\n",
            "Epoch: 916 Loss: 12.888522807396663\n",
            "Epoch: 917 Loss: 12.874641520159251\n",
            "Epoch: 918 Loss: 12.860745315605447\n",
            "Epoch: 919 Loss: 12.846863524964254\n",
            "Epoch: 920 Loss: 12.833061207618343\n",
            "Epoch: 921 Loss: 12.819542117454088\n",
            "Epoch: 922 Loss: 12.806119401304048\n",
            "Epoch: 923 Loss: 12.792896928279292\n",
            "Epoch: 924 Loss: 12.779480242260517\n",
            "Epoch: 925 Loss: 12.765851452501826\n",
            "Epoch: 926 Loss: 12.752188308289064\n",
            "Epoch: 927 Loss: 12.738530179750365\n",
            "Epoch: 928 Loss: 12.724947602741667\n",
            "Epoch: 929 Loss: 12.711548476694523\n",
            "Epoch: 930 Loss: 12.698716382030769\n",
            "Epoch: 931 Loss: 12.685399969372003\n",
            "Epoch: 932 Loss: 12.671995860912673\n",
            "Epoch: 933 Loss: 12.658573284827574\n",
            "Epoch: 934 Loss: 12.645188244263357\n",
            "Epoch: 935 Loss: 12.631812502016588\n",
            "Epoch: 936 Loss: 12.618464153400195\n",
            "Epoch: 937 Loss: 12.605195691909534\n",
            "Epoch: 938 Loss: 12.592192757613846\n",
            "Epoch: 939 Loss: 12.579066614373062\n",
            "Epoch: 940 Loss: 12.565832132399308\n",
            "Epoch: 941 Loss: 12.552594901121918\n",
            "Epoch: 942 Loss: 12.539421799163534\n",
            "Epoch: 943 Loss: 12.526953818478361\n",
            "Epoch: 944 Loss: 12.521330457715186\n",
            "Epoch: 945 Loss: 12.5086882996888\n",
            "Epoch: 946 Loss: 12.495741923727252\n",
            "Epoch: 947 Loss: 12.482782469559204\n",
            "Epoch: 948 Loss: 12.469807972937048\n",
            "Epoch: 949 Loss: 12.456827914577845\n",
            "Epoch: 950 Loss: 12.443849063402126\n",
            "Epoch: 951 Loss: 12.430879613390749\n",
            "Epoch: 952 Loss: 12.42004497659911\n",
            "Epoch: 953 Loss: 12.408166730673212\n",
            "Epoch: 954 Loss: 12.395391170815941\n",
            "Epoch: 955 Loss: 12.382603417090312\n",
            "Epoch: 956 Loss: 12.36980925468885\n",
            "Epoch: 957 Loss: 12.35701713367792\n",
            "Epoch: 958 Loss: 12.344234210804395\n",
            "Epoch: 959 Loss: 12.331500561368506\n",
            "Epoch: 960 Loss: 12.318771616778944\n",
            "Epoch: 961 Loss: 12.306070017599858\n",
            "Epoch: 962 Loss: 12.29344854659674\n",
            "Epoch: 963 Loss: 12.28115217455568\n",
            "Epoch: 964 Loss: 12.26879416564939\n",
            "Epoch: 965 Loss: 12.256291712639232\n",
            "Epoch: 966 Loss: 12.243746369371305\n",
            "Epoch: 967 Loss: 12.2311928365557\n",
            "Epoch: 968 Loss: 12.218659946323505\n",
            "Epoch: 969 Loss: 12.206267332565906\n",
            "Epoch: 970 Loss: 12.194016089254353\n",
            "Epoch: 971 Loss: 12.181762541940287\n",
            "Epoch: 972 Loss: 12.169467813507442\n",
            "Epoch: 973 Loss: 12.157577879656557\n",
            "Epoch: 974 Loss: 12.14528516095294\n",
            "Epoch: 975 Loss: 12.132931974531006\n",
            "Epoch: 976 Loss: 12.12057787416711\n",
            "Epoch: 977 Loss: 12.108251829419626\n",
            "Epoch: 978 Loss: 12.096007447966171\n",
            "Epoch: 979 Loss: 12.08877675728747\n",
            "Epoch: 980 Loss: 12.076948576133965\n",
            "Epoch: 981 Loss: 12.06499858049488\n",
            "Epoch: 982 Loss: 12.053018482043392\n",
            "Epoch: 983 Loss: 12.04101047582743\n",
            "Epoch: 984 Loss: 12.028983485838099\n",
            "Epoch: 985 Loss: 12.016941438465011\n",
            "Epoch: 986 Loss: 12.004890880283414\n",
            "Epoch: 987 Loss: 11.992836764106192\n",
            "Epoch: 988 Loss: 11.980785260934065\n",
            "Epoch: 989 Loss: 11.96875363192319\n",
            "Epoch: 990 Loss: 11.956797987300453\n",
            "Epoch: 991 Loss: 11.946829453782968\n",
            "Epoch: 992 Loss: 11.936705777068203\n",
            "Epoch: 993 Loss: 11.924953256813932\n",
            "Epoch: 994 Loss: 11.913168023244587\n",
            "Epoch: 995 Loss: 11.90136638789102\n",
            "Epoch: 996 Loss: 11.889557685599293\n",
            "Epoch: 997 Loss: 11.879447128775158\n",
            "Epoch: 998 Loss: 11.867747237372336\n",
            "Epoch: 999 Loss: 11.856296829834784\n",
            "Epoch: 1000 Loss: 11.844655321070125\n",
            "Epoch: 1001 Loss: 11.83297137227824\n",
            "Epoch: 1002 Loss: 11.821276300860537\n",
            "Epoch: 1003 Loss: 11.809585925139954\n",
            "Epoch: 1004 Loss: 11.79791231784625\n",
            "Epoch: 1005 Loss: 11.786264734921764\n",
            "Epoch: 1006 Loss: 11.774717210949726\n",
            "Epoch: 1007 Loss: 11.76334982605557\n",
            "Epoch: 1008 Loss: 11.752015759062994\n",
            "Epoch: 1009 Loss: 11.740585546449706\n",
            "Epoch: 1010 Loss: 11.729905474351645\n",
            "Epoch: 1011 Loss: 11.718557156437862\n",
            "Epoch: 1012 Loss: 11.70716542037428\n",
            "Epoch: 1013 Loss: 11.69575278301966\n",
            "Epoch: 1014 Loss: 11.684333232282956\n",
            "Epoch: 1015 Loss: 11.672913972800677\n",
            "Epoch: 1016 Loss: 11.661491476443434\n",
            "Epoch: 1017 Loss: 11.65010051736612\n",
            "Epoch: 1018 Loss: 11.643879399570944\n",
            "Epoch: 1019 Loss: 11.634874347122683\n",
            "Epoch: 1020 Loss: 11.627594093889833\n",
            "Epoch: 1021 Loss: 11.616538591319921\n",
            "Epoch: 1022 Loss: 11.605388436459037\n",
            "Epoch: 1023 Loss: 11.594233813938088\n",
            "Epoch: 1024 Loss: 11.5830725888534\n",
            "Epoch: 1025 Loss: 11.571909449497943\n",
            "Epoch: 1026 Loss: 11.560747198160332\n",
            "Epoch: 1027 Loss: 11.549587061956954\n",
            "Epoch: 1028 Loss: 11.538429668167215\n",
            "Epoch: 1029 Loss: 11.527285203776175\n",
            "Epoch: 1030 Loss: 11.516167213124975\n",
            "Epoch: 1031 Loss: 11.50510353965139\n",
            "Epoch: 1032 Loss: 11.49566366339449\n",
            "Epoch: 1033 Loss: 11.484791227145656\n",
            "Epoch: 1034 Loss: 11.473821516101406\n",
            "Epoch: 1035 Loss: 11.466496304503455\n",
            "Epoch: 1036 Loss: 11.45696798779546\n",
            "Epoch: 1037 Loss: 11.446031972561642\n",
            "Epoch: 1038 Loss: 11.435094069500199\n",
            "Epoch: 1039 Loss: 11.424164357361832\n",
            "Epoch: 1040 Loss: 11.413250223721585\n",
            "Epoch: 1041 Loss: 11.402355407506525\n",
            "Epoch: 1042 Loss: 11.391493757197688\n",
            "Epoch: 1043 Loss: 11.380707352409987\n",
            "Epoch: 1044 Loss: 11.375721611785277\n",
            "Epoch: 1045 Loss: 11.366390144687456\n",
            "Epoch: 1046 Loss: 11.355691298967521\n",
            "Epoch: 1047 Loss: 11.344971644276491\n",
            "Epoch: 1048 Loss: 11.334248915062282\n",
            "Epoch: 1049 Loss: 11.323525978500513\n",
            "Epoch: 1050 Loss: 11.312815244012864\n",
            "Epoch: 1051 Loss: 11.302131468008254\n",
            "Epoch: 1052 Loss: 11.29152716301879\n",
            "Epoch: 1053 Loss: 11.281025771891356\n",
            "Epoch: 1054 Loss: 11.270608704943802\n",
            "Epoch: 1055 Loss: 11.260216155911746\n",
            "Epoch: 1056 Loss: 11.252755137786743\n",
            "Epoch: 1057 Loss: 11.242345507000287\n",
            "Epoch: 1058 Loss: 11.23190873380376\n",
            "Epoch: 1059 Loss: 11.221460296166153\n",
            "Epoch: 1060 Loss: 11.211006325314795\n",
            "Epoch: 1061 Loss: 11.200552370853508\n",
            "Epoch: 1062 Loss: 11.190104313633096\n",
            "Epoch: 1063 Loss: 11.179663280108192\n",
            "Epoch: 1064 Loss: 11.169236089107265\n",
            "Epoch: 1065 Loss: 11.158824983357277\n",
            "Epoch: 1066 Loss: 11.148476139697907\n",
            "Epoch: 1067 Loss: 11.138384352204113\n",
            "Epoch: 1068 Loss: 11.128304705001254\n",
            "Epoch: 1069 Loss: 11.118118662204974\n",
            "Epoch: 1070 Loss: 11.107892399934832\n",
            "Epoch: 1071 Loss: 11.097644677126059\n",
            "Epoch: 1072 Loss: 11.087390074723887\n",
            "Epoch: 1073 Loss: 11.077136813523651\n",
            "Epoch: 1074 Loss: 11.06689442192671\n",
            "Epoch: 1075 Loss: 11.056708260319486\n",
            "Epoch: 1076 Loss: 11.053065001157814\n",
            "Epoch: 1077 Loss: 11.0468572663026\n",
            "Epoch: 1078 Loss: 11.036787117365972\n",
            "Epoch: 1079 Loss: 11.0266764303728\n",
            "Epoch: 1080 Loss: 11.016548236383613\n",
            "Epoch: 1081 Loss: 11.006423109683924\n",
            "Epoch: 1082 Loss: 10.998672393650962\n",
            "Epoch: 1083 Loss: 10.992651512263096\n",
            "Epoch: 1084 Loss: 10.99461936954029\n",
            "Epoch: 1085 Loss: 10.989712457480772\n",
            "Epoch: 1086 Loss: 10.981089991132153\n",
            "Epoch: 1087 Loss: 10.974441262991178\n",
            "Epoch: 1088 Loss: 10.96460721248301\n",
            "Epoch: 1089 Loss: 10.95474448539385\n",
            "Epoch: 1090 Loss: 10.944868854792416\n",
            "Epoch: 1091 Loss: 10.934988668607026\n",
            "Epoch: 1092 Loss: 10.925107509606793\n",
            "Epoch: 1093 Loss: 10.915229434241127\n",
            "Epoch: 1094 Loss: 10.905357004812071\n",
            "Epoch: 1095 Loss: 10.895493154008154\n",
            "Epoch: 1096 Loss: 10.885639511526689\n",
            "Epoch: 1097 Loss: 10.875799334186533\n",
            "Epoch: 1098 Loss: 10.865986532661934\n",
            "Epoch: 1099 Loss: 10.85624109902572\n",
            "Epoch: 1100 Loss: 10.846713325607844\n",
            "Epoch: 1101 Loss: 10.8372234512709\n",
            "Epoch: 1102 Loss: 10.828643807383967\n",
            "Epoch: 1103 Loss: 10.82160965707566\n",
            "Epoch: 1104 Loss: 10.820098959181486\n",
            "Epoch: 1105 Loss: 10.814514555486214\n",
            "Epoch: 1106 Loss: 10.80622767756011\n",
            "Epoch: 1107 Loss: 10.796680453307387\n",
            "Epoch: 1108 Loss: 10.787122108966111\n",
            "Epoch: 1109 Loss: 10.777557263729648\n",
            "Epoch: 1110 Loss: 10.767986759779426\n",
            "Epoch: 1111 Loss: 10.758413428886291\n",
            "Epoch: 1112 Loss: 10.748841192892232\n",
            "Epoch: 1113 Loss: 10.739273548549667\n",
            "Epoch: 1114 Loss: 10.729711935505085\n",
            "Epoch: 1115 Loss: 10.720159446572099\n",
            "Epoch: 1116 Loss: 10.710620434147081\n",
            "Epoch: 1117 Loss: 10.701142478358303\n",
            "Epoch: 1118 Loss: 10.691807964454476\n",
            "Epoch: 1119 Loss: 10.682549954288122\n",
            "Epoch: 1120 Loss: 10.673188618503294\n",
            "Epoch: 1121 Loss: 10.663787277999827\n",
            "Epoch: 1122 Loss: 10.654397275702955\n",
            "Epoch: 1123 Loss: 10.650761466455666\n",
            "Epoch: 1124 Loss: 10.643793664944178\n",
            "Epoch: 1125 Loss: 10.635865717183234\n",
            "Epoch: 1126 Loss: 10.627881004965213\n",
            "Epoch: 1127 Loss: 10.619870291001465\n",
            "Epoch: 1128 Loss: 10.610613464016375\n",
            "Epoch: 1129 Loss: 10.601324813954982\n",
            "Epoch: 1130 Loss: 10.592034812931324\n",
            "Epoch: 1131 Loss: 10.582745859850265\n",
            "Epoch: 1132 Loss: 10.573460153220008\n",
            "Epoch: 1133 Loss: 10.564181908010353\n",
            "Epoch: 1134 Loss: 10.554913952222746\n",
            "Epoch: 1135 Loss: 10.54566403269669\n",
            "Epoch: 1136 Loss: 10.536474021254534\n",
            "Epoch: 1137 Loss: 10.527430029731093\n",
            "Epoch: 1138 Loss: 10.51839368208744\n",
            "Epoch: 1139 Loss: 10.515865619069295\n",
            "Epoch: 1140 Loss: 10.508100487490736\n",
            "Epoch: 1141 Loss: 10.499026533261361\n",
            "Epoch: 1142 Loss: 10.489935071125366\n",
            "Epoch: 1143 Loss: 10.483239694261766\n",
            "Epoch: 1144 Loss: 10.479966426484927\n",
            "Epoch: 1145 Loss: 10.472153790329113\n",
            "Epoch: 1146 Loss: 10.463200592564755\n",
            "Epoch: 1147 Loss: 10.454204292762398\n",
            "Epoch: 1148 Loss: 10.445194974348205\n",
            "Epoch: 1149 Loss: 10.43618219450648\n",
            "Epoch: 1150 Loss: 10.427171587344255\n",
            "Epoch: 1151 Loss: 10.418166994886239\n",
            "Epoch: 1152 Loss: 10.409175418006962\n",
            "Epoch: 1153 Loss: 10.400235219230053\n",
            "Epoch: 1154 Loss: 10.391393364022834\n",
            "Epoch: 1155 Loss: 10.382624726302758\n",
            "Epoch: 1156 Loss: 10.373813992008689\n",
            "Epoch: 1157 Loss: 10.364982274982196\n",
            "Epoch: 1158 Loss: 10.356223000498685\n",
            "Epoch: 1159 Loss: 10.352381677288653\n",
            "Epoch: 1160 Loss: 10.343644300305682\n",
            "Epoch: 1161 Loss: 10.334817968472349\n",
            "Epoch: 1162 Loss: 10.326011826928355\n",
            "Epoch: 1163 Loss: 10.3172637369489\n",
            "Epoch: 1164 Loss: 10.312411776391293\n",
            "Epoch: 1165 Loss: 10.30490421678481\n",
            "Epoch: 1166 Loss: 10.30089969589832\n",
            "Epoch: 1167 Loss: 10.292148581506364\n",
            "Epoch: 1168 Loss: 10.283424308431075\n",
            "Epoch: 1169 Loss: 10.278180097347718\n",
            "Epoch: 1170 Loss: 10.276535680686846\n",
            "Epoch: 1171 Loss: 10.297282612350179\n",
            "Epoch: 1172 Loss: 10.2910828763996\n",
            "Epoch: 1173 Loss: 10.284807699532587\n",
            "Epoch: 1174 Loss: 10.278464596996407\n",
            "Epoch: 1175 Loss: 10.271884316189036\n",
            "Epoch: 1176 Loss: 10.26454551417237\n",
            "Epoch: 1177 Loss: 10.256973454083662\n",
            "Epoch: 1178 Loss: 10.248385339469484\n",
            "Epoch: 1179 Loss: 10.239776545411473\n",
            "Epoch: 1180 Loss: 10.231162418386633\n",
            "Epoch: 1181 Loss: 10.222553612367388\n",
            "Epoch: 1182 Loss: 10.213952638381611\n",
            "Epoch: 1183 Loss: 10.205398010359803\n",
            "Epoch: 1184 Loss: 10.196930352939447\n",
            "Epoch: 1185 Loss: 10.18851619139012\n",
            "Epoch: 1186 Loss: 10.182655112147483\n",
            "Epoch: 1187 Loss: 10.17538800692875\n",
            "Epoch: 1188 Loss: 10.176167587305123\n",
            "Epoch: 1189 Loss: 10.183336669902719\n",
            "Epoch: 1190 Loss: 10.178252934987531\n",
            "Epoch: 1191 Loss: 10.171029177167668\n",
            "Epoch: 1192 Loss: 10.163758391054376\n",
            "Epoch: 1193 Loss: 10.156467509715672\n",
            "Epoch: 1194 Loss: 10.149134136698253\n",
            "Epoch: 1195 Loss: 10.141747774848035\n",
            "Epoch: 1196 Loss: 10.134362966157674\n",
            "Epoch: 1197 Loss: 10.126118257358957\n",
            "Epoch: 1198 Loss: 10.119033427336909\n",
            "Epoch: 1199 Loss: 10.110735376655812\n",
            "Epoch: 1200 Loss: 10.102420354210894\n",
            "Epoch: 1201 Loss: 10.094097753063412\n",
            "Epoch: 1202 Loss: 10.085777408031827\n",
            "Epoch: 1203 Loss: 10.077464382152346\n",
            "Epoch: 1204 Loss: 10.06915875397155\n",
            "Epoch: 1205 Loss: 10.060861831257231\n",
            "Epoch: 1206 Loss: 10.052589881619305\n",
            "Epoch: 1207 Loss: 10.04439976902518\n",
            "Epoch: 1208 Loss: 10.036280435774284\n",
            "Epoch: 1209 Loss: 10.0281199215002\n",
            "Epoch: 1210 Loss: 10.021261984301269\n",
            "Epoch: 1211 Loss: 10.013116772408301\n",
            "Epoch: 1212 Loss: 10.006168196784454\n",
            "Epoch: 1213 Loss: 9.9980697349014\n",
            "Epoch: 1214 Loss: 9.99000061803987\n",
            "Epoch: 1215 Loss: 9.981931555798962\n",
            "Epoch: 1216 Loss: 9.981613916115412\n",
            "Epoch: 1217 Loss: 9.975727972642934\n",
            "Epoch: 1218 Loss: 9.970376568173407\n",
            "Epoch: 1219 Loss: 9.96518412727844\n",
            "Epoch: 1220 Loss: 9.958101073414088\n",
            "Epoch: 1221 Loss: 9.950748930038909\n",
            "Epoch: 1222 Loss: 9.943925102875564\n",
            "Epoch: 1223 Loss: 9.937058949121955\n",
            "Epoch: 1224 Loss: 9.930160423706386\n",
            "Epoch: 1225 Loss: 9.923221175538727\n",
            "Epoch: 1226 Loss: 9.915926617671222\n",
            "Epoch: 1227 Loss: 9.90817526556571\n",
            "Epoch: 1228 Loss: 9.900863544672388\n",
            "Epoch: 1229 Loss: 9.894766818158919\n",
            "Epoch: 1230 Loss: 9.886948747475321\n",
            "Epoch: 1231 Loss: 9.879180719054647\n",
            "Epoch: 1232 Loss: 9.871582854709375\n",
            "Epoch: 1233 Loss: 9.864164777131187\n",
            "Epoch: 1234 Loss: 9.856267060326488\n",
            "Epoch: 1235 Loss: 9.848673442594103\n",
            "Epoch: 1236 Loss: 9.841188416432784\n",
            "Epoch: 1237 Loss: 9.833665429064162\n",
            "Epoch: 1238 Loss: 9.82623805305897\n",
            "Epoch: 1239 Loss: 9.818436629353846\n",
            "Epoch: 1240 Loss: 9.810702105933744\n",
            "Epoch: 1241 Loss: 9.805211703097116\n",
            "Epoch: 1242 Loss: 9.797600767531115\n",
            "Epoch: 1243 Loss: 9.789898342249634\n",
            "Epoch: 1244 Loss: 9.782153489697867\n",
            "Epoch: 1245 Loss: 9.77441162501356\n",
            "Epoch: 1246 Loss: 9.766641904855259\n",
            "Epoch: 1247 Loss: 9.758879923462267\n",
            "Epoch: 1248 Loss: 9.751198287505542\n",
            "Epoch: 1249 Loss: 9.743601512587553\n",
            "Epoch: 1250 Loss: 9.735905270368374\n",
            "Epoch: 1251 Loss: 9.728208933159268\n",
            "Epoch: 1252 Loss: 9.720567041038695\n",
            "Epoch: 1253 Loss: 9.713002756193847\n",
            "Epoch: 1254 Loss: 9.705471548030769\n",
            "Epoch: 1255 Loss: 9.697861920333951\n",
            "Epoch: 1256 Loss: 9.690268004543583\n",
            "Epoch: 1257 Loss: 9.68274332985538\n",
            "Epoch: 1258 Loss: 9.675225346940492\n",
            "Epoch: 1259 Loss: 9.668665235744415\n",
            "Epoch: 1260 Loss: 9.663859858175865\n",
            "Epoch: 1261 Loss: 9.656426564639741\n",
            "Epoch: 1262 Loss: 9.648971875476086\n",
            "Epoch: 1263 Loss: 9.641491178217967\n",
            "Epoch: 1264 Loss: 9.634041416287873\n",
            "Epoch: 1265 Loss: 9.626537057171609\n",
            "Epoch: 1266 Loss: 9.619015468818368\n",
            "Epoch: 1267 Loss: 9.611490436956345\n",
            "Epoch: 1268 Loss: 9.603971981681607\n",
            "Epoch: 1269 Loss: 9.596480284484228\n",
            "Epoch: 1270 Loss: 9.589064729551675\n",
            "Epoch: 1271 Loss: 9.58175321917801\n",
            "Epoch: 1272 Loss: 9.575745921397454\n",
            "Epoch: 1273 Loss: 9.568449257145991\n",
            "Epoch: 1274 Loss: 9.561107698643161\n",
            "Epoch: 1275 Loss: 9.553738992718246\n",
            "Epoch: 1276 Loss: 9.546349977662855\n",
            "Epoch: 1277 Loss: 9.538946397458322\n",
            "Epoch: 1278 Loss: 9.532887554352511\n",
            "Epoch: 1279 Loss: 9.525631363487731\n",
            "Epoch: 1280 Loss: 9.518284688790422\n",
            "Epoch: 1281 Loss: 9.510916034592157\n",
            "Epoch: 1282 Loss: 9.503544686667281\n",
            "Epoch: 1283 Loss: 9.496175405213409\n",
            "Epoch: 1284 Loss: 9.488821574092361\n",
            "Epoch: 1285 Loss: 9.481691767021776\n",
            "Epoch: 1286 Loss: 9.474525762924987\n",
            "Epoch: 1287 Loss: 9.467249509628905\n",
            "Epoch: 1288 Loss: 9.459953422636087\n",
            "Epoch: 1289 Loss: 9.45265297285083\n",
            "Epoch: 1290 Loss: 9.445364537627773\n",
            "Epoch: 1291 Loss: 9.438421865040425\n",
            "Epoch: 1292 Loss: 9.432065698367285\n",
            "Epoch: 1293 Loss: 9.424952648797612\n",
            "Epoch: 1294 Loss: 9.417803382187422\n",
            "Epoch: 1295 Loss: 9.410636706978936\n",
            "Epoch: 1296 Loss: 9.4034590709104\n",
            "Epoch: 1297 Loss: 9.396275792147785\n",
            "Epoch: 1298 Loss: 9.38908871811982\n",
            "Epoch: 1299 Loss: 9.38385311366483\n",
            "Epoch: 1300 Loss: 9.376817620839926\n",
            "Epoch: 1301 Loss: 9.369742037573868\n",
            "Epoch: 1302 Loss: 9.362649812006214\n",
            "Epoch: 1303 Loss: 9.35554475829058\n",
            "Epoch: 1304 Loss: 9.348431005226747\n",
            "Epoch: 1305 Loss: 9.341314244021303\n",
            "Epoch: 1306 Loss: 9.334197315797619\n",
            "Epoch: 1307 Loss: 9.327089521817074\n",
            "Epoch: 1308 Loss: 9.320003342011752\n",
            "Epoch: 1309 Loss: 9.31301049017591\n",
            "Epoch: 1310 Loss: 9.306037730461426\n",
            "Epoch: 1311 Loss: 9.29900461002376\n",
            "Epoch: 1312 Loss: 9.291977165953298\n",
            "Epoch: 1313 Loss: 9.285000304958526\n",
            "Epoch: 1314 Loss: 9.278112772855705\n",
            "Epoch: 1315 Loss: 9.271884149390267\n",
            "Epoch: 1316 Loss: 9.264985138024512\n",
            "Epoch: 1317 Loss: 9.25805919719115\n",
            "Epoch: 1318 Loss: 9.251118747810656\n",
            "Epoch: 1319 Loss: 9.24417010066075\n",
            "Epoch: 1320 Loss: 9.237218000072035\n",
            "Epoch: 1321 Loss: 9.23026609249128\n",
            "Epoch: 1322 Loss: 9.2233214202023\n",
            "Epoch: 1323 Loss: 9.21641892676141\n",
            "Epoch: 1324 Loss: 9.209591698272886\n",
            "Epoch: 1325 Loss: 9.202839058680821\n",
            "Epoch: 1326 Loss: 9.19888200181907\n",
            "Epoch: 1327 Loss: 9.19233254800939\n",
            "Epoch: 1328 Loss: 9.185577451951525\n",
            "Epoch: 1329 Loss: 9.178791747268669\n",
            "Epoch: 1330 Loss: 9.171991262233288\n",
            "Epoch: 1331 Loss: 9.165181997828581\n",
            "Epoch: 1332 Loss: 9.15836668089882\n",
            "Epoch: 1333 Loss: 9.15154909320878\n",
            "Epoch: 1334 Loss: 9.144732517028041\n",
            "Epoch: 1335 Loss: 9.137919866646904\n",
            "Epoch: 1336 Loss: 9.132764887480263\n",
            "Epoch: 1337 Loss: 9.126111149061257\n",
            "Epoch: 1338 Loss: 9.119431837345982\n",
            "Epoch: 1339 Loss: 9.112734655533096\n",
            "Epoch: 1340 Loss: 9.106022782981832\n",
            "Epoch: 1341 Loss: 9.099302593090835\n",
            "Epoch: 1342 Loss: 9.092579259966788\n",
            "Epoch: 1343 Loss: 9.085855943604871\n",
            "Epoch: 1344 Loss: 9.079136913023943\n",
            "Epoch: 1345 Loss: 9.07242741070405\n",
            "Epoch: 1346 Loss: 9.065772111370304\n",
            "Epoch: 1347 Loss: 9.059213290772139\n",
            "Epoch: 1348 Loss: 9.052625861106058\n",
            "Epoch: 1349 Loss: 9.045996166125962\n",
            "Epoch: 1350 Loss: 9.03937635606677\n",
            "Epoch: 1351 Loss: 9.0327901349111\n",
            "Epoch: 1352 Loss: 9.027110658957785\n",
            "Epoch: 1353 Loss: 9.02074463452549\n",
            "Epoch: 1354 Loss: 9.014293365374979\n",
            "Epoch: 1355 Loss: 9.00780546260902\n",
            "Epoch: 1356 Loss: 9.00129218926026\n",
            "Epoch: 1357 Loss: 8.994761042071515\n",
            "Epoch: 1358 Loss: 8.988216753062886\n",
            "Epoch: 1359 Loss: 8.98166514527134\n",
            "Epoch: 1360 Loss: 8.975114904989953\n",
            "Epoch: 1361 Loss: 8.968570749504183\n",
            "Epoch: 1362 Loss: 8.962071460112107\n",
            "Epoch: 1363 Loss: 8.955636544750433\n",
            "Epoch: 1364 Loss: 8.949222186664755\n",
            "Epoch: 1365 Loss: 8.942798788307014\n",
            "Epoch: 1366 Loss: 8.936336214188957\n",
            "Epoch: 1367 Loss: 8.930321795490315\n",
            "Epoch: 1368 Loss: 8.92397708929024\n",
            "Epoch: 1369 Loss: 8.91757364171882\n",
            "Epoch: 1370 Loss: 8.911147508920815\n",
            "Epoch: 1371 Loss: 8.904710529313528\n",
            "Epoch: 1372 Loss: 8.898266545915607\n",
            "Epoch: 1373 Loss: 8.891820515671505\n",
            "Epoch: 1374 Loss: 8.885383220247352\n",
            "Epoch: 1375 Loss: 8.878981598476802\n",
            "Epoch: 1376 Loss: 8.872660343854289\n",
            "Epoch: 1377 Loss: 8.866330766738928\n",
            "Epoch: 1378 Loss: 8.862637829623353\n",
            "Epoch: 1379 Loss: 8.857257835393769\n",
            "Epoch: 1380 Loss: 8.850974116600325\n",
            "Epoch: 1381 Loss: 8.844660272797924\n",
            "Epoch: 1382 Loss: 8.838338813489697\n",
            "Epoch: 1383 Loss: 8.832012515404395\n",
            "Epoch: 1384 Loss: 8.825683472725109\n",
            "Epoch: 1385 Loss: 8.819353999183296\n",
            "Epoch: 1386 Loss: 8.813027328524052\n",
            "Epoch: 1387 Loss: 8.806703362731778\n",
            "Epoch: 1388 Loss: 8.800386090560096\n",
            "Epoch: 1389 Loss: 8.79408760769668\n",
            "Epoch: 1390 Loss: 8.787909280060148\n",
            "Epoch: 1391 Loss: 8.78169148285113\n",
            "Epoch: 1392 Loss: 8.77546179262796\n",
            "Epoch: 1393 Loss: 8.769250401348765\n",
            "Epoch: 1394 Loss: 8.76306676404797\n",
            "Epoch: 1395 Loss: 8.756912134215797\n",
            "Epoch: 1396 Loss: 8.750761016860405\n",
            "Epoch: 1397 Loss: 8.744588172639117\n",
            "Epoch: 1398 Loss: 8.738420464516405\n",
            "Epoch: 1399 Loss: 8.73249008916655\n",
            "Epoch: 1400 Loss: 8.726618939496175\n",
            "Epoch: 1401 Loss: 8.720495936557656\n",
            "Epoch: 1402 Loss: 8.714358266267853\n",
            "Epoch: 1403 Loss: 8.708214108623721\n",
            "Epoch: 1404 Loss: 8.702065059810446\n",
            "Epoch: 1405 Loss: 8.695914653360708\n",
            "Epoch: 1406 Loss: 8.68983087869448\n",
            "Epoch: 1407 Loss: 8.684062016312264\n",
            "Epoch: 1408 Loss: 8.678019580378603\n",
            "Epoch: 1409 Loss: 8.671947477508803\n",
            "Epoch: 1410 Loss: 8.66586220569065\n",
            "Epoch: 1411 Loss: 8.659772343609115\n",
            "Epoch: 1412 Loss: 8.653680277892589\n",
            "Epoch: 1413 Loss: 8.64759083140686\n",
            "Epoch: 1414 Loss: 8.641563241023036\n",
            "Epoch: 1415 Loss: 8.635545117338177\n",
            "Epoch: 1416 Loss: 8.629502292013992\n",
            "Epoch: 1417 Loss: 8.623452725601217\n",
            "Epoch: 1418 Loss: 8.617434191613887\n",
            "Epoch: 1419 Loss: 8.611448016960296\n",
            "Epoch: 1420 Loss: 8.605438457235747\n",
            "Epoch: 1421 Loss: 8.599450285893985\n",
            "Epoch: 1422 Loss: 8.593993256442694\n",
            "Epoch: 1423 Loss: 8.589043709429532\n",
            "Epoch: 1424 Loss: 8.583100056889371\n",
            "Epoch: 1425 Loss: 8.577146863369338\n",
            "Epoch: 1426 Loss: 8.571187262336574\n",
            "Epoch: 1427 Loss: 8.565224500506037\n",
            "Epoch: 1428 Loss: 8.559262590663097\n",
            "Epoch: 1429 Loss: 8.553340804308995\n",
            "Epoch: 1430 Loss: 8.54741579397604\n",
            "Epoch: 1431 Loss: 8.541488677105063\n",
            "Epoch: 1432 Loss: 8.535569086174203\n",
            "Epoch: 1433 Loss: 8.529704640371031\n",
            "Epoch: 1434 Loss: 8.523899447276225\n",
            "Epoch: 1435 Loss: 8.518103044604684\n",
            "Epoch: 1436 Loss: 8.515871053403021\n",
            "Epoch: 1437 Loss: 8.510348188080014\n",
            "Epoch: 1438 Loss: 8.504550034382545\n",
            "Epoch: 1439 Loss: 8.498745450166627\n",
            "Epoch: 1440 Loss: 8.492935554410634\n",
            "Epoch: 1441 Loss: 8.487117450607657\n",
            "Epoch: 1442 Loss: 8.481295155229825\n",
            "Epoch: 1443 Loss: 8.475471139666732\n",
            "Epoch: 1444 Loss: 8.469648824211575\n",
            "Epoch: 1445 Loss: 8.463826409768355\n",
            "Epoch: 1446 Loss: 8.45800866402923\n",
            "Epoch: 1447 Loss: 8.452199047794812\n",
            "Epoch: 1448 Loss: 8.446432514331581\n",
            "Epoch: 1449 Loss: 8.440704042100815\n",
            "Epoch: 1450 Loss: 8.434968819636666\n",
            "Epoch: 1451 Loss: 8.429234349006567\n",
            "Epoch: 1452 Loss: 8.423514412112947\n",
            "Epoch: 1453 Loss: 8.418149181059576\n",
            "Epoch: 1454 Loss: 8.412474408736388\n",
            "Epoch: 1455 Loss: 8.40677840487622\n",
            "Epoch: 1456 Loss: 8.401070217589496\n",
            "Epoch: 1457 Loss: 8.395355238608337\n",
            "Epoch: 1458 Loss: 8.389638833172823\n",
            "Epoch: 1459 Loss: 8.383921001644651\n",
            "Epoch: 1460 Loss: 8.378208836309696\n",
            "Epoch: 1461 Loss: 8.372511928837497\n",
            "Epoch: 1462 Loss: 8.366855172300847\n",
            "Epoch: 1463 Loss: 8.361266687212163\n",
            "Epoch: 1464 Loss: 8.355654958099535\n",
            "Epoch: 1465 Loss: 8.350089359219742\n",
            "Epoch: 1466 Loss: 8.344525733117703\n",
            "Epoch: 1467 Loss: 8.340990323852038\n",
            "Epoch: 1468 Loss: 8.336404243763239\n",
            "Epoch: 1469 Loss: 8.331778446812507\n",
            "Epoch: 1470 Loss: 8.327154419372144\n",
            "Epoch: 1471 Loss: 8.32252664729177\n",
            "Epoch: 1472 Loss: 8.31696573062282\n",
            "Epoch: 1473 Loss: 8.311361414828669\n",
            "Epoch: 1474 Loss: 8.305751316499293\n",
            "Epoch: 1475 Loss: 8.300152281234405\n",
            "Epoch: 1476 Loss: 8.29456276601641\n",
            "Epoch: 1477 Loss: 8.289046067810174\n",
            "Epoch: 1478 Loss: 8.283672225107042\n",
            "Epoch: 1479 Loss: 8.278182822631221\n",
            "Epoch: 1480 Loss: 8.272669668655546\n",
            "Epoch: 1481 Loss: 8.267146324902658\n",
            "Epoch: 1482 Loss: 8.261617260211333\n",
            "Epoch: 1483 Loss: 8.256086111438574\n",
            "Epoch: 1484 Loss: 8.250553294685396\n",
            "Epoch: 1485 Loss: 8.245027898815975\n",
            "Epoch: 1486 Loss: 8.239517717359075\n",
            "Epoch: 1487 Loss: 8.234067820562332\n",
            "Epoch: 1488 Loss: 8.228639172590881\n",
            "Epoch: 1489 Loss: 8.223193576143144\n",
            "Epoch: 1490 Loss: 8.218403829066704\n",
            "Epoch: 1491 Loss: 8.213945059885075\n",
            "Epoch: 1492 Loss: 8.209462188473255\n",
            "Epoch: 1493 Loss: 8.20496665304327\n",
            "Epoch: 1494 Loss: 8.200466700935982\n",
            "Epoch: 1495 Loss: 8.195955707601094\n",
            "Epoch: 1496 Loss: 8.190514091199285\n",
            "Epoch: 1497 Loss: 8.185071268527686\n",
            "Epoch: 1498 Loss: 8.179633292241032\n",
            "Epoch: 1499 Loss: 8.174201235068605\n",
            "Epoch: 1500 Loss: 8.168780964501352\n",
            "Epoch: 1501 Loss: 8.1637437632668\n",
            "Epoch: 1502 Loss: 8.158397695626924\n",
            "Epoch: 1503 Loss: 8.153029793182647\n",
            "Epoch: 1504 Loss: 8.147654287503165\n",
            "Epoch: 1505 Loss: 8.142274722999337\n",
            "Epoch: 1506 Loss: 8.13689246872869\n",
            "Epoch: 1507 Loss: 8.131517079969733\n",
            "Epoch: 1508 Loss: 8.126171394094378\n",
            "Epoch: 1509 Loss: 8.120853889478191\n",
            "Epoch: 1510 Loss: 8.115561812336209\n",
            "Epoch: 1511 Loss: 8.110262821226694\n",
            "Epoch: 1512 Loss: 8.104955186766444\n",
            "Epoch: 1513 Loss: 8.099685811930886\n",
            "Epoch: 1514 Loss: 8.094437725680272\n",
            "Epoch: 1515 Loss: 8.089151462925702\n",
            "Epoch: 1516 Loss: 8.083855820294813\n",
            "Epoch: 1517 Loss: 8.078557339880277\n",
            "Epoch: 1518 Loss: 8.073259439183822\n",
            "Epoch: 1519 Loss: 8.067972753146389\n",
            "Epoch: 1520 Loss: 8.06273543942279\n",
            "Epoch: 1521 Loss: 8.057552501803542\n",
            "Epoch: 1522 Loss: 8.052326953522458\n",
            "Epoch: 1523 Loss: 8.04708000515028\n",
            "Epoch: 1524 Loss: 8.04184866057578\n",
            "Epoch: 1525 Loss: 8.03964617814901\n",
            "Epoch: 1526 Loss: 8.03634047997869\n",
            "Epoch: 1527 Loss: 8.031182240889827\n",
            "Epoch: 1528 Loss: 8.026011625940233\n",
            "Epoch: 1529 Loss: 8.020832471478151\n",
            "Epoch: 1530 Loss: 8.015648376665956\n",
            "Epoch: 1531 Loss: 8.01067785645808\n",
            "Epoch: 1532 Loss: 8.00550231108547\n",
            "Epoch: 1533 Loss: 8.00031479998703\n",
            "Epoch: 1534 Loss: 7.9951306588747855\n",
            "Epoch: 1535 Loss: 7.989949856871688\n",
            "Epoch: 1536 Loss: 7.984779488042354\n",
            "Epoch: 1537 Loss: 7.979624836411836\n",
            "Epoch: 1538 Loss: 7.974514505862092\n",
            "Epoch: 1539 Loss: 7.969418776261867\n",
            "Epoch: 1540 Loss: 7.964872989164114\n",
            "Epoch: 1541 Loss: 7.9598178777941\n",
            "Epoch: 1542 Loss: 7.954723279097666\n",
            "Epoch: 1543 Loss: 7.949624537868125\n",
            "Epoch: 1544 Loss: 7.9445213189828205\n",
            "Epoch: 1545 Loss: 7.939416396127979\n",
            "Epoch: 1546 Loss: 7.934313147172287\n",
            "Epoch: 1547 Loss: 7.930147945484189\n",
            "Epoch: 1548 Loss: 7.92705786842064\n",
            "Epoch: 1549 Loss: 7.9231475944414695\n",
            "Epoch: 1550 Loss: 7.919042224878236\n",
            "Epoch: 1551 Loss: 7.914922721146989\n",
            "Epoch: 1552 Loss: 7.910794116541928\n",
            "Epoch: 1553 Loss: 7.90575959340094\n",
            "Epoch: 1554 Loss: 7.900719053789728\n",
            "Epoch: 1555 Loss: 7.895678057229073\n",
            "Epoch: 1556 Loss: 7.890639557055938\n",
            "Epoch: 1557 Loss: 7.885600750436758\n",
            "Epoch: 1558 Loss: 7.880564393762187\n",
            "Epoch: 1559 Loss: 7.875539980842553\n",
            "Epoch: 1560 Loss: 7.870553489256927\n",
            "Epoch: 1561 Loss: 7.866498733909294\n",
            "Epoch: 1562 Loss: 7.861517681630198\n",
            "Epoch: 1563 Loss: 7.85652974309035\n",
            "Epoch: 1564 Loss: 7.851552369109628\n",
            "Epoch: 1565 Loss: 7.8466213622272605\n",
            "Epoch: 1566 Loss: 7.842609880420588\n",
            "Epoch: 1567 Loss: 7.837657237477726\n",
            "Epoch: 1568 Loss: 7.8326898668803375\n",
            "Epoch: 1569 Loss: 7.827750743679394\n",
            "Epoch: 1570 Loss: 7.822857509116607\n",
            "Epoch: 1571 Loss: 7.817937147891983\n",
            "Epoch: 1572 Loss: 7.813004692836301\n",
            "Epoch: 1573 Loss: 7.808070718755514\n",
            "Epoch: 1574 Loss: 7.80313630753967\n",
            "Epoch: 1575 Loss: 7.798207103308854\n",
            "Epoch: 1576 Loss: 7.79330801571482\n",
            "Epoch: 1577 Loss: 7.788437333000528\n",
            "Epoch: 1578 Loss: 7.78445584673139\n",
            "Epoch: 1579 Loss: 7.779585167404431\n",
            "Epoch: 1580 Loss: 7.775238624696325\n",
            "Epoch: 1581 Loss: 7.770427687598606\n",
            "Epoch: 1582 Loss: 7.765591147418788\n",
            "Epoch: 1583 Loss: 7.760743867490502\n",
            "Epoch: 1584 Loss: 7.755891628161714\n",
            "Epoch: 1585 Loss: 7.7510387853405724\n",
            "Epoch: 1586 Loss: 7.746188351785967\n",
            "Epoch: 1587 Loss: 7.743141328408778\n",
            "Epoch: 1588 Loss: 7.738322051373508\n",
            "Epoch: 1589 Loss: 7.733498636356615\n",
            "Epoch: 1590 Loss: 7.728674185267632\n",
            "Epoch: 1591 Loss: 7.723850058738027\n",
            "Epoch: 1592 Loss: 7.720112685098539\n",
            "Epoch: 1593 Loss: 7.71625961091333\n",
            "Epoch: 1594 Loss: 7.711536680998077\n",
            "Epoch: 1595 Loss: 7.70680091809531\n",
            "Epoch: 1596 Loss: 7.702056953908377\n",
            "Epoch: 1597 Loss: 7.697304694701502\n",
            "Epoch: 1598 Loss: 7.692547290016039\n",
            "Epoch: 1599 Loss: 7.687787011710767\n",
            "Epoch: 1600 Loss: 7.683026240886493\n",
            "Epoch: 1601 Loss: 7.678262608515793\n",
            "Epoch: 1602 Loss: 7.6735004276991035\n",
            "Epoch: 1603 Loss: 7.668741503639846\n",
            "Epoch: 1604 Loss: 7.663995033393382\n",
            "Epoch: 1605 Loss: 7.659273443072746\n",
            "Epoch: 1606 Loss: 7.654575414819084\n",
            "Epoch: 1607 Loss: 7.649919529302035\n",
            "Epoch: 1608 Loss: 7.645221936237637\n",
            "Epoch: 1609 Loss: 7.64051664938652\n",
            "Epoch: 1610 Loss: 7.637109271015329\n",
            "Epoch: 1611 Loss: 7.632454594321792\n",
            "Epoch: 1612 Loss: 7.627791640652837\n",
            "Epoch: 1613 Loss: 7.623167800947273\n",
            "Epoch: 1614 Loss: 7.618514253099135\n",
            "Epoch: 1615 Loss: 7.61385419801727\n",
            "Epoch: 1616 Loss: 7.609190980429557\n",
            "Epoch: 1617 Loss: 7.604528602584627\n",
            "Epoch: 1618 Loss: 7.599863765821303\n",
            "Epoch: 1619 Loss: 7.595199075614581\n",
            "Epoch: 1620 Loss: 7.590535243276548\n",
            "Epoch: 1621 Loss: 7.585877223799297\n",
            "Epoch: 1622 Loss: 7.581221675083431\n",
            "Epoch: 1623 Loss: 7.57658515177758\n",
            "Epoch: 1624 Loss: 7.581104894049488\n",
            "Epoch: 1625 Loss: 7.5797018799178355\n",
            "Epoch: 1626 Loss: 7.576970968380972\n",
            "Epoch: 1627 Loss: 7.57258840354063\n",
            "Epoch: 1628 Loss: 7.568134747759268\n",
            "Epoch: 1629 Loss: 7.563644179259816\n",
            "Epoch: 1630 Loss: 7.559132224743152\n",
            "Epoch: 1631 Loss: 7.554601110717538\n",
            "Epoch: 1632 Loss: 7.550055181403413\n",
            "Epoch: 1633 Loss: 7.545498623435454\n",
            "Epoch: 1634 Loss: 7.540936458279958\n",
            "Epoch: 1635 Loss: 7.536372995103874\n",
            "Epoch: 1636 Loss: 7.531807451066575\n",
            "Epoch: 1637 Loss: 7.527244349101296\n",
            "Epoch: 1638 Loss: 7.522690533845433\n",
            "Epoch: 1639 Loss: 7.5181456501970905\n",
            "Epoch: 1640 Loss: 7.51371402877406\n",
            "Epoch: 1641 Loss: 7.509253315522361\n",
            "Epoch: 1642 Loss: 7.505630527780454\n",
            "Epoch: 1643 Loss: 7.5019708358431325\n",
            "Epoch: 1644 Loss: 7.498310024528176\n",
            "Epoch: 1645 Loss: 7.493804068248783\n",
            "Epoch: 1646 Loss: 7.48927895566684\n",
            "Epoch: 1647 Loss: 7.4847562507942635\n",
            "Epoch: 1648 Loss: 7.480423776050787\n",
            "Epoch: 1649 Loss: 7.475974615094407\n",
            "Epoch: 1650 Loss: 7.4715148578701\n",
            "Epoch: 1651 Loss: 7.467031801647766\n",
            "Epoch: 1652 Loss: 7.46254309478768\n",
            "Epoch: 1653 Loss: 7.458053980128771\n",
            "Epoch: 1654 Loss: 7.453566517588096\n",
            "Epoch: 1655 Loss: 7.449089974191007\n",
            "Epoch: 1656 Loss: 7.444662530376792\n",
            "Epoch: 1657 Loss: 7.44023650144655\n",
            "Epoch: 1658 Loss: 7.4358065231577255\n",
            "Epoch: 1659 Loss: 7.431374994196156\n",
            "Epoch: 1660 Loss: 7.4269565110770275\n",
            "Epoch: 1661 Loss: 7.422567448026306\n",
            "Epoch: 1662 Loss: 7.419274238759056\n",
            "Epoch: 1663 Loss: 7.415838711366328\n",
            "Epoch: 1664 Loss: 7.411438908132637\n",
            "Epoch: 1665 Loss: 7.407035768669708\n",
            "Epoch: 1666 Loss: 7.402632250112011\n",
            "Epoch: 1667 Loss: 7.398228583167375\n",
            "Epoch: 1668 Loss: 7.393825851731451\n",
            "Epoch: 1669 Loss: 7.389424716450572\n",
            "Epoch: 1670 Loss: 7.385026334755175\n",
            "Epoch: 1671 Loss: 7.380633670169363\n",
            "Epoch: 1672 Loss: 7.376241526276694\n",
            "Epoch: 1673 Loss: 7.372741500453152\n",
            "Epoch: 1674 Loss: 7.368375514230629\n",
            "Epoch: 1675 Loss: 7.363999323828915\n",
            "Epoch: 1676 Loss: 7.359623472876988\n",
            "Epoch: 1677 Loss: 7.355261043742994\n",
            "Epoch: 1678 Loss: 7.3509404047561135\n",
            "Epoch: 1679 Loss: 7.3466173318315695\n",
            "Epoch: 1680 Loss: 7.342281429901341\n",
            "Epoch: 1681 Loss: 7.3379356023612825\n",
            "Epoch: 1682 Loss: 7.333594728647427\n",
            "Epoch: 1683 Loss: 7.3292910131470475\n",
            "Epoch: 1684 Loss: 7.3250022109761845\n",
            "Epoch: 1685 Loss: 7.320706380849418\n",
            "Epoch: 1686 Loss: 7.316389996134398\n",
            "Epoch: 1687 Loss: 7.312071636708957\n",
            "Epoch: 1688 Loss: 7.307764892628494\n",
            "Epoch: 1689 Loss: 7.303875287810812\n",
            "Epoch: 1690 Loss: 7.300480762208084\n",
            "Epoch: 1691 Loss: 7.297067487383742\n",
            "Epoch: 1692 Loss: 7.293647261513527\n",
            "Epoch: 1693 Loss: 7.290222479310451\n",
            "Epoch: 1694 Loss: 7.286845945183951\n",
            "Epoch: 1695 Loss: 7.283217028073843\n",
            "Epoch: 1696 Loss: 7.279808665380644\n",
            "Epoch: 1697 Loss: 7.276086685863021\n",
            "Epoch: 1698 Loss: 7.271856767399556\n",
            "Epoch: 1699 Loss: 7.267618812444829\n",
            "Epoch: 1700 Loss: 7.263378612777948\n",
            "Epoch: 1701 Loss: 7.259138653376218\n",
            "Epoch: 1702 Loss: 7.254899322838198\n",
            "Epoch: 1703 Loss: 7.250668377371076\n",
            "Epoch: 1704 Loss: 7.246433659798761\n",
            "Epoch: 1705 Loss: 7.242202227552582\n",
            "Epoch: 1706 Loss: 7.237981100774971\n",
            "Epoch: 1707 Loss: 7.233768548013555\n",
            "Epoch: 1708 Loss: 7.229574027658039\n",
            "Epoch: 1709 Loss: 7.225389089271889\n",
            "Epoch: 1710 Loss: 7.221208996838838\n",
            "Epoch: 1711 Loss: 7.217024464057819\n",
            "Epoch: 1712 Loss: 7.212849576147922\n",
            "Epoch: 1713 Loss: 7.208683992541832\n",
            "Epoch: 1714 Loss: 7.204527569073759\n",
            "Epoch: 1715 Loss: 7.200361956668506\n",
            "Epoch: 1716 Loss: 7.196192483185573\n",
            "Epoch: 1717 Loss: 7.192569964409603\n",
            "Epoch: 1718 Loss: 7.189266177987598\n",
            "Epoch: 1719 Loss: 7.1859466609887335\n",
            "Epoch: 1720 Loss: 7.182622726534168\n",
            "Epoch: 1721 Loss: 7.179298639025086\n",
            "Epoch: 1722 Loss: 7.1759754683165315\n",
            "Epoch: 1723 Loss: 7.173209368966602\n",
            "Epoch: 1724 Loss: 7.169252269641001\n",
            "Epoch: 1725 Loss: 7.166011020800631\n",
            "Epoch: 1726 Loss: 7.161915229527453\n",
            "Epoch: 1727 Loss: 7.157809803691633\n",
            "Epoch: 1728 Loss: 7.153701761561027\n",
            "Epoch: 1729 Loss: 7.149593641157682\n",
            "Epoch: 1730 Loss: 7.145486435512247\n",
            "Epoch: 1731 Loss: 7.141380665922087\n",
            "Epoch: 1732 Loss: 7.137277669722106\n",
            "Epoch: 1733 Loss: 7.133177464511666\n",
            "Epoch: 1734 Loss: 7.129081279855458\n",
            "Epoch: 1735 Loss: 7.124990281976404\n",
            "Epoch: 1736 Loss: 7.120920572850555\n",
            "Epoch: 1737 Loss: 7.116891329550785\n",
            "Epoch: 1738 Loss: 7.112836285892989\n",
            "Epoch: 1739 Loss: 7.1087674562007\n",
            "Epoch: 1740 Loss: 7.1046971596040125\n",
            "Epoch: 1741 Loss: 7.100630907496683\n",
            "Epoch: 1742 Loss: 7.096574271274059\n",
            "Epoch: 1743 Loss: 7.0925433246620315\n",
            "Epoch: 1744 Loss: 7.088520915998481\n",
            "Epoch: 1745 Loss: 7.084546231264172\n",
            "Epoch: 1746 Loss: 7.080558877323037\n",
            "Epoch: 1747 Loss: 7.076541478784818\n",
            "Epoch: 1748 Loss: 7.0725210042699675\n",
            "Epoch: 1749 Loss: 7.068499076026749\n",
            "Epoch: 1750 Loss: 7.064477982704698\n",
            "Epoch: 1751 Loss: 7.060458834397859\n",
            "Epoch: 1752 Loss: 7.056442361825271\n",
            "Epoch: 1753 Loss: 7.053611269014739\n",
            "Epoch: 1754 Loss: 7.049658505881986\n",
            "Epoch: 1755 Loss: 7.045675988282342\n",
            "Epoch: 1756 Loss: 7.041698129161087\n",
            "Epoch: 1757 Loss: 7.037717332969056\n",
            "Epoch: 1758 Loss: 7.033737416628069\n",
            "Epoch: 1759 Loss: 7.029758379400671\n",
            "Epoch: 1760 Loss: 7.02578089822357\n",
            "Epoch: 1761 Loss: 7.021806210277163\n",
            "Epoch: 1762 Loss: 7.018057958239092\n",
            "Epoch: 1763 Loss: 7.014934046707929\n",
            "Epoch: 1764 Loss: 7.012204220289583\n",
            "Epoch: 1765 Loss: 7.009074967583395\n",
            "Epoch: 1766 Loss: 7.005163472724099\n",
            "Epoch: 1767 Loss: 7.001245290561129\n",
            "Epoch: 1768 Loss: 6.997324596639703\n",
            "Epoch: 1769 Loss: 6.993402851470678\n",
            "Epoch: 1770 Loss: 6.989481267226327\n",
            "Epoch: 1771 Loss: 6.985560316155988\n",
            "Epoch: 1772 Loss: 6.981639605353128\n",
            "Epoch: 1773 Loss: 6.977720633356241\n",
            "Epoch: 1774 Loss: 6.973804398935349\n",
            "Epoch: 1775 Loss: 6.969892103500102\n",
            "Epoch: 1776 Loss: 6.965998540774379\n",
            "Epoch: 1777 Loss: 6.962123642650255\n",
            "Epoch: 1778 Loss: 6.958247442990412\n",
            "Epoch: 1779 Loss: 6.954370333698685\n",
            "Epoch: 1780 Loss: 6.951582404883054\n",
            "Epoch: 1781 Loss: 6.947715224378528\n",
            "Epoch: 1782 Loss: 6.9438438963976745\n",
            "Epoch: 1783 Loss: 6.939970882074126\n",
            "Epoch: 1784 Loss: 6.936097713502235\n",
            "Epoch: 1785 Loss: 6.932226260644097\n",
            "Epoch: 1786 Loss: 6.928357461760098\n",
            "Epoch: 1787 Loss: 6.924492841968178\n",
            "Epoch: 1788 Loss: 6.920641588729091\n",
            "Epoch: 1789 Loss: 6.916800783336681\n",
            "Epoch: 1790 Loss: 6.912972978760339\n",
            "Epoch: 1791 Loss: 6.909146709326183\n",
            "Epoch: 1792 Loss: 6.905317447110479\n",
            "Epoch: 1793 Loss: 6.901493423532699\n",
            "Epoch: 1794 Loss: 6.8976875827604145\n",
            "Epoch: 1795 Loss: 6.893884374622098\n",
            "Epoch: 1796 Loss: 6.890362819923406\n",
            "Epoch: 1797 Loss: 6.886565810385856\n",
            "Epoch: 1798 Loss: 6.882766730215567\n",
            "Epoch: 1799 Loss: 6.878966415079489\n",
            "Epoch: 1800 Loss: 6.875166739074109\n",
            "Epoch: 1801 Loss: 6.871368793097193\n",
            "Epoch: 1802 Loss: 6.867571514531826\n",
            "Epoch: 1803 Loss: 6.863776692970633\n",
            "Epoch: 1804 Loss: 6.859985267879204\n",
            "Epoch: 1805 Loss: 6.8562017396695945\n",
            "Epoch: 1806 Loss: 6.852430029022788\n",
            "Epoch: 1807 Loss: 6.848668201874978\n",
            "Epoch: 1808 Loss: 6.8450793986891165\n",
            "Epoch: 1809 Loss: 6.84133302791485\n",
            "Epoch: 1810 Loss: 6.837579981819317\n",
            "Epoch: 1811 Loss: 6.8338268983997965\n",
            "Epoch: 1812 Loss: 6.830074608483534\n",
            "Epoch: 1813 Loss: 6.826323552412619\n",
            "Epoch: 1814 Loss: 6.822587526162587\n",
            "Epoch: 1815 Loss: 6.818868982602325\n",
            "Epoch: 1816 Loss: 6.815133301536918\n",
            "Epoch: 1817 Loss: 6.811396928691709\n",
            "Epoch: 1818 Loss: 6.80766250916896\n",
            "Epoch: 1819 Loss: 6.803981127620506\n",
            "Epoch: 1820 Loss: 6.80026970494812\n",
            "Epoch: 1821 Loss: 6.796552886428589\n",
            "Epoch: 1822 Loss: 6.792836047378064\n",
            "Epoch: 1823 Loss: 6.789122837175578\n",
            "Epoch: 1824 Loss: 6.785415117426004\n",
            "Epoch: 1825 Loss: 6.781712090418041\n",
            "Epoch: 1826 Loss: 6.778023782284109\n",
            "Epoch: 1827 Loss: 6.774349038944064\n",
            "Epoch: 1828 Loss: 6.770681067361235\n",
            "Epoch: 1829 Loss: 6.767185390676007\n",
            "Epoch: 1830 Loss: 6.764229536851761\n",
            "Epoch: 1831 Loss: 6.760581649253046\n",
            "Epoch: 1832 Loss: 6.75693005814126\n",
            "Epoch: 1833 Loss: 6.753277678756479\n",
            "Epoch: 1834 Loss: 6.74962446681707\n",
            "Epoch: 1835 Loss: 6.745974182882338\n",
            "Epoch: 1836 Loss: 6.742326284469105\n",
            "Epoch: 1837 Loss: 6.738673497601\n",
            "Epoch: 1838 Loss: 6.735023129306001\n",
            "Epoch: 1839 Loss: 6.731373810025149\n",
            "Epoch: 1840 Loss: 6.727730257245663\n",
            "Epoch: 1841 Loss: 6.724100264807837\n",
            "Epoch: 1842 Loss: 6.720600195650788\n",
            "Epoch: 1843 Loss: 6.71699312601094\n",
            "Epoch: 1844 Loss: 6.713369224491098\n",
            "Epoch: 1845 Loss: 6.709743854514732\n",
            "Epoch: 1846 Loss: 6.706119878413966\n",
            "Epoch: 1847 Loss: 6.7025569247681105\n",
            "Epoch: 1848 Loss: 6.698969725460759\n",
            "Epoch: 1849 Loss: 6.695370593610036\n",
            "Epoch: 1850 Loss: 6.6917704893555285\n",
            "Epoch: 1851 Loss: 6.688229414785925\n",
            "Epoch: 1852 Loss: 6.684655115416389\n",
            "Epoch: 1853 Loss: 6.6810698579920595\n",
            "Epoch: 1854 Loss: 6.677508544046715\n",
            "Epoch: 1855 Loss: 6.674637066795603\n",
            "Epoch: 1856 Loss: 6.671076214154642\n",
            "Epoch: 1857 Loss: 6.66751129288515\n",
            "Epoch: 1858 Loss: 6.663946137982406\n",
            "Epoch: 1859 Loss: 6.660382776015269\n",
            "Epoch: 1860 Loss: 6.656821149055389\n",
            "Epoch: 1861 Loss: 6.653261487817869\n",
            "Epoch: 1862 Loss: 6.649703566496456\n",
            "Epoch: 1863 Loss: 6.646147916059272\n",
            "Epoch: 1864 Loss: 6.64259593101118\n",
            "Epoch: 1865 Loss: 6.639047033496379\n",
            "Epoch: 1866 Loss: 6.635748831540545\n",
            "Epoch: 1867 Loss: 6.632235317047731\n",
            "Epoch: 1868 Loss: 6.628708628562556\n",
            "Epoch: 1869 Loss: 6.625177389862405\n",
            "Epoch: 1870 Loss: 6.621645772575906\n",
            "Epoch: 1871 Loss: 6.618118550653518\n",
            "Epoch: 1872 Loss: 6.614602494803843\n",
            "Epoch: 1873 Loss: 6.611099517468218\n",
            "Epoch: 1874 Loss: 6.607605694366083\n",
            "Epoch: 1875 Loss: 6.605354653294026\n",
            "Epoch: 1876 Loss: 6.601888991218976\n",
            "Epoch: 1877 Loss: 6.5984169209840955\n",
            "Epoch: 1878 Loss: 6.594942296950211\n",
            "Epoch: 1879 Loss: 6.591466174221677\n",
            "Epoch: 1880 Loss: 6.58798869524055\n",
            "Epoch: 1881 Loss: 6.584510272950431\n",
            "Epoch: 1882 Loss: 6.5810317046059446\n",
            "Epoch: 1883 Loss: 6.577553950320685\n",
            "Epoch: 1884 Loss: 6.574077195583972\n",
            "Epoch: 1885 Loss: 6.570601758644686\n",
            "Epoch: 1886 Loss: 6.567128631838063\n",
            "Epoch: 1887 Loss: 6.56365988644232\n",
            "Epoch: 1888 Loss: 6.560203843616047\n",
            "Epoch: 1889 Loss: 6.556766967176732\n",
            "Epoch: 1890 Loss: 6.553331805993038\n",
            "Epoch: 1891 Loss: 6.549893997360235\n",
            "Epoch: 1892 Loss: 6.546461430656281\n",
            "Epoch: 1893 Loss: 6.543047308683114\n",
            "Epoch: 1894 Loss: 6.540116138363913\n",
            "Epoch: 1895 Loss: 6.53671569657843\n",
            "Epoch: 1896 Loss: 6.533292861755875\n",
            "Epoch: 1897 Loss: 6.529870968586992\n",
            "Epoch: 1898 Loss: 6.526448336820743\n",
            "Epoch: 1899 Loss: 6.523026698381166\n",
            "Epoch: 1900 Loss: 6.519606934509387\n",
            "Epoch: 1901 Loss: 6.51619069219933\n",
            "Epoch: 1902 Loss: 6.51277851642208\n",
            "Epoch: 1903 Loss: 6.509372038229802\n",
            "Epoch: 1904 Loss: 6.505972741970752\n",
            "Epoch: 1905 Loss: 6.503306875137263\n",
            "Epoch: 1906 Loss: 6.5005761140294664\n",
            "Epoch: 1907 Loss: 6.497253839497556\n",
            "Epoch: 1908 Loss: 6.493940206369145\n",
            "Epoch: 1909 Loss: 6.490570855121589\n",
            "Epoch: 1910 Loss: 6.487201417581449\n",
            "Epoch: 1911 Loss: 6.4838324059613015\n",
            "Epoch: 1912 Loss: 6.480462933502707\n",
            "Epoch: 1913 Loss: 6.477094165022508\n",
            "Epoch: 1914 Loss: 6.473759926703991\n",
            "Epoch: 1915 Loss: 6.470403896057414\n",
            "Epoch: 1916 Loss: 6.467044924665107\n",
            "Epoch: 1917 Loss: 6.4636872123288684\n",
            "Epoch: 1918 Loss: 6.460330430972927\n",
            "Epoch: 1919 Loss: 6.456980242369546\n",
            "Epoch: 1920 Loss: 6.45364638984989\n",
            "Epoch: 1921 Loss: 6.450318796937035\n",
            "Epoch: 1922 Loss: 6.446989805714944\n",
            "Epoch: 1923 Loss: 6.443846979488758\n",
            "Epoch: 1924 Loss: 6.440548612241102\n",
            "Epoch: 1925 Loss: 6.4372424845941385\n",
            "Epoch: 1926 Loss: 6.433932757261844\n",
            "Epoch: 1927 Loss: 6.43062081859141\n",
            "Epoch: 1928 Loss: 6.427307094591317\n",
            "Epoch: 1929 Loss: 6.423994059880494\n",
            "Epoch: 1930 Loss: 6.420683199308452\n",
            "Epoch: 1931 Loss: 6.417375108008781\n",
            "Epoch: 1932 Loss: 6.414072667933795\n",
            "Epoch: 1933 Loss: 6.4107841051536\n",
            "Epoch: 1934 Loss: 6.407559064394412\n",
            "Epoch: 1935 Loss: 6.404282498025275\n",
            "Epoch: 1936 Loss: 6.40100143091435\n",
            "Epoch: 1937 Loss: 6.397720406306947\n",
            "Epoch: 1938 Loss: 6.394438827741196\n",
            "Epoch: 1939 Loss: 6.3911579673458965\n",
            "Epoch: 1940 Loss: 6.387878799505926\n",
            "Epoch: 1941 Loss: 6.384602663828409\n",
            "Epoch: 1942 Loss: 6.38133205779892\n",
            "Epoch: 1943 Loss: 6.378078378052428\n",
            "Epoch: 1944 Loss: 6.374828434642083\n",
            "Epoch: 1945 Loss: 6.371577700635952\n",
            "Epoch: 1946 Loss: 6.368331216654206\n",
            "Epoch: 1947 Loss: 6.365090102585999\n",
            "Epoch: 1948 Loss: 6.361846188879424\n",
            "Epoch: 1949 Loss: 6.358608497786744\n",
            "Epoch: 1950 Loss: 6.35536970771228\n",
            "Epoch: 1951 Loss: 6.352137601044145\n",
            "Epoch: 1952 Loss: 6.348908952750507\n",
            "Epoch: 1953 Loss: 6.345688839027688\n",
            "Epoch: 1954 Loss: 6.342463363758457\n",
            "Epoch: 1955 Loss: 6.3392428523111946\n",
            "Epoch: 1956 Loss: 6.336037592421399\n",
            "Epoch: 1957 Loss: 6.333420049709956\n",
            "Epoch: 1958 Loss: 6.332056808201126\n",
            "Epoch: 1959 Loss: 6.328857045121672\n",
            "Epoch: 1960 Loss: 6.32565501847163\n",
            "Epoch: 1961 Loss: 6.322454013949492\n",
            "Epoch: 1962 Loss: 6.319254999735218\n",
            "Epoch: 1963 Loss: 6.316057999966928\n",
            "Epoch: 1964 Loss: 6.312862412948154\n",
            "Epoch: 1965 Loss: 6.309668177423975\n",
            "Epoch: 1966 Loss: 6.3064788082256475\n",
            "Epoch: 1967 Loss: 6.303288045840742\n",
            "Epoch: 1968 Loss: 6.300098986405211\n",
            "Epoch: 1969 Loss: 6.296912935082641\n",
            "Epoch: 1970 Loss: 6.293729422094696\n",
            "Epoch: 1971 Loss: 6.290557574553212\n",
            "Epoch: 1972 Loss: 6.287390905021133\n",
            "Epoch: 1973 Loss: 6.284232815384205\n",
            "Epoch: 1974 Loss: 6.281071085455188\n",
            "Epoch: 1975 Loss: 6.277913649824615\n",
            "Epoch: 1976 Loss: 6.274764846234076\n",
            "Epoch: 1977 Loss: 6.271627182138326\n",
            "Epoch: 1978 Loss: 6.268481198154894\n",
            "Epoch: 1979 Loss: 6.265333296459277\n",
            "Epoch: 1980 Loss: 6.262191722126345\n",
            "Epoch: 1981 Loss: 6.259054940074182\n",
            "Epoch: 1982 Loss: 6.255920571707815\n",
            "Epoch: 1983 Loss: 6.252793161369737\n",
            "Epoch: 1984 Loss: 6.249672459110198\n",
            "Epoch: 1985 Loss: 6.24750514403047\n",
            "Epoch: 1986 Loss: 6.244383985011992\n",
            "Epoch: 1987 Loss: 6.241259403228534\n",
            "Epoch: 1988 Loss: 6.238133957819721\n",
            "Epoch: 1989 Loss: 6.236011922259794\n",
            "Epoch: 1990 Loss: 6.233669500030506\n",
            "Epoch: 1991 Loss: 6.230570713167929\n",
            "Epoch: 1992 Loss: 6.227468073626404\n",
            "Epoch: 1993 Loss: 6.224365179780718\n",
            "Epoch: 1994 Loss: 6.2212619528906234\n",
            "Epoch: 1995 Loss: 6.21815779743281\n",
            "Epoch: 1996 Loss: 6.215055794104618\n",
            "Epoch: 1997 Loss: 6.211955988737182\n",
            "Epoch: 1998 Loss: 6.208857829901196\n",
            "Epoch: 1999 Loss: 6.2057625079590615\n",
            "Epoch: 2000 Loss: 6.202675386235197\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "G3ZCpjff_IFA",
        "outputId": "10e4e138-8ec9-4d1a-8321-68796feb11d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from torchvision.transforms.functional import normalize, to_pil_image, to_tensor\n",
        "\n",
        "correct = 0\n",
        "total = 30\n",
        "number_of_steps = 200\n",
        "list_of_examples = []\n",
        "perimeters = []\n",
        "with torch.no_grad():\n",
        "    for data in overfit_generator:\n",
        "        list_of_frames = []\n",
        "        print('***************************')\n",
        "        x = data[0].to(device)\n",
        "        y = data[1]\n",
        "        first_image = data[2]\n",
        "        #second_image = data[3].numpy()\n",
        "        #second_image = to_pil_image(data[3].int())\n",
        "        root_image = to_pil_image(data[3].int())\n",
        "        second_translation = data[4]\n",
        "        Tt = torch.zeros((number_of_steps, 3), dtype=torch.int16)\n",
        "        print(y)\n",
        "        print('/*/*/*/*/*/*/*/*/*/*/*/*/*/*')\n",
        "        j = 0\n",
        "        perimeters.append(get_perimeter(first_image[0, ...].numpy()))\n",
        "        while j < number_of_steps - 1:\n",
        "            out = policy_net(x.float())\n",
        "            print(out)\n",
        "            action = torch.argmax(out, dim=1)\n",
        "            Tt[j + 1, :] = q_value.take_action(action, Tt[j, :])\n",
        "            if j > 5 and torch.all(Tt[j + 1, :].eq(Tt[j - 1, :])):\n",
        "                if torch.dist(Tt[j + 1, :].float(), y).item() <= 1:\n",
        "                    correct += 1\n",
        "                break\n",
        "            translation = (Tt[j + 1, 1].item() + second_translation[0, 0].item(), Tt[j + 1, 2].item() + second_translation[0, 1].item())\n",
        "            #translated = translate_image(second_image[0, ...], Tt_new[1].item(), Tt_new[2].item())\n",
        "            #current_image = rotate_image(translated, Tt_new[0].item(), (second_position[0, 0] + Tt_new[1].item(), second_position[0, 1] + Tt_new[2].item()))\n",
        "            current_image = to_tensor(affine(root_image, angle=Tt[j + 1, 0].item(), translate=translation, scale=1, shear=0))\n",
        "            list_of_frames.append(current_image.numpy().reshape((50, 50)))\n",
        "            diff = first_image - current_image\n",
        "            diff /= 255\n",
        "            x = diff.unsqueeze(0).to(device)\n",
        "            j += 1\n",
        "        stack_of_frames = np.stack(list_of_frames)\n",
        "        #stack_of_frames = np.pad(stack_of_frames, ((0, number_of_steps - stack_of_frames.shape[0]), (0, 0), (0, 0)))\n",
        "        list_of_examples.append(stack_of_frames)\n",
        "        mask = Tt.eq(torch.zeros((number_of_steps, 3))).all(dim=1)[5:]\n",
        "        Tt = torch.cat((Tt[:5], Tt[5:][~mask]))\n",
        "        print(Tt)\n",
        "    print('accuracy {}%'.format(100 * correct / total))"
      ],
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "***************************\n",
            "tensor([[30,  0,  0]], dtype=torch.int16)\n",
            "/*/*/*/*/*/*/*/*/*/*/*/*/*/*\n",
            "tensor([[10.0437,  8.9694,  8.9694,  8.0345,  8.9694,  8.9695]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.1672,  8.9347,  8.9347,  8.1268,  8.9347,  8.9347]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.1672,  8.9347,  8.9347,  8.1268,  8.9347,  8.9347]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.1758,  8.9323,  8.9323,  8.1333,  8.9323,  8.9323]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.2005,  8.9273,  8.9273,  8.1551,  8.9273,  8.9274]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.2475,  8.9179,  8.9179,  8.1968,  8.9179,  8.9179]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.2845,  8.9104,  8.9104,  8.2295,  8.9104,  8.9104]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.2458,  8.9182,  8.9182,  8.1952,  8.9182,  8.9182]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.2310,  8.9212,  8.9212,  8.1821,  8.9212,  8.9212]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.2566,  8.9160,  8.9160,  8.2048,  8.9160,  8.9160]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.2672,  8.9139,  8.9139,  8.2142,  8.9139,  8.9139]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.3818,  8.8909,  8.8909,  8.3156,  8.8908,  8.8908]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.1651,  8.9353,  8.9353,  8.1252,  8.9353,  8.9353]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.0035,  8.9866,  8.9866,  8.0029,  8.9866,  8.9867]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.0841,  8.9526,  8.9526, 10.0959,  8.9526,  8.9527]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 0,  0,  0],\n",
            "        [ 1,  0,  0],\n",
            "        [ 2,  0,  0],\n",
            "        [ 3,  0,  0],\n",
            "        [ 4,  0,  0],\n",
            "        [ 5,  0,  0],\n",
            "        [ 6,  0,  0],\n",
            "        [ 7,  0,  0],\n",
            "        [ 8,  0,  0],\n",
            "        [ 9,  0,  0],\n",
            "        [10,  0,  0],\n",
            "        [11,  0,  0],\n",
            "        [12,  0,  0],\n",
            "        [13,  0,  0],\n",
            "        [14,  0,  0],\n",
            "        [13,  0,  0]], dtype=torch.int16)\n",
            "***************************\n",
            "tensor([[-50,   0,   0]], dtype=torch.int16)\n",
            "/*/*/*/*/*/*/*/*/*/*/*/*/*/*\n",
            "tensor([[ 8.0199,  8.9794,  8.9794, 10.0186,  8.9794,  8.9794]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.0913,  8.9501,  8.9501, 10.1048,  8.9501,  8.9502]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.0913,  8.9501,  8.9501, 10.1048,  8.9501,  8.9502]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.0949,  8.9488,  8.9488, 10.1093,  8.9489,  8.9489]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.1103,  8.9434,  8.9434, 10.1285,  8.9435,  8.9435]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.1194,  8.9403,  8.9403, 10.1398,  8.9403,  8.9404]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.1213,  8.9396,  8.9396, 10.1422,  8.9396,  8.9397]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.1264,  8.9378,  8.9378, 10.1485,  8.9378,  8.9379]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.1754,  8.9244,  8.9244, 10.2087,  8.9244,  8.9245]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.1785,  8.9237,  8.9237, 10.2124,  8.9237,  8.9238]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.1445,  8.9315,  8.9315, 10.1709,  8.9315,  8.9316]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.1889,  8.9213,  8.9213, 10.2252,  8.9213,  8.9214]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.2001,  8.9188,  8.9188, 10.2388,  8.9188,  8.9188]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.2216,  8.9138,  8.9138, 10.2652,  8.9138,  8.9138]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.1595,  8.9281,  8.9281, 10.1893,  8.9281,  8.9281]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.1546,  8.9292,  8.9292, 10.1833,  8.9292,  8.9293]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.1483,  8.9306,  8.9307, 10.1756,  8.9307,  8.9307]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.1403,  8.9330,  8.9330, 10.1658,  8.9330,  8.9331]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.2183,  8.9146,  8.9146, 10.2611,  8.9146,  8.9146]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.2314,  8.9116,  8.9116, 10.2771,  8.9116,  8.9116]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.2952,  8.8969,  8.8969, 10.3551,  8.8969,  8.8969]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.2117,  8.9161,  8.9161, 10.2530,  8.9161,  8.9161]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.1654,  8.9267,  8.9267, 10.1965,  8.9267,  8.9268]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.0035,  8.9866,  8.9866,  8.0029,  8.9866,  8.9867]],\n",
            "       device='cuda:0')\n",
            "tensor([[  0,   0,   0],\n",
            "        [ -1,   0,   0],\n",
            "        [ -2,   0,   0],\n",
            "        [ -3,   0,   0],\n",
            "        [ -4,   0,   0],\n",
            "        [ -5,   0,   0],\n",
            "        [ -6,   0,   0],\n",
            "        [ -7,   0,   0],\n",
            "        [ -8,   0,   0],\n",
            "        [ -9,   0,   0],\n",
            "        [-10,   0,   0],\n",
            "        [-11,   0,   0],\n",
            "        [-12,   0,   0],\n",
            "        [-13,   0,   0],\n",
            "        [-14,   0,   0],\n",
            "        [-15,   0,   0],\n",
            "        [-16,   0,   0],\n",
            "        [-17,   0,   0],\n",
            "        [-18,   0,   0],\n",
            "        [-19,   0,   0],\n",
            "        [-20,   0,   0],\n",
            "        [-21,   0,   0],\n",
            "        [-22,   0,   0],\n",
            "        [-23,   0,   0],\n",
            "        [-22,   0,   0]], dtype=torch.int16)\n",
            "***************************\n",
            "tensor([[-50,   0,   0]], dtype=torch.int16)\n",
            "/*/*/*/*/*/*/*/*/*/*/*/*/*/*\n",
            "tensor([[ 8.0199,  8.9794,  8.9794, 10.0186,  8.9794,  8.9794]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.0199,  8.9794,  8.9794, 10.0186,  8.9794,  8.9794]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.0199,  8.9794,  8.9794, 10.0186,  8.9794,  8.9794]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.0218,  8.9784,  8.9784, 10.0208,  8.9784,  8.9783]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.0437,  8.9664,  8.9664, 10.0458,  8.9665,  8.9666]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.0433,  8.9666,  8.9666, 10.0453,  8.9666,  8.9667]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.0410,  8.9678,  8.9678, 10.0426,  8.9678,  8.9679]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.0448,  8.9661,  8.9661, 10.0471,  8.9661,  8.9662]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.0462,  8.9657,  8.9657, 10.0488,  8.9657,  8.9658]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.0465,  8.9656,  8.9656, 10.0492,  8.9656,  8.9657]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.0481,  8.9651,  8.9651, 10.0513,  8.9651,  8.9652]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.0497,  8.9646,  8.9646, 10.0532,  8.9646,  8.9647]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.0539,  8.9631,  8.9631, 10.0585,  8.9631,  8.9632]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.0586,  8.9615,  8.9615, 10.0643,  8.9615,  8.9616]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.0586,  8.9615,  8.9615, 10.0642,  8.9615,  8.9616]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.0742,  8.9560,  8.9560, 10.0836,  8.9561,  8.9562]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.0638,  8.9597,  8.9597, 10.0708,  8.9597,  8.9598]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.0831,  8.9529,  8.9529, 10.0947,  8.9529,  8.9530]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.0919,  8.9499,  8.9499, 10.1056,  8.9499,  8.9500]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.1016,  8.9465,  8.9465, 10.1177,  8.9465,  8.9466]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.1090,  8.9439,  8.9439, 10.1269,  8.9439,  8.9440]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.1164,  8.9413,  8.9413, 10.1361,  8.9413,  8.9414]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.1109,  8.9432,  8.9432, 10.1293,  8.9433,  8.9433]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.1323,  8.9358,  8.9358, 10.1558,  8.9358,  8.9359]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.1427,  8.9322,  8.9322, 10.1687,  8.9322,  8.9322]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.1654,  8.9267,  8.9267, 10.1964,  8.9267,  8.9268]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.1775,  8.9239,  8.9239, 10.2113,  8.9240,  8.9240]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.2260,  8.9128,  8.9128, 10.2705,  8.9128,  8.9128]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.2526,  8.9067,  8.9067, 10.3030,  8.9067,  8.9067]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.2698,  8.9027,  8.9027, 10.3240,  8.9027,  8.9027]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.3255,  8.8900,  8.8900, 10.3921,  8.8900,  8.8900]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.3263,  8.8904,  8.8904, 10.3932,  8.8904,  8.8904]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.3273,  8.8909,  8.8909, 10.3945,  8.8909,  8.8909]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.3279,  8.8913,  8.8913, 10.3954,  8.8912,  8.8912]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.3327,  8.8937,  8.8937, 10.4017,  8.8937,  8.8935]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.3334,  8.8940,  8.8940, 10.4026,  8.8940,  8.8939]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.3350,  8.8948,  8.8948, 10.4046,  8.8948,  8.8946]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.3357,  8.8952,  8.8952, 10.4056,  8.8951,  8.8950]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.3368,  8.8958,  8.8958, 10.4071,  8.8957,  8.8956]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.3343,  8.8945,  8.8945, 10.4038,  8.8945,  8.8943]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.3339,  8.8943,  8.8943, 10.4032,  8.8942,  8.8941]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.3266,  8.8906,  8.8906, 10.3936,  8.8906,  8.8905]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.0035,  8.9866,  8.9866,  8.0029,  8.9866,  8.9867]],\n",
            "       device='cuda:0')\n",
            "tensor([[  0,   0,   0],\n",
            "        [ -1,   0,   0],\n",
            "        [ -2,   0,   0],\n",
            "        [ -3,   0,   0],\n",
            "        [ -4,   0,   0],\n",
            "        [ -5,   0,   0],\n",
            "        [ -6,   0,   0],\n",
            "        [ -7,   0,   0],\n",
            "        [ -8,   0,   0],\n",
            "        [ -9,   0,   0],\n",
            "        [-10,   0,   0],\n",
            "        [-11,   0,   0],\n",
            "        [-12,   0,   0],\n",
            "        [-13,   0,   0],\n",
            "        [-14,   0,   0],\n",
            "        [-15,   0,   0],\n",
            "        [-16,   0,   0],\n",
            "        [-17,   0,   0],\n",
            "        [-18,   0,   0],\n",
            "        [-19,   0,   0],\n",
            "        [-20,   0,   0],\n",
            "        [-21,   0,   0],\n",
            "        [-22,   0,   0],\n",
            "        [-23,   0,   0],\n",
            "        [-24,   0,   0],\n",
            "        [-25,   0,   0],\n",
            "        [-26,   0,   0],\n",
            "        [-27,   0,   0],\n",
            "        [-28,   0,   0],\n",
            "        [-29,   0,   0],\n",
            "        [-30,   0,   0],\n",
            "        [-31,   0,   0],\n",
            "        [-32,   0,   0],\n",
            "        [-33,   0,   0],\n",
            "        [-34,   0,   0],\n",
            "        [-35,   0,   0],\n",
            "        [-36,   0,   0],\n",
            "        [-37,   0,   0],\n",
            "        [-38,   0,   0],\n",
            "        [-39,   0,   0],\n",
            "        [-40,   0,   0],\n",
            "        [-41,   0,   0],\n",
            "        [-42,   0,   0],\n",
            "        [-41,   0,   0]], dtype=torch.int16)\n",
            "***************************\n",
            "tensor([[-40,   0,   0]], dtype=torch.int16)\n",
            "/*/*/*/*/*/*/*/*/*/*/*/*/*/*\n",
            "tensor([[ 8.0199,  8.9794,  8.9794, 10.0186,  8.9794,  8.9794]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.0695,  8.9621,  8.9621,  8.0538,  8.9621,  8.9622]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.0695,  8.9621,  8.9621,  8.0538,  8.9621,  8.9622]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.0695,  8.9621,  8.9621,  8.0538,  8.9621,  8.9622]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.0695,  8.9621,  8.9621,  8.0538,  8.9621,  8.9622]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.1070,  8.9516,  8.9516,  8.0818,  8.9516,  8.9517]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.1692,  8.9341,  8.9341,  8.1283,  8.9341,  8.9342]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.1831,  8.9308,  8.9308,  8.1398,  8.9308,  8.9309]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.1871,  8.9300,  8.9300,  8.1433,  8.9300,  8.9301]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.2391,  8.9196,  8.9196,  8.1893,  8.9196,  8.9196]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.2731,  8.9127,  8.9127,  8.2194,  8.9127,  8.9127]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.2954,  8.9082,  8.9082,  8.2391,  8.9082,  8.9082]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.3348,  8.9003,  8.9003,  8.2740,  8.9003,  8.9003]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.3762,  8.8920,  8.8920,  8.3106,  8.8920,  8.8919]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.3837,  8.8909,  8.8909,  8.3153,  8.8908,  8.8908]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.3994,  8.8923,  8.8923,  8.3063,  8.8923,  8.8922]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.3985,  8.8923,  8.8922,  8.3068,  8.8922,  8.8921]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.4045,  8.8928,  8.8928,  8.3033,  8.8928,  8.8927]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.4015,  8.8925,  8.8925,  8.3050,  8.8925,  8.8924]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.3956,  8.8920,  8.8920,  8.3085,  8.8919,  8.8919]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.3960,  8.8920,  8.8920,  8.3082,  8.8920,  8.8919]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.3895,  8.8914,  8.8914,  8.3120,  8.8914,  8.8913]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.3909,  8.8915,  8.8915,  8.3112,  8.8915,  8.8915]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.3752,  8.8922,  8.8922,  8.3097,  8.8922,  8.8921]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.1093,  8.9509,  8.9509,  8.0836,  8.9509,  8.9510]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.0035,  8.9866,  8.9866,  8.0029,  8.9866,  8.9867]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.1838,  8.9225,  8.9225, 10.2190,  8.9225,  8.9225]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 0,  0,  0],\n",
            "        [-1,  0,  0],\n",
            "        [ 0,  0,  0],\n",
            "        [ 1,  0,  0],\n",
            "        [ 2,  0,  0],\n",
            "        [ 3,  0,  0],\n",
            "        [ 4,  0,  0],\n",
            "        [ 5,  0,  0],\n",
            "        [ 6,  0,  0],\n",
            "        [ 7,  0,  0],\n",
            "        [ 8,  0,  0],\n",
            "        [ 9,  0,  0],\n",
            "        [10,  0,  0],\n",
            "        [11,  0,  0],\n",
            "        [12,  0,  0],\n",
            "        [13,  0,  0],\n",
            "        [14,  0,  0],\n",
            "        [15,  0,  0],\n",
            "        [16,  0,  0],\n",
            "        [17,  0,  0],\n",
            "        [18,  0,  0],\n",
            "        [19,  0,  0],\n",
            "        [20,  0,  0],\n",
            "        [21,  0,  0],\n",
            "        [22,  0,  0],\n",
            "        [23,  0,  0],\n",
            "        [24,  0,  0],\n",
            "        [23,  0,  0]], dtype=torch.int16)\n",
            "***************************\n",
            "tensor([[40,  0,  0]], dtype=torch.int16)\n",
            "/*/*/*/*/*/*/*/*/*/*/*/*/*/*\n",
            "tensor([[10.0189,  8.9793,  8.9793,  8.0152,  8.9793,  8.9794]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.0199,  8.9794,  8.9794, 10.0186,  8.9794,  8.9794]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.0199,  8.9794,  8.9794, 10.0186,  8.9794,  8.9794]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.0199,  8.9794,  8.9794, 10.0186,  8.9794,  8.9794]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.0199,  8.9794,  8.9794, 10.0186,  8.9794,  8.9794]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.0199,  8.9794,  8.9794, 10.0186,  8.9794,  8.9794]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.0199,  8.9794,  8.9794, 10.0186,  8.9794,  8.9794]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.0439,  8.9664,  8.9664, 10.0460,  8.9664,  8.9665]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.0453,  8.9660,  8.9660, 10.0477,  8.9660,  8.9661]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.0446,  8.9661,  8.9662, 10.0469,  8.9662,  8.9663]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.0443,  8.9663,  8.9663, 10.0465,  8.9663,  8.9664]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.0440,  8.9663,  8.9664, 10.0461,  8.9664,  8.9665]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.0436,  8.9665,  8.9665, 10.0457,  8.9665,  8.9666]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.0452,  8.9660,  8.9660, 10.0476,  8.9660,  8.9661]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.0451,  8.9660,  8.9660, 10.0475,  8.9660,  8.9661]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.0471,  8.9654,  8.9654, 10.0500,  8.9654,  8.9655]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.0476,  8.9653,  8.9653, 10.0506,  8.9653,  8.9654]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.0534,  8.9633,  8.9633, 10.0578,  8.9633,  8.9634]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.0508,  8.9642,  8.9642, 10.0546,  8.9642,  8.9643]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.0545,  8.9629,  8.9629, 10.0591,  8.9629,  8.9630]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.0540,  8.9631,  8.9631, 10.0586,  8.9631,  8.9632]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.0532,  8.9634,  8.9634, 10.0575,  8.9634,  8.9635]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.0541,  8.9630,  8.9630, 10.0587,  8.9631,  8.9632]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.0503,  8.9643,  8.9643, 10.0540,  8.9644,  8.9645]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.0650,  8.9592,  8.9592, 10.0722,  8.9593,  8.9594]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.0605,  8.9608,  8.9608, 10.0666,  8.9608,  8.9609]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.0661,  8.9589,  8.9589, 10.0736,  8.9589,  8.9590]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.0662,  8.9588,  8.9588, 10.0737,  8.9589,  8.9589]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.0817,  8.9534,  8.9534, 10.0929,  8.9535,  8.9535]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.0862,  8.9519,  8.9519, 10.0985,  8.9519,  8.9520]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.0911,  8.9501,  8.9501, 10.1046,  8.9502,  8.9502]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.0968,  8.9482,  8.9482, 10.1117,  8.9482,  8.9483]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.1159,  8.9415,  8.9415, 10.1355,  8.9415,  8.9416]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.1132,  8.9424,  8.9424, 10.1320,  8.9425,  8.9425]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.1207,  8.9398,  8.9398, 10.1414,  8.9398,  8.9399]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.1484,  8.9306,  8.9306, 10.1757,  8.9306,  8.9307]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.1639,  8.9271,  8.9271, 10.1947,  8.9271,  8.9271]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.1822,  8.9229,  8.9229, 10.2170,  8.9229,  8.9229]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.2165,  8.9150,  8.9150, 10.2589,  8.9150,  8.9150]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.2159,  8.9151,  8.9151, 10.2582,  8.9151,  8.9151]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.2411,  8.9093,  8.9093, 10.2890,  8.9093,  8.9093]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.2199,  8.9142,  8.9142, 10.2631,  8.9142,  8.9142]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.2756,  8.9014,  8.9014, 10.3312,  8.9014,  8.9014]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.2869,  8.8988,  8.8988, 10.3449,  8.8988,  8.8988]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.3105,  8.8934,  8.8934, 10.3738,  8.8934,  8.8933]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.3265,  8.8906,  8.8906, 10.3935,  8.8905,  8.8905]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.3271,  8.8909,  8.8909, 10.3943,  8.8908,  8.8908]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.3133,  8.8927,  8.8927, 10.3772,  8.8927,  8.8927]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.3279,  8.8913,  8.8913, 10.3953,  8.8912,  8.8912]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.2976,  8.8964,  8.8963, 10.3580,  8.8963,  8.8963]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.1920,  8.9206,  8.9206, 10.2290,  8.9206,  8.9207]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.1330,  8.9355,  8.9355, 10.1567,  8.9356,  8.9356]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.0035,  8.9866,  8.9866,  8.0029,  8.9866,  8.9867]],\n",
            "       device='cuda:0')\n",
            "tensor([[  0,   0,   0],\n",
            "        [  1,   0,   0],\n",
            "        [  0,   0,   0],\n",
            "        [ -1,   0,   0],\n",
            "        [ -2,   0,   0],\n",
            "        [ -3,   0,   0],\n",
            "        [ -4,   0,   0],\n",
            "        [ -5,   0,   0],\n",
            "        [ -6,   0,   0],\n",
            "        [ -7,   0,   0],\n",
            "        [ -8,   0,   0],\n",
            "        [ -9,   0,   0],\n",
            "        [-10,   0,   0],\n",
            "        [-11,   0,   0],\n",
            "        [-12,   0,   0],\n",
            "        [-13,   0,   0],\n",
            "        [-14,   0,   0],\n",
            "        [-15,   0,   0],\n",
            "        [-16,   0,   0],\n",
            "        [-17,   0,   0],\n",
            "        [-18,   0,   0],\n",
            "        [-19,   0,   0],\n",
            "        [-20,   0,   0],\n",
            "        [-21,   0,   0],\n",
            "        [-22,   0,   0],\n",
            "        [-23,   0,   0],\n",
            "        [-24,   0,   0],\n",
            "        [-25,   0,   0],\n",
            "        [-26,   0,   0],\n",
            "        [-27,   0,   0],\n",
            "        [-28,   0,   0],\n",
            "        [-29,   0,   0],\n",
            "        [-30,   0,   0],\n",
            "        [-31,   0,   0],\n",
            "        [-32,   0,   0],\n",
            "        [-33,   0,   0],\n",
            "        [-34,   0,   0],\n",
            "        [-35,   0,   0],\n",
            "        [-36,   0,   0],\n",
            "        [-37,   0,   0],\n",
            "        [-38,   0,   0],\n",
            "        [-39,   0,   0],\n",
            "        [-40,   0,   0],\n",
            "        [-41,   0,   0],\n",
            "        [-42,   0,   0],\n",
            "        [-43,   0,   0],\n",
            "        [-44,   0,   0],\n",
            "        [-45,   0,   0],\n",
            "        [-46,   0,   0],\n",
            "        [-47,   0,   0],\n",
            "        [-48,   0,   0],\n",
            "        [-49,   0,   0],\n",
            "        [-50,   0,   0],\n",
            "        [-49,   0,   0]], dtype=torch.int16)\n",
            "***************************\n",
            "tensor([[20,  0,  0]], dtype=torch.int16)\n",
            "/*/*/*/*/*/*/*/*/*/*/*/*/*/*\n",
            "tensor([[10.1369,  8.9432,  8.9432,  8.1042,  8.9432,  8.9432]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.0946,  8.9489,  8.9489, 10.1090,  8.9489,  8.9490]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.0946,  8.9489,  8.9489, 10.1090,  8.9489,  8.9490]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.0946,  8.9489,  8.9489, 10.1090,  8.9489,  8.9490]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.0946,  8.9489,  8.9489, 10.1090,  8.9490,  8.9490]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.0940,  8.9491,  8.9491, 10.1082,  8.9492,  8.9492]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.1210,  8.9397,  8.9397, 10.1418,  8.9397,  8.9398]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.1563,  8.9288,  8.9288, 10.1854,  8.9288,  8.9289]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.2038,  8.9179,  8.9179, 10.2434,  8.9179,  8.9179]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.2152,  8.9152,  8.9152, 10.2574,  8.9152,  8.9152]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.2126,  8.9155,  8.9155, 10.2541,  8.9155,  8.9155]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.2831,  8.8989,  8.8989, 10.3402,  8.8988,  8.8988]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.3231,  8.8902,  8.8902, 10.3891,  8.8902,  8.8902]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.3276,  8.8911,  8.8911, 10.3949,  8.8911,  8.8910]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.3272,  8.8909,  8.8909, 10.3944,  8.8909,  8.8908]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.3295,  8.8921,  8.8921, 10.3974,  8.8920,  8.8919]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.3335,  8.8941,  8.8941, 10.4027,  8.8940,  8.8939]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.3355,  8.8951,  8.8951, 10.4053,  8.8950,  8.8949]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.3344,  8.8946,  8.8946, 10.4039,  8.8945,  8.8944]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.3282,  8.8914,  8.8914, 10.3957,  8.8914,  8.8913]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.3037,  8.8949,  8.8949, 10.3655,  8.8949,  8.8949]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.0035,  8.9866,  8.9866,  8.0029,  8.9866,  8.9867]],\n",
            "       device='cuda:0')\n",
            "tensor([[  0,   0,   0],\n",
            "        [  1,   0,   0],\n",
            "        [  0,   0,   0],\n",
            "        [ -1,   0,   0],\n",
            "        [ -2,   0,   0],\n",
            "        [ -3,   0,   0],\n",
            "        [ -4,   0,   0],\n",
            "        [ -5,   0,   0],\n",
            "        [ -6,   0,   0],\n",
            "        [ -7,   0,   0],\n",
            "        [ -8,   0,   0],\n",
            "        [ -9,   0,   0],\n",
            "        [-10,   0,   0],\n",
            "        [-11,   0,   0],\n",
            "        [-12,   0,   0],\n",
            "        [-13,   0,   0],\n",
            "        [-14,   0,   0],\n",
            "        [-15,   0,   0],\n",
            "        [-16,   0,   0],\n",
            "        [-17,   0,   0],\n",
            "        [-18,   0,   0],\n",
            "        [-19,   0,   0],\n",
            "        [-18,   0,   0]], dtype=torch.int16)\n",
            "***************************\n",
            "tensor([[-20,   0,   0]], dtype=torch.int16)\n",
            "/*/*/*/*/*/*/*/*/*/*/*/*/*/*\n",
            "tensor([[ 8.1404,  8.9329,  8.9329, 10.1659,  8.9330,  8.9330]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.1404,  8.9329,  8.9329, 10.1659,  8.9330,  8.9330]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.1404,  8.9329,  8.9329, 10.1659,  8.9330,  8.9330]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.1442,  8.9316,  8.9316, 10.1706,  8.9317,  8.9317]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.1572,  8.9286,  8.9286, 10.1865,  8.9286,  8.9287]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.1839,  8.9225,  8.9225, 10.2191,  8.9225,  8.9225]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.2087,  8.9168,  8.9168, 10.2494,  8.9168,  8.9168]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.2586,  8.9053,  8.9053, 10.3104,  8.9053,  8.9053]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.2212,  8.9139,  8.9139, 10.2647,  8.9139,  8.9139]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.2357,  8.9106,  8.9106, 10.2824,  8.9106,  8.9106]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.2913,  8.8978,  8.8978, 10.3503,  8.8978,  8.8978]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.3268,  8.8907,  8.8907, 10.3939,  8.8907,  8.8906]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.3291,  8.8918,  8.8918, 10.3968,  8.8918,  8.8917]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.3310,  8.8928,  8.8928, 10.3993,  8.8928,  8.8927]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.3288,  8.8917,  8.8917, 10.3965,  8.8917,  8.8916]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.3280,  8.8913,  8.8913, 10.3954,  8.8913,  8.8912]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.2979,  8.8963,  8.8963, 10.3585,  8.8963,  8.8962]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.2725,  8.9021,  8.9021, 10.3274,  8.9021,  8.9021]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.1842,  8.9224,  8.9224, 10.2194,  8.9224,  8.9225]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.0035,  8.9866,  8.9866,  8.0029,  8.9866,  8.9867]],\n",
            "       device='cuda:0')\n",
            "tensor([[  0,   0,   0],\n",
            "        [ -1,   0,   0],\n",
            "        [ -2,   0,   0],\n",
            "        [ -3,   0,   0],\n",
            "        [ -4,   0,   0],\n",
            "        [ -5,   0,   0],\n",
            "        [ -6,   0,   0],\n",
            "        [ -7,   0,   0],\n",
            "        [ -8,   0,   0],\n",
            "        [ -9,   0,   0],\n",
            "        [-10,   0,   0],\n",
            "        [-11,   0,   0],\n",
            "        [-12,   0,   0],\n",
            "        [-13,   0,   0],\n",
            "        [-14,   0,   0],\n",
            "        [-15,   0,   0],\n",
            "        [-16,   0,   0],\n",
            "        [-17,   0,   0],\n",
            "        [-18,   0,   0],\n",
            "        [-19,   0,   0],\n",
            "        [-18,   0,   0]], dtype=torch.int16)\n",
            "***************************\n",
            "tensor([[50,  0,  0]], dtype=torch.int16)\n",
            "/*/*/*/*/*/*/*/*/*/*/*/*/*/*\n",
            "tensor([[10.0049,  8.9860,  8.9860,  8.0040,  8.9860,  8.9860]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.0047,  8.9861,  8.9861,  8.0038,  8.9861,  8.9862]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.0047,  8.9861,  8.9861,  8.0038,  8.9861,  8.9862]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.0035,  8.9866,  8.9866,  8.0029,  8.9866,  8.9867]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.0083,  8.9843,  8.9843,  8.0067,  8.9844,  8.9844]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.0140,  8.9816,  8.9816,  8.0113,  8.9817,  8.9817]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.0197,  8.9789,  8.9789,  8.0158,  8.9790,  8.9790]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.0170,  8.9802,  8.9802,  8.0137,  8.9802,  8.9803]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.0173,  8.9801,  8.9801,  8.0139,  8.9801,  8.9801]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.0190,  8.9793,  8.9793,  8.0153,  8.9793,  8.9793]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.0206,  8.9785,  8.9785,  8.0165,  8.9785,  8.9786]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.0306,  8.9738,  8.9738,  8.0245,  8.9738,  8.9739]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.0316,  8.9733,  8.9733,  8.0254,  8.9733,  8.9733]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.0317,  8.9732,  8.9732,  8.0255,  8.9732,  8.9733]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.0315,  8.9733,  8.9733,  8.0253,  8.9734,  8.9734]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.0407,  8.9702,  8.9702,  8.0323,  8.9702,  8.9703]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.0364,  8.9714,  8.9714,  8.0291,  8.9715,  8.9715]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.0691,  8.9622,  8.9622,  8.0535,  8.9623,  8.9623]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.0563,  8.9658,  8.9658,  8.0440,  8.9658,  8.9659]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.0657,  8.9632,  8.9632,  8.0510,  8.9632,  8.9633]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.0752,  8.9605,  8.9605,  8.0581,  8.9605,  8.9606]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.0821,  8.9586,  8.9586,  8.0632,  8.9586,  8.9587]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.0942,  8.9552,  8.9552,  8.0723,  8.9552,  8.9552]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.1079,  8.9513,  8.9513,  8.0825,  8.9513,  8.9514]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.1143,  8.9495,  8.9495,  8.0873,  8.9495,  8.9496]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.1168,  8.9488,  8.9488,  8.0892,  8.9488,  8.9489]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.1317,  8.9447,  8.9447,  8.1003,  8.9447,  8.9447]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.1373,  8.9431,  8.9431,  8.1045,  8.9431,  8.9431]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.1552,  8.9380,  8.9380,  8.1178,  8.9381,  8.9381]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.1603,  8.9366,  8.9366,  8.1217,  8.9366,  8.9366]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.1643,  8.9355,  8.9355,  8.1246,  8.9355,  8.9355]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.1671,  8.9347,  8.9347,  8.1268,  8.9347,  8.9347]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.2017,  8.9271,  8.9271,  8.1562,  8.9271,  8.9271]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.1951,  8.9284,  8.9284,  8.1504,  8.9284,  8.9284]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.2101,  8.9254,  8.9254,  8.1637,  8.9254,  8.9254]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.2070,  8.9260,  8.9260,  8.1609,  8.9260,  8.9260]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.2068,  8.9261,  8.9261,  8.1607,  8.9261,  8.9261]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.2485,  8.9177,  8.9177,  8.1977,  8.9177,  8.9177]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.2640,  8.9145,  8.9145,  8.2114,  8.9145,  8.9145]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.2930,  8.9087,  8.9087,  8.2370,  8.9087,  8.9087]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.2881,  8.9097,  8.9097,  8.2327,  8.9097,  8.9097]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.3816,  8.8909,  8.8909,  8.3154,  8.8909,  8.8908]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.4125,  8.8936,  8.8936,  8.2987,  8.8935,  8.8934]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.4105,  8.8934,  8.8934,  8.2999,  8.8933,  8.8932]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.4012,  8.8925,  8.8925,  8.3052,  8.8925,  8.8924]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.3276,  8.8911,  8.8911, 10.3949,  8.8911,  8.8910]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 0,  0,  0],\n",
            "        [ 1,  0,  0],\n",
            "        [ 2,  0,  0],\n",
            "        [ 3,  0,  0],\n",
            "        [ 4,  0,  0],\n",
            "        [ 5,  0,  0],\n",
            "        [ 6,  0,  0],\n",
            "        [ 7,  0,  0],\n",
            "        [ 8,  0,  0],\n",
            "        [ 9,  0,  0],\n",
            "        [10,  0,  0],\n",
            "        [11,  0,  0],\n",
            "        [12,  0,  0],\n",
            "        [13,  0,  0],\n",
            "        [14,  0,  0],\n",
            "        [15,  0,  0],\n",
            "        [16,  0,  0],\n",
            "        [17,  0,  0],\n",
            "        [18,  0,  0],\n",
            "        [19,  0,  0],\n",
            "        [20,  0,  0],\n",
            "        [21,  0,  0],\n",
            "        [22,  0,  0],\n",
            "        [23,  0,  0],\n",
            "        [24,  0,  0],\n",
            "        [25,  0,  0],\n",
            "        [26,  0,  0],\n",
            "        [27,  0,  0],\n",
            "        [28,  0,  0],\n",
            "        [29,  0,  0],\n",
            "        [30,  0,  0],\n",
            "        [31,  0,  0],\n",
            "        [32,  0,  0],\n",
            "        [33,  0,  0],\n",
            "        [34,  0,  0],\n",
            "        [35,  0,  0],\n",
            "        [36,  0,  0],\n",
            "        [37,  0,  0],\n",
            "        [38,  0,  0],\n",
            "        [39,  0,  0],\n",
            "        [40,  0,  0],\n",
            "        [41,  0,  0],\n",
            "        [42,  0,  0],\n",
            "        [43,  0,  0],\n",
            "        [44,  0,  0],\n",
            "        [45,  0,  0],\n",
            "        [44,  0,  0]], dtype=torch.int16)\n",
            "***************************\n",
            "tensor([[20,  0,  0]], dtype=torch.int16)\n",
            "/*/*/*/*/*/*/*/*/*/*/*/*/*/*\n",
            "tensor([[10.1489,  8.9398,  8.9398,  8.1131,  8.9398,  8.9399]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.0958,  8.9485,  8.9485, 10.1104,  8.9485,  8.9486]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.0958,  8.9485,  8.9485, 10.1104,  8.9485,  8.9486]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.0958,  8.9485,  8.9485, 10.1104,  8.9485,  8.9486]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.0958,  8.9485,  8.9485, 10.1104,  8.9485,  8.9486]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.1118,  8.9429,  8.9429, 10.1303,  8.9430,  8.9430]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.1357,  8.9346,  8.9346, 10.1601,  8.9346,  8.9347]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.1505,  8.9301,  8.9301, 10.1783,  8.9302,  8.9302]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.1904,  8.9210,  8.9210, 10.2270,  8.9210,  8.9210]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.1832,  8.9226,  8.9226, 10.2182,  8.9226,  8.9227]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.2596,  8.9051,  8.9051, 10.3117,  8.9051,  8.9051]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.2667,  8.9034,  8.9034, 10.3202,  8.9034,  8.9034]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.2719,  8.9023,  8.9023, 10.3266,  8.9022,  8.9022]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.2830,  8.8997,  8.8997, 10.3402,  8.8997,  8.8997]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.2688,  8.9029,  8.9029, 10.3228,  8.9029,  8.9029]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.3273,  8.8910,  8.8910, 10.3946,  8.8909,  8.8909]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.3297,  8.8922,  8.8922, 10.3977,  8.8921,  8.8921]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.3322,  8.8934,  8.8934, 10.4009,  8.8934,  8.8933]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.3345,  8.8946,  8.8946, 10.4040,  8.8945,  8.8944]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.3304,  8.8925,  8.8925, 10.3986,  8.8925,  8.8924]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.3202,  8.8912,  8.8912, 10.3856,  8.8911,  8.8911]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.0035,  8.9866,  8.9866,  8.0029,  8.9866,  8.9867]],\n",
            "       device='cuda:0')\n",
            "tensor([[  0,   0,   0],\n",
            "        [  1,   0,   0],\n",
            "        [  0,   0,   0],\n",
            "        [ -1,   0,   0],\n",
            "        [ -2,   0,   0],\n",
            "        [ -3,   0,   0],\n",
            "        [ -4,   0,   0],\n",
            "        [ -5,   0,   0],\n",
            "        [ -6,   0,   0],\n",
            "        [ -7,   0,   0],\n",
            "        [ -8,   0,   0],\n",
            "        [ -9,   0,   0],\n",
            "        [-10,   0,   0],\n",
            "        [-11,   0,   0],\n",
            "        [-12,   0,   0],\n",
            "        [-13,   0,   0],\n",
            "        [-14,   0,   0],\n",
            "        [-15,   0,   0],\n",
            "        [-16,   0,   0],\n",
            "        [-17,   0,   0],\n",
            "        [-18,   0,   0],\n",
            "        [-19,   0,   0],\n",
            "        [-18,   0,   0]], dtype=torch.int16)\n",
            "***************************\n",
            "tensor([[-40,   0,   0]], dtype=torch.int16)\n",
            "/*/*/*/*/*/*/*/*/*/*/*/*/*/*\n",
            "tensor([[ 8.0199,  8.9794,  8.9794, 10.0186,  8.9794,  8.9794]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.0678,  8.9583,  8.9583, 10.0757,  8.9583,  8.9584]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.0697,  8.9576,  8.9576, 10.0781,  8.9576,  8.9577]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.0844,  8.9525,  8.9525, 10.0963,  8.9525,  8.9526]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.1010,  8.9467,  8.9467, 10.1169,  8.9467,  8.9468]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.1119,  8.9429,  8.9429, 10.1305,  8.9429,  8.9430]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.1147,  8.9419,  8.9419, 10.1339,  8.9419,  8.9420]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.1272,  8.9375,  8.9375, 10.1495,  8.9376,  8.9376]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.1216,  8.9395,  8.9395, 10.1426,  8.9395,  8.9396]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.0998,  8.9471,  8.9471, 10.1155,  8.9471,  8.9472]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.1397,  8.9332,  8.9332, 10.1650,  8.9332,  8.9333]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.1573,  8.9286,  8.9286, 10.1865,  8.9286,  8.9287]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.1884,  8.9214,  8.9214, 10.2245,  8.9215,  8.9215]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.2082,  8.9169,  8.9169, 10.2487,  8.9169,  8.9169]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.1835,  8.9226,  8.9226, 10.2186,  8.9226,  8.9226]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.2384,  8.9100,  8.9100, 10.2856,  8.9100,  8.9100]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.2410,  8.9093,  8.9093, 10.2889,  8.9093,  8.9093]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.2689,  8.9029,  8.9029, 10.3230,  8.9029,  8.9029]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.2796,  8.9005,  8.9005, 10.3360,  8.9005,  8.9005]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.3034,  8.8950,  8.8950, 10.3652,  8.8950,  8.8950]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.3134,  8.8927,  8.8927, 10.3773,  8.8927,  8.8927]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.3276,  8.8911,  8.8911, 10.3949,  8.8911,  8.8910]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.3255,  8.8900,  8.8900, 10.3921,  8.8900,  8.8900]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.2861,  8.8990,  8.8990, 10.3440,  8.8990,  8.8990]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.3259,  8.8903,  8.8903, 10.3927,  8.8902,  8.8902]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.2378,  8.9072,  8.9072, 10.2845,  8.9072,  8.9072]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.0035,  8.9866,  8.9866,  8.0029,  8.9866,  8.9867]],\n",
            "       device='cuda:0')\n",
            "tensor([[  0,   0,   0],\n",
            "        [ -1,   0,   0],\n",
            "        [ -2,   0,   0],\n",
            "        [ -3,   0,   0],\n",
            "        [ -4,   0,   0],\n",
            "        [ -5,   0,   0],\n",
            "        [ -6,   0,   0],\n",
            "        [ -7,   0,   0],\n",
            "        [ -8,   0,   0],\n",
            "        [ -9,   0,   0],\n",
            "        [-10,   0,   0],\n",
            "        [-11,   0,   0],\n",
            "        [-12,   0,   0],\n",
            "        [-13,   0,   0],\n",
            "        [-14,   0,   0],\n",
            "        [-15,   0,   0],\n",
            "        [-16,   0,   0],\n",
            "        [-17,   0,   0],\n",
            "        [-18,   0,   0],\n",
            "        [-19,   0,   0],\n",
            "        [-20,   0,   0],\n",
            "        [-21,   0,   0],\n",
            "        [-22,   0,   0],\n",
            "        [-23,   0,   0],\n",
            "        [-24,   0,   0],\n",
            "        [-25,   0,   0],\n",
            "        [-26,   0,   0],\n",
            "        [-25,   0,   0]], dtype=torch.int16)\n",
            "***************************\n",
            "tensor([[20,  0,  0]], dtype=torch.int16)\n",
            "/*/*/*/*/*/*/*/*/*/*/*/*/*/*\n",
            "tensor([[10.1360,  8.9434,  8.9434,  8.1035,  8.9434,  8.9435]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.0908,  8.9561,  8.9561,  8.0698,  8.9561,  8.9562]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.0931,  8.9555,  8.9555,  8.0714,  8.9555,  8.9556]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.1360,  8.9434,  8.9434,  8.1035,  8.9434,  8.9435]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.1471,  8.9403,  8.9403,  8.1118,  8.9403,  8.9404]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.1441,  8.9412,  8.9412,  8.1095,  8.9412,  8.9412]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.1398,  8.9424,  8.9424,  8.1063,  8.9424,  8.9424]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.1537,  8.9385,  8.9385,  8.1167,  8.9385,  8.9385]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.1617,  8.9362,  8.9362,  8.1227,  8.9362,  8.9363]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.2218,  8.9230,  8.9230,  8.1740,  8.9230,  8.9231]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.2382,  8.9197,  8.9197,  8.1885,  8.9197,  8.9198]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.2351,  8.9204,  8.9204,  8.1858,  8.9204,  8.9204]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.2405,  8.9193,  8.9193,  8.1906,  8.9193,  8.9193]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.2844,  8.9104,  8.9104,  8.2294,  8.9104,  8.9104]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.2703,  8.9133,  8.9133,  8.2169,  8.9133,  8.9133]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.2341,  8.9206,  8.9206,  8.1849,  8.9206,  8.9206]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.2298,  8.9214,  8.9214,  8.1811,  8.9214,  8.9215]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.2006,  8.9273,  8.9273,  8.1553,  8.9273,  8.9273]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.2436,  8.9186,  8.9186,  8.1933,  8.9186,  8.9187]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.1462,  8.9406,  8.9406,  8.1111,  8.9406,  8.9406]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.1575,  8.9374,  8.9374,  8.1195,  8.9374,  8.9375]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.0763,  8.9602,  8.9602,  8.0589,  8.9602,  8.9603]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.0251,  8.9764,  8.9764,  8.0202,  8.9764,  8.9765]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.0035,  8.9866,  8.9866,  8.0029,  8.9866,  8.9867]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.1399,  8.9313,  8.9313, 10.1650,  8.9313,  8.9314]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 0,  0,  0],\n",
            "        [ 1,  0,  0],\n",
            "        [ 2,  0,  0],\n",
            "        [ 3,  0,  0],\n",
            "        [ 4,  0,  0],\n",
            "        [ 5,  0,  0],\n",
            "        [ 6,  0,  0],\n",
            "        [ 7,  0,  0],\n",
            "        [ 8,  0,  0],\n",
            "        [ 9,  0,  0],\n",
            "        [10,  0,  0],\n",
            "        [11,  0,  0],\n",
            "        [12,  0,  0],\n",
            "        [13,  0,  0],\n",
            "        [14,  0,  0],\n",
            "        [15,  0,  0],\n",
            "        [16,  0,  0],\n",
            "        [17,  0,  0],\n",
            "        [18,  0,  0],\n",
            "        [19,  0,  0],\n",
            "        [20,  0,  0],\n",
            "        [21,  0,  0],\n",
            "        [22,  0,  0],\n",
            "        [23,  0,  0],\n",
            "        [24,  0,  0],\n",
            "        [23,  0,  0]], dtype=torch.int16)\n",
            "***************************\n",
            "tensor([[20,  0,  0]], dtype=torch.int16)\n",
            "/*/*/*/*/*/*/*/*/*/*/*/*/*/*\n",
            "tensor([[10.1161,  8.9490,  8.9490,  8.0886,  8.9490,  8.9491]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.0415,  8.9675,  8.9675, 10.0432,  8.9675,  8.9676]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.0415,  8.9675,  8.9675, 10.0432,  8.9675,  8.9676]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.0415,  8.9675,  8.9675, 10.0432,  8.9675,  8.9676]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.0415,  8.9675,  8.9675, 10.0432,  8.9675,  8.9676]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.0456,  8.9658,  8.9659, 10.0482,  8.9659,  8.9660]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.0477,  8.9652,  8.9652, 10.0507,  8.9653,  8.9654]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.0468,  8.9655,  8.9655, 10.0496,  8.9655,  8.9656]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.0466,  8.9655,  8.9655, 10.0494,  8.9656,  8.9657]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.0473,  8.9653,  8.9653, 10.0503,  8.9654,  8.9655]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.0479,  8.9652,  8.9652, 10.0510,  8.9652,  8.9653]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.0548,  8.9628,  8.9628, 10.0596,  8.9628,  8.9629]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.0543,  8.9630,  8.9630, 10.0589,  8.9630,  8.9631]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.0506,  8.9642,  8.9643, 10.0544,  8.9643,  8.9644]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.0500,  8.9645,  8.9645, 10.0536,  8.9645,  8.9646]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.0515,  8.9639,  8.9639, 10.0555,  8.9640,  8.9641]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.0614,  8.9605,  8.9605, 10.0678,  8.9605,  8.9606]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.0594,  8.9612,  8.9612, 10.0653,  8.9612,  8.9613]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.0574,  8.9619,  8.9619, 10.0628,  8.9619,  8.9620]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.0516,  8.9639,  8.9639, 10.0556,  8.9639,  8.9640]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.0510,  8.9641,  8.9641, 10.0548,  8.9642,  8.9643]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.0636,  8.9597,  8.9597, 10.0705,  8.9598,  8.9598]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.0707,  8.9572,  8.9572, 10.0793,  8.9573,  8.9574]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.0724,  8.9567,  8.9567, 10.0814,  8.9567,  8.9568]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.0724,  8.9567,  8.9567, 10.0814,  8.9567,  8.9568]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.0736,  8.9563,  8.9563, 10.0829,  8.9563,  8.9564]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.0988,  8.9474,  8.9474, 10.1142,  8.9475,  8.9475]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.1069,  8.9446,  8.9446, 10.1243,  8.9446,  8.9447]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.1148,  8.9419,  8.9419, 10.1341,  8.9419,  8.9420]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.1267,  8.9377,  8.9377, 10.1488,  8.9378,  8.9378]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.1270,  8.9376,  8.9376, 10.1492,  8.9377,  8.9377]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.1359,  8.9345,  8.9345, 10.1603,  8.9345,  8.9346]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.1699,  8.9257,  8.9257, 10.2020,  8.9257,  8.9257]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.1939,  8.9202,  8.9202, 10.2313,  8.9202,  8.9202]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.2003,  8.9187,  8.9187, 10.2391,  8.9187,  8.9187]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.2020,  8.9183,  8.9183, 10.2412,  8.9183,  8.9184]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.2172,  8.9148,  8.9148, 10.2597,  8.9148,  8.9149]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.2382,  8.9100,  8.9100, 10.2855,  8.9100,  8.9100]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.2146,  8.9154,  8.9154, 10.2566,  8.9154,  8.9155]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.2013,  8.9185,  8.9185, 10.2404,  8.9185,  8.9185]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.2493,  8.9074,  8.9074, 10.2990,  8.9074,  8.9074]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.2423,  8.9090,  8.9090, 10.2905,  8.9090,  8.9091]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.2984,  8.8962,  8.8962, 10.3590,  8.8961,  8.8961]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.3274,  8.8910,  8.8910, 10.3947,  8.8910,  8.8909]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.3307,  8.8927,  8.8926, 10.3989,  8.8926,  8.8925]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.3278,  8.8912,  8.8912, 10.3951,  8.8912,  8.8911]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.3308,  8.8927,  8.8927, 10.3991,  8.8927,  8.8926]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.3330,  8.8938,  8.8938, 10.4020,  8.8938,  8.8937]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.3332,  8.8940,  8.8940, 10.4024,  8.8939,  8.8938]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.3402,  8.8975,  8.8975, 10.4115,  8.8974,  8.8972]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.3340,  8.8943,  8.8943, 10.4033,  8.8943,  8.8942]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.3279,  8.8913,  8.8912, 10.3953,  8.8912,  8.8912]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.0035,  8.9866,  8.9866,  8.0029,  8.9866,  8.9867]],\n",
            "       device='cuda:0')\n",
            "tensor([[  0,   0,   0],\n",
            "        [  1,   0,   0],\n",
            "        [  0,   0,   0],\n",
            "        [ -1,   0,   0],\n",
            "        [ -2,   0,   0],\n",
            "        [ -3,   0,   0],\n",
            "        [ -4,   0,   0],\n",
            "        [ -5,   0,   0],\n",
            "        [ -6,   0,   0],\n",
            "        [ -7,   0,   0],\n",
            "        [ -8,   0,   0],\n",
            "        [ -9,   0,   0],\n",
            "        [-10,   0,   0],\n",
            "        [-11,   0,   0],\n",
            "        [-12,   0,   0],\n",
            "        [-13,   0,   0],\n",
            "        [-14,   0,   0],\n",
            "        [-15,   0,   0],\n",
            "        [-16,   0,   0],\n",
            "        [-17,   0,   0],\n",
            "        [-18,   0,   0],\n",
            "        [-19,   0,   0],\n",
            "        [-20,   0,   0],\n",
            "        [-21,   0,   0],\n",
            "        [-22,   0,   0],\n",
            "        [-23,   0,   0],\n",
            "        [-24,   0,   0],\n",
            "        [-25,   0,   0],\n",
            "        [-26,   0,   0],\n",
            "        [-27,   0,   0],\n",
            "        [-28,   0,   0],\n",
            "        [-29,   0,   0],\n",
            "        [-30,   0,   0],\n",
            "        [-31,   0,   0],\n",
            "        [-32,   0,   0],\n",
            "        [-33,   0,   0],\n",
            "        [-34,   0,   0],\n",
            "        [-35,   0,   0],\n",
            "        [-36,   0,   0],\n",
            "        [-37,   0,   0],\n",
            "        [-38,   0,   0],\n",
            "        [-39,   0,   0],\n",
            "        [-40,   0,   0],\n",
            "        [-41,   0,   0],\n",
            "        [-42,   0,   0],\n",
            "        [-43,   0,   0],\n",
            "        [-44,   0,   0],\n",
            "        [-45,   0,   0],\n",
            "        [-46,   0,   0],\n",
            "        [-47,   0,   0],\n",
            "        [-48,   0,   0],\n",
            "        [-49,   0,   0],\n",
            "        [-50,   0,   0],\n",
            "        [-49,   0,   0]], dtype=torch.int16)\n",
            "***************************\n",
            "tensor([[-20,   0,   0]], dtype=torch.int16)\n",
            "/*/*/*/*/*/*/*/*/*/*/*/*/*/*\n",
            "tensor([[ 8.1130,  8.9425,  8.9425, 10.1318,  8.9425,  8.9426]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.1652,  8.9352,  8.9352,  8.1253,  8.9352,  8.9353]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.1652,  8.9352,  8.9352,  8.1253,  8.9352,  8.9353]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.1652,  8.9352,  8.9352,  8.1253,  8.9352,  8.9353]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.1652,  8.9352,  8.9352,  8.1253,  8.9352,  8.9353]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.2124,  8.9249,  8.9249,  8.1657,  8.9249,  8.9249]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.2616,  8.9150,  8.9150,  8.2093,  8.9150,  8.9150]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.2757,  8.9122,  8.9122,  8.2217,  8.9122,  8.9122]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.3016,  8.9070,  8.9070,  8.2446,  8.9070,  8.9070]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.2858,  8.9102,  8.9102,  8.2306,  8.9102,  8.9102]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.3002,  8.9073,  8.9073,  8.2434,  8.9073,  8.9072]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.3052,  8.9063,  8.9063,  8.2478,  8.9063,  8.9063]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.2912,  8.9091,  8.9091,  8.2355,  8.9091,  8.9091]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.2309,  8.9212,  8.9212,  8.1821,  8.9212,  8.9212]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.2884,  8.9096,  8.9096,  8.2329,  8.9096,  8.9096]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.2162,  8.9242,  8.9242,  8.1691,  8.9242,  8.9242]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.0035,  8.9866,  8.9866,  8.0029,  8.9866,  8.9867]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.3261,  8.8903,  8.8903, 10.3929,  8.8903,  8.8903]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 0,  0,  0],\n",
            "        [-1,  0,  0],\n",
            "        [ 0,  0,  0],\n",
            "        [ 1,  0,  0],\n",
            "        [ 2,  0,  0],\n",
            "        [ 3,  0,  0],\n",
            "        [ 4,  0,  0],\n",
            "        [ 5,  0,  0],\n",
            "        [ 6,  0,  0],\n",
            "        [ 7,  0,  0],\n",
            "        [ 8,  0,  0],\n",
            "        [ 9,  0,  0],\n",
            "        [10,  0,  0],\n",
            "        [11,  0,  0],\n",
            "        [12,  0,  0],\n",
            "        [13,  0,  0],\n",
            "        [14,  0,  0],\n",
            "        [15,  0,  0],\n",
            "        [14,  0,  0]], dtype=torch.int16)\n",
            "***************************\n",
            "tensor([[40,  0,  0]], dtype=torch.int16)\n",
            "/*/*/*/*/*/*/*/*/*/*/*/*/*/*\n",
            "tensor([[10.0035,  8.9866,  8.9866,  8.0029,  8.9866,  8.9867]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.0426,  8.9669,  8.9669, 10.0445,  8.9669,  8.9670]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.0426,  8.9669,  8.9669, 10.0445,  8.9669,  8.9670]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.0426,  8.9669,  8.9669, 10.0445,  8.9669,  8.9670]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.0426,  8.9669,  8.9669, 10.0445,  8.9669,  8.9670]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.0487,  8.9649,  8.9649, 10.0520,  8.9650,  8.9651]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.0462,  8.9657,  8.9657, 10.0489,  8.9657,  8.9658]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.0462,  8.9657,  8.9657, 10.0489,  8.9657,  8.9658]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.0516,  8.9639,  8.9639, 10.0556,  8.9640,  8.9641]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.0651,  8.9592,  8.9592, 10.0723,  8.9592,  8.9593]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.1002,  8.9470,  8.9470, 10.1159,  8.9470,  8.9471]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.0982,  8.9477,  8.9477, 10.1135,  8.9477,  8.9478]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.1074,  8.9445,  8.9445, 10.1248,  8.9445,  8.9446]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.1379,  8.9338,  8.9338, 10.1627,  8.9339,  8.9339]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.1539,  8.9294,  8.9294, 10.1824,  8.9294,  8.9294]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.1646,  8.9269,  8.9269, 10.1955,  8.9269,  8.9270]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.1691,  8.9259,  8.9259, 10.2010,  8.9259,  8.9259]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.1841,  8.9224,  8.9224, 10.2193,  8.9224,  8.9225]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.2015,  8.9184,  8.9184, 10.2406,  8.9184,  8.9185]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.2314,  8.9116,  8.9116, 10.2771,  8.9116,  8.9116]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.2611,  8.9047,  8.9047, 10.3134,  8.9047,  8.9047]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.3002,  8.8958,  8.8958, 10.3612,  8.8957,  8.8957]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.3274,  8.8910,  8.8910, 10.3947,  8.8910,  8.8909]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.3304,  8.8925,  8.8925, 10.3986,  8.8925,  8.8924]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.3293,  8.8920,  8.8920, 10.3972,  8.8919,  8.8919]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.3317,  8.8932,  8.8932, 10.4003,  8.8931,  8.8930]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.3295,  8.8921,  8.8921, 10.3975,  8.8920,  8.8920]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.3309,  8.8928,  8.8928, 10.3992,  8.8927,  8.8926]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.3823,  8.8908,  8.8908,  8.3160,  8.8907,  8.8907]],\n",
            "       device='cuda:0')\n",
            "tensor([[  0,   0,   0],\n",
            "        [  1,   0,   0],\n",
            "        [  0,   0,   0],\n",
            "        [ -1,   0,   0],\n",
            "        [ -2,   0,   0],\n",
            "        [ -3,   0,   0],\n",
            "        [ -4,   0,   0],\n",
            "        [ -5,   0,   0],\n",
            "        [ -6,   0,   0],\n",
            "        [ -7,   0,   0],\n",
            "        [ -8,   0,   0],\n",
            "        [ -9,   0,   0],\n",
            "        [-10,   0,   0],\n",
            "        [-11,   0,   0],\n",
            "        [-12,   0,   0],\n",
            "        [-13,   0,   0],\n",
            "        [-14,   0,   0],\n",
            "        [-15,   0,   0],\n",
            "        [-16,   0,   0],\n",
            "        [-17,   0,   0],\n",
            "        [-18,   0,   0],\n",
            "        [-19,   0,   0],\n",
            "        [-20,   0,   0],\n",
            "        [-21,   0,   0],\n",
            "        [-22,   0,   0],\n",
            "        [-23,   0,   0],\n",
            "        [-24,   0,   0],\n",
            "        [-25,   0,   0],\n",
            "        [-26,   0,   0],\n",
            "        [-25,   0,   0]], dtype=torch.int16)\n",
            "***************************\n",
            "tensor([[50,  0,  0]], dtype=torch.int16)\n",
            "/*/*/*/*/*/*/*/*/*/*/*/*/*/*\n",
            "tensor([[10.0035,  8.9866,  8.9866,  8.0029,  8.9866,  8.9867]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.3409,  8.8991,  8.8991,  8.2793,  8.8991,  8.8991]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.3319,  8.9009,  8.9009,  8.2715,  8.9009,  8.9009]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.3156,  8.9042,  8.9042,  8.2570,  8.9042,  8.9041]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.3835,  8.8908,  8.8908,  8.3155,  8.8908,  8.8908]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.3829,  8.8908,  8.8908,  8.3158,  8.8908,  8.8907]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.3841,  8.8909,  8.8909,  8.3151,  8.8909,  8.8908]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.3301,  8.9013,  8.9013,  8.2698,  8.9012,  8.9012]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.3283,  8.9016,  8.9016,  8.2682,  8.9016,  8.9016]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.2680,  8.9137,  8.9137,  8.2149,  8.9137,  8.9137]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.1917,  8.9291,  8.9291,  8.1474,  8.9291,  8.9291]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.1908,  8.9199,  8.9199, 10.2273,  8.9199,  8.9200]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 0,  0,  0],\n",
            "        [ 1,  0,  0],\n",
            "        [ 2,  0,  0],\n",
            "        [ 3,  0,  0],\n",
            "        [ 4,  0,  0],\n",
            "        [ 5,  0,  0],\n",
            "        [ 6,  0,  0],\n",
            "        [ 7,  0,  0],\n",
            "        [ 8,  0,  0],\n",
            "        [ 9,  0,  0],\n",
            "        [10,  0,  0],\n",
            "        [11,  0,  0],\n",
            "        [10,  0,  0]], dtype=torch.int16)\n",
            "***************************\n",
            "tensor([[-50,   0,   0]], dtype=torch.int16)\n",
            "/*/*/*/*/*/*/*/*/*/*/*/*/*/*\n",
            "tensor([[ 8.0199,  8.9794,  8.9794, 10.0186,  8.9794,  8.9794]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.0049,  8.9859,  8.9859,  8.0040,  8.9860,  8.9860]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.0049,  8.9859,  8.9859,  8.0040,  8.9860,  8.9860]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.0049,  8.9859,  8.9859,  8.0040,  8.9860,  8.9860]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.0049,  8.9859,  8.9859,  8.0040,  8.9860,  8.9860]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.0060,  8.9854,  8.9854,  8.0049,  8.9854,  8.9855]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.0045,  8.9861,  8.9861,  8.0037,  8.9862,  8.9862]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.0064,  8.9852,  8.9852,  8.0052,  8.9853,  8.9853]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.0048,  8.9860,  8.9860,  8.0039,  8.9860,  8.9861]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.0043,  8.9863,  8.9863,  8.0035,  8.9863,  8.9863]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.0074,  8.9847,  8.9848,  8.0060,  8.9848,  8.9848]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.0116,  8.9828,  8.9828,  8.0094,  8.9828,  8.9828]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.0151,  8.9811,  8.9811,  8.0122,  8.9811,  8.9812]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.0283,  8.9748,  8.9748,  8.0227,  8.9749,  8.9749]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.0262,  8.9758,  8.9758,  8.0211,  8.9759,  8.9759]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.0381,  8.9709,  8.9709,  8.0304,  8.9710,  8.9710]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.0502,  8.9675,  8.9676,  8.0394,  8.9676,  8.9676]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.0461,  8.9687,  8.9687,  8.0364,  8.9687,  8.9688]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.0659,  8.9631,  8.9631,  8.0511,  8.9632,  8.9632]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.0762,  8.9603,  8.9603,  8.0588,  8.9603,  8.9603]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.1001,  8.9535,  8.9535,  8.0767,  8.9535,  8.9536]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.0849,  8.9578,  8.9578,  8.0653,  8.9578,  8.9579]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.1307,  8.9449,  8.9449,  8.0995,  8.9449,  8.9450]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.1519,  8.9390,  8.9390,  8.1154,  8.9390,  8.9390]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.1630,  8.9358,  8.9358,  8.1237,  8.9359,  8.9359]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.2442,  8.9185,  8.9185,  8.1938,  8.9185,  8.9185]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.2190,  8.9236,  8.9236,  8.1715,  8.9236,  8.9236]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.2176,  8.9239,  8.9239,  8.1703,  8.9239,  8.9239]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.1885,  8.9297,  8.9297,  8.1446,  8.9297,  8.9298]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.1745,  8.9326,  8.9326,  8.1323,  8.9326,  8.9327]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.2619,  8.9150,  8.9150,  8.2095,  8.9150,  8.9150]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.2558,  8.9162,  8.9162,  8.2041,  8.9162,  8.9162]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.2690,  8.9136,  8.9136,  8.2157,  8.9136,  8.9136]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.2307,  8.9212,  8.9212,  8.1819,  8.9213,  8.9213]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.2804,  8.9112,  8.9112,  8.2259,  8.9112,  8.9112]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.2323,  8.9209,  8.9209,  8.1833,  8.9209,  8.9209]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.2643,  8.9145,  8.9145,  8.2116,  8.9145,  8.9145]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.1991,  8.9276,  8.9276,  8.1539,  8.9276,  8.9276]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.2249,  8.9224,  8.9224,  8.1768,  8.9224,  8.9224]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.0035,  8.9866,  8.9866,  8.0029,  8.9866,  8.9867]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.1265,  8.9378,  8.9378, 10.1485,  8.9378,  8.9379]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 0,  0,  0],\n",
            "        [-1,  0,  0],\n",
            "        [ 0,  0,  0],\n",
            "        [ 1,  0,  0],\n",
            "        [ 2,  0,  0],\n",
            "        [ 3,  0,  0],\n",
            "        [ 4,  0,  0],\n",
            "        [ 5,  0,  0],\n",
            "        [ 6,  0,  0],\n",
            "        [ 7,  0,  0],\n",
            "        [ 8,  0,  0],\n",
            "        [ 9,  0,  0],\n",
            "        [10,  0,  0],\n",
            "        [11,  0,  0],\n",
            "        [12,  0,  0],\n",
            "        [13,  0,  0],\n",
            "        [14,  0,  0],\n",
            "        [15,  0,  0],\n",
            "        [16,  0,  0],\n",
            "        [17,  0,  0],\n",
            "        [18,  0,  0],\n",
            "        [19,  0,  0],\n",
            "        [20,  0,  0],\n",
            "        [21,  0,  0],\n",
            "        [22,  0,  0],\n",
            "        [23,  0,  0],\n",
            "        [24,  0,  0],\n",
            "        [25,  0,  0],\n",
            "        [26,  0,  0],\n",
            "        [27,  0,  0],\n",
            "        [28,  0,  0],\n",
            "        [29,  0,  0],\n",
            "        [30,  0,  0],\n",
            "        [31,  0,  0],\n",
            "        [32,  0,  0],\n",
            "        [33,  0,  0],\n",
            "        [34,  0,  0],\n",
            "        [35,  0,  0],\n",
            "        [36,  0,  0],\n",
            "        [37,  0,  0],\n",
            "        [38,  0,  0],\n",
            "        [37,  0,  0]], dtype=torch.int16)\n",
            "***************************\n",
            "tensor([[-20,   0,   0]], dtype=torch.int16)\n",
            "/*/*/*/*/*/*/*/*/*/*/*/*/*/*\n",
            "tensor([[ 8.1160,  8.9415,  8.9415, 10.1355,  8.9415,  8.9416]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.0199,  8.9794,  8.9794, 10.0186,  8.9794,  8.9794]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.0199,  8.9794,  8.9794, 10.0186,  8.9794,  8.9794]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.0434,  8.9665,  8.9665, 10.0454,  8.9666,  8.9667]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.0366,  8.9702,  8.9702, 10.0376,  8.9702,  8.9703]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.0439,  8.9664,  8.9664, 10.0460,  8.9664,  8.9665]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.0457,  8.9658,  8.9658, 10.0482,  8.9659,  8.9660]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.0526,  8.9636,  8.9636, 10.0568,  8.9636,  8.9637]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.0484,  8.9650,  8.9650, 10.0517,  8.9650,  8.9651]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.0686,  8.9580,  8.9580, 10.0767,  8.9580,  8.9581]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.0616,  8.9604,  8.9604, 10.0680,  8.9605,  8.9606]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.0773,  8.9550,  8.9550, 10.0875,  8.9550,  8.9551]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.0953,  8.9487,  8.9487, 10.1099,  8.9487,  8.9488]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.0938,  8.9492,  8.9492, 10.1080,  8.9492,  8.9493]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.0963,  8.9483,  8.9483, 10.1111,  8.9484,  8.9484]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.1085,  8.9441,  8.9441, 10.1262,  8.9441,  8.9442]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.1141,  8.9421,  8.9421, 10.1332,  8.9421,  8.9422]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.1126,  8.9426,  8.9426, 10.1314,  8.9427,  8.9427]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.1160,  8.9415,  8.9415, 10.1355,  8.9415,  8.9416]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.1323,  8.9358,  8.9358, 10.1558,  8.9358,  8.9359]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.1438,  8.9318,  8.9318, 10.1700,  8.9318,  8.9319]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.1658,  8.9266,  8.9266, 10.1970,  8.9266,  8.9267]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.1791,  8.9236,  8.9236, 10.2132,  8.9236,  8.9236]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.2004,  8.9187,  8.9187, 10.2393,  8.9187,  8.9187]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.2516,  8.9069,  8.9069, 10.3018,  8.9069,  8.9069]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.2844,  8.8994,  8.8994, 10.3420,  8.8994,  8.8993]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.3199,  8.8912,  8.8912, 10.3852,  8.8912,  8.8912]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.2972,  8.8964,  8.8964, 10.3575,  8.8964,  8.8964]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.3262,  8.8904,  8.8904, 10.3931,  8.8904,  8.8903]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.3313,  8.8930,  8.8930, 10.3998,  8.8929,  8.8928]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.3315,  8.8931,  8.8931, 10.4001,  8.8931,  8.8930]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.3328,  8.8938,  8.8937, 10.4018,  8.8937,  8.8936]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.3315,  8.8931,  8.8931, 10.4000,  8.8930,  8.8929]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.3323,  8.8935,  8.8935, 10.4012,  8.8935,  8.8933]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.3324,  8.8935,  8.8935, 10.4012,  8.8935,  8.8934]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.3331,  8.8939,  8.8939, 10.4022,  8.8938,  8.8937]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.3314,  8.8930,  8.8930, 10.4000,  8.8930,  8.8929]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.3327,  8.8937,  8.8937, 10.4016,  8.8936,  8.8935]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.0035,  8.9866,  8.9866,  8.0029,  8.9866,  8.9867]],\n",
            "       device='cuda:0')\n",
            "tensor([[  0,   0,   0],\n",
            "        [ -1,   0,   0],\n",
            "        [ -2,   0,   0],\n",
            "        [ -3,   0,   0],\n",
            "        [ -4,   0,   0],\n",
            "        [ -5,   0,   0],\n",
            "        [ -6,   0,   0],\n",
            "        [ -7,   0,   0],\n",
            "        [ -8,   0,   0],\n",
            "        [ -9,   0,   0],\n",
            "        [-10,   0,   0],\n",
            "        [-11,   0,   0],\n",
            "        [-12,   0,   0],\n",
            "        [-13,   0,   0],\n",
            "        [-14,   0,   0],\n",
            "        [-15,   0,   0],\n",
            "        [-16,   0,   0],\n",
            "        [-17,   0,   0],\n",
            "        [-18,   0,   0],\n",
            "        [-19,   0,   0],\n",
            "        [-20,   0,   0],\n",
            "        [-21,   0,   0],\n",
            "        [-22,   0,   0],\n",
            "        [-23,   0,   0],\n",
            "        [-24,   0,   0],\n",
            "        [-25,   0,   0],\n",
            "        [-26,   0,   0],\n",
            "        [-27,   0,   0],\n",
            "        [-28,   0,   0],\n",
            "        [-29,   0,   0],\n",
            "        [-30,   0,   0],\n",
            "        [-31,   0,   0],\n",
            "        [-32,   0,   0],\n",
            "        [-33,   0,   0],\n",
            "        [-34,   0,   0],\n",
            "        [-35,   0,   0],\n",
            "        [-36,   0,   0],\n",
            "        [-37,   0,   0],\n",
            "        [-38,   0,   0],\n",
            "        [-37,   0,   0]], dtype=torch.int16)\n",
            "***************************\n",
            "tensor([[-40,   0,   0]], dtype=torch.int16)\n",
            "/*/*/*/*/*/*/*/*/*/*/*/*/*/*\n",
            "tensor([[ 8.0199,  8.9794,  8.9794, 10.0186,  8.9794,  8.9794]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.0399,  8.9684,  8.9684, 10.0413,  8.9684,  8.9685]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.0386,  8.9691,  8.9691, 10.0399,  8.9691,  8.9692]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.0277,  8.9751,  8.9751, 10.0275,  8.9751,  8.9751]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.0450,  8.9661,  8.9661, 10.0473,  8.9661,  8.9662]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.0487,  8.9649,  8.9649, 10.0520,  8.9650,  8.9651]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.0782,  8.9546,  8.9546, 10.0886,  8.9547,  8.9548]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.0878,  8.9513,  8.9513, 10.1006,  8.9513,  8.9514]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.0597,  8.9611,  8.9611, 10.0657,  8.9611,  8.9612]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.0446,  8.9662,  8.9662, 10.0469,  8.9662,  8.9663]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.0670,  8.9585,  8.9585, 10.0747,  8.9586,  8.9587]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.0782,  8.9546,  8.9546, 10.0887,  8.9547,  8.9547]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.0544,  8.9629,  8.9630, 10.0590,  8.9630,  8.9631]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.0609,  8.9607,  8.9607, 10.0671,  8.9607,  8.9608]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.0618,  8.9603,  8.9603, 10.0683,  8.9604,  8.9605]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.0575,  8.9618,  8.9618, 10.0629,  8.9619,  8.9620]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.0536,  8.9632,  8.9632, 10.0581,  8.9632,  8.9633]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.1175,  8.9409,  8.9409, 10.1374,  8.9410,  8.9410]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.0795,  8.9542,  8.9542, 10.0902,  8.9542,  8.9543]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.0668,  8.9586,  8.9586, 10.0745,  8.9586,  8.9587]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.1417,  8.9325,  8.9325, 10.1674,  8.9325,  8.9326]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.1201,  8.9400,  8.9400, 10.1406,  8.9401,  8.9401]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.1529,  8.9296,  8.9296, 10.1812,  8.9296,  8.9297]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.1243,  8.9386,  8.9386, 10.1459,  8.9386,  8.9387]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.1445,  8.9315,  8.9315, 10.1709,  8.9316,  8.9316]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.1409,  8.9328,  8.9328, 10.1665,  8.9328,  8.9329]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.0820,  8.9533,  8.9533, 10.0934,  8.9533,  8.9534]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.1993,  8.9190,  8.9190, 10.2379,  8.9190,  8.9191]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.0035,  8.9866,  8.9866,  8.0029,  8.9866,  8.9867]],\n",
            "       device='cuda:0')\n",
            "tensor([[  0,   0,   0],\n",
            "        [ -1,   0,   0],\n",
            "        [ -2,   0,   0],\n",
            "        [ -3,   0,   0],\n",
            "        [ -4,   0,   0],\n",
            "        [ -5,   0,   0],\n",
            "        [ -6,   0,   0],\n",
            "        [ -7,   0,   0],\n",
            "        [ -8,   0,   0],\n",
            "        [ -9,   0,   0],\n",
            "        [-10,   0,   0],\n",
            "        [-11,   0,   0],\n",
            "        [-12,   0,   0],\n",
            "        [-13,   0,   0],\n",
            "        [-14,   0,   0],\n",
            "        [-15,   0,   0],\n",
            "        [-16,   0,   0],\n",
            "        [-17,   0,   0],\n",
            "        [-18,   0,   0],\n",
            "        [-19,   0,   0],\n",
            "        [-20,   0,   0],\n",
            "        [-21,   0,   0],\n",
            "        [-22,   0,   0],\n",
            "        [-23,   0,   0],\n",
            "        [-24,   0,   0],\n",
            "        [-25,   0,   0],\n",
            "        [-26,   0,   0],\n",
            "        [-27,   0,   0],\n",
            "        [-28,   0,   0],\n",
            "        [-27,   0,   0]], dtype=torch.int16)\n",
            "***************************\n",
            "tensor([[50,  0,  0]], dtype=torch.int16)\n",
            "/*/*/*/*/*/*/*/*/*/*/*/*/*/*\n",
            "tensor([[10.0035,  8.9866,  8.9866,  8.0029,  8.9866,  8.9867]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.0487,  8.9649,  8.9649, 10.0520,  8.9649,  8.9650]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.0487,  8.9649,  8.9649, 10.0520,  8.9649,  8.9650]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.0487,  8.9649,  8.9649, 10.0520,  8.9649,  8.9650]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.0487,  8.9649,  8.9649, 10.0520,  8.9649,  8.9650]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.0527,  8.9635,  8.9635, 10.0570,  8.9635,  8.9636]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.0803,  8.9539,  8.9539, 10.0912,  8.9539,  8.9540]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.0925,  8.9497,  8.9497, 10.1063,  8.9497,  8.9498]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.0989,  8.9474,  8.9474, 10.1143,  8.9475,  8.9475]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.1168,  8.9412,  8.9412, 10.1365,  8.9412,  8.9413]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.1445,  8.9315,  8.9315, 10.1710,  8.9315,  8.9316]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.1956,  8.9198,  8.9198, 10.2334,  8.9198,  8.9198]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.1809,  8.9232,  8.9232, 10.2154,  8.9232,  8.9232]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.2111,  8.9162,  8.9162, 10.2523,  8.9162,  8.9162]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.1995,  8.9189,  8.9189, 10.2381,  8.9189,  8.9189]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.2103,  8.9164,  8.9164, 10.2513,  8.9164,  8.9164]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.2218,  8.9138,  8.9138, 10.2654,  8.9138,  8.9138]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.2285,  8.9122,  8.9122, 10.2736,  8.9122,  8.9122]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.2566,  8.9058,  8.9058, 10.3079,  8.9058,  8.9058]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.2926,  8.8975,  8.8975, 10.3519,  8.8975,  8.8975]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.3027,  8.8952,  8.8952, 10.3643,  8.8952,  8.8951]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.3189,  8.8914,  8.8914, 10.3841,  8.8914,  8.8914]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.2937,  8.8972,  8.8972, 10.3533,  8.8972,  8.8972]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.3274,  8.8910,  8.8910, 10.3947,  8.8910,  8.8909]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.3310,  8.8928,  8.8928, 10.3994,  8.8928,  8.8927]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.3306,  8.8926,  8.8926, 10.3989,  8.8926,  8.8925]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.3282,  8.8914,  8.8914, 10.3958,  8.8914,  8.8913]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.2839,  8.8995,  8.8995, 10.3413,  8.8995,  8.8995]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.0035,  8.9866,  8.9866,  8.0029,  8.9866,  8.9867]],\n",
            "       device='cuda:0')\n",
            "tensor([[  0,   0,   0],\n",
            "        [  1,   0,   0],\n",
            "        [  0,   0,   0],\n",
            "        [ -1,   0,   0],\n",
            "        [ -2,   0,   0],\n",
            "        [ -3,   0,   0],\n",
            "        [ -4,   0,   0],\n",
            "        [ -5,   0,   0],\n",
            "        [ -6,   0,   0],\n",
            "        [ -7,   0,   0],\n",
            "        [ -8,   0,   0],\n",
            "        [ -9,   0,   0],\n",
            "        [-10,   0,   0],\n",
            "        [-11,   0,   0],\n",
            "        [-12,   0,   0],\n",
            "        [-13,   0,   0],\n",
            "        [-14,   0,   0],\n",
            "        [-15,   0,   0],\n",
            "        [-16,   0,   0],\n",
            "        [-17,   0,   0],\n",
            "        [-18,   0,   0],\n",
            "        [-19,   0,   0],\n",
            "        [-20,   0,   0],\n",
            "        [-21,   0,   0],\n",
            "        [-22,   0,   0],\n",
            "        [-23,   0,   0],\n",
            "        [-24,   0,   0],\n",
            "        [-25,   0,   0],\n",
            "        [-26,   0,   0],\n",
            "        [-25,   0,   0]], dtype=torch.int16)\n",
            "***************************\n",
            "tensor([[50,  0,  0]], dtype=torch.int16)\n",
            "/*/*/*/*/*/*/*/*/*/*/*/*/*/*\n",
            "tensor([[10.0035,  8.9866,  8.9866,  8.0029,  8.9866,  8.9867]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.0035,  8.9866,  8.9866,  8.0029,  8.9866,  8.9867]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.0035,  8.9866,  8.9866,  8.0029,  8.9866,  8.9867]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.0035,  8.9866,  8.9866,  8.0029,  8.9866,  8.9867]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.0035,  8.9866,  8.9866,  8.0029,  8.9866,  8.9867]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.0035,  8.9866,  8.9866,  8.0029,  8.9866,  8.9867]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.0035,  8.9866,  8.9866,  8.0029,  8.9866,  8.9867]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.0035,  8.9866,  8.9866,  8.0029,  8.9866,  8.9867]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.0035,  8.9866,  8.9866,  8.0029,  8.9866,  8.9867]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.0049,  8.9860,  8.9860,  8.0040,  8.9860,  8.9861]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.0067,  8.9851,  8.9851,  8.0054,  8.9851,  8.9852]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.0223,  8.9777,  8.9777,  8.0179,  8.9777,  8.9778]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.0342,  8.9721,  8.9721,  8.0274,  8.9721,  8.9722]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.0345,  8.9720,  8.9720,  8.0277,  8.9720,  8.9720]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.0334,  8.9724,  8.9724,  8.0268,  8.9725,  8.9725]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.0169,  8.9803,  8.9803,  8.0136,  8.9803,  8.9804]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.0249,  8.9765,  8.9765,  8.0200,  8.9765,  8.9766]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.0455,  8.9689,  8.9689,  8.0359,  8.9689,  8.9689]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.0579,  8.9654,  8.9654,  8.0451,  8.9654,  8.9655]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.0577,  8.9655,  8.9655,  8.0450,  8.9655,  8.9655]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.0662,  8.9631,  8.9631,  8.0513,  8.9631,  8.9631]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.0648,  8.9634,  8.9634,  8.0503,  8.9635,  8.9635]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.0922,  8.9557,  8.9557,  8.0708,  8.9558,  8.9558]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.0644,  8.9636,  8.9636,  8.0500,  8.9636,  8.9636]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.0824,  8.9585,  8.9585,  8.0635,  8.9585,  8.9586]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.1135,  8.9497,  8.9498,  8.0867,  8.9498,  8.9498]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.1267,  8.9461,  8.9461,  8.0965,  8.9461,  8.9461]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.1381,  8.9428,  8.9428,  8.1051,  8.9429,  8.9429]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.1150,  8.9493,  8.9493,  8.0878,  8.9494,  8.9494]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.1661,  8.9350,  8.9350,  8.1260,  8.9350,  8.9350]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.1514,  8.9391,  8.9391,  8.1150,  8.9391,  8.9391]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.2065,  8.9261,  8.9261,  8.1605,  8.9261,  8.9261]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.2196,  8.9235,  8.9235,  8.1721,  8.9235,  8.9235]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.2189,  8.9236,  8.9236,  8.1714,  8.9236,  8.9237]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.2227,  8.9229,  8.9229,  8.1748,  8.9229,  8.9229]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.1747,  8.9326,  8.9326,  8.1324,  8.9326,  8.9326]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.2206,  8.9233,  8.9233,  8.1730,  8.9233,  8.9233]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.2393,  8.9195,  8.9195,  8.1895,  8.9195,  8.9195]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.2613,  8.9151,  8.9151,  8.2090,  8.9151,  8.9151]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.3034,  8.9066,  8.9066,  8.2462,  8.9066,  8.9066]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.2182,  8.9238,  8.9238,  8.1708,  8.9238,  8.9238]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.2094,  8.9255,  8.9255,  8.1630,  8.9255,  8.9256]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.1930,  8.9288,  8.9288,  8.1485,  8.9288,  8.9289]],\n",
            "       device='cuda:0')\n",
            "tensor([[10.0035,  8.9866,  8.9866,  8.0029,  8.9866,  8.9867]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 8.1715,  8.9253,  8.9253, 10.2040,  8.9253,  8.9254]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 0,  0,  0],\n",
            "        [ 1,  0,  0],\n",
            "        [ 2,  0,  0],\n",
            "        [ 3,  0,  0],\n",
            "        [ 4,  0,  0],\n",
            "        [ 5,  0,  0],\n",
            "        [ 6,  0,  0],\n",
            "        [ 7,  0,  0],\n",
            "        [ 8,  0,  0],\n",
            "        [ 9,  0,  0],\n",
            "        [10,  0,  0],\n",
            "        [11,  0,  0],\n",
            "        [12,  0,  0],\n",
            "        [13,  0,  0],\n",
            "        [14,  0,  0],\n",
            "        [15,  0,  0],\n",
            "        [16,  0,  0],\n",
            "        [17,  0,  0],\n",
            "        [18,  0,  0],\n",
            "        [19,  0,  0],\n",
            "        [20,  0,  0],\n",
            "        [21,  0,  0],\n",
            "        [22,  0,  0],\n",
            "        [23,  0,  0],\n",
            "        [24,  0,  0],\n",
            "        [25,  0,  0],\n",
            "        [26,  0,  0],\n",
            "        [27,  0,  0],\n",
            "        [28,  0,  0],\n",
            "        [29,  0,  0],\n",
            "        [30,  0,  0],\n",
            "        [31,  0,  0],\n",
            "        [32,  0,  0],\n",
            "        [33,  0,  0],\n",
            "        [34,  0,  0],\n",
            "        [35,  0,  0],\n",
            "        [36,  0,  0],\n",
            "        [37,  0,  0],\n",
            "        [38,  0,  0],\n",
            "        [39,  0,  0],\n",
            "        [40,  0,  0],\n",
            "        [41,  0,  0],\n",
            "        [42,  0,  0],\n",
            "        [43,  0,  0],\n",
            "        [44,  0,  0],\n",
            "        [43,  0,  0]], dtype=torch.int16)\n",
            "accuracy 0.0%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "xABwAh5y_IFH"
      },
      "source": [
        "def display(img, perimeters, slice_number):\n",
        "    plt.figure(figsize=(16, 9))\n",
        "    plt.plot(perimeters[:, 1], perimeters[:, 0], '-r', linewidth=1)\n",
        "    plt.imshow(img[slice_number, ...], cmap='gray')"
      ],
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "8yEDFmD6_IFO"
      },
      "source": [
        "from ipywidgets import interact, interactive, fixed, interact_manual\n",
        "import ipywidgets as widgets\n",
        "import matplotlib.backends.backend_pdf\n",
        "\n",
        "pdf = matplotlib.backends.backend_pdf.PdfPages(\"/content/output.pdf\")\n",
        "library = np.concatenate(list_of_examples)\n",
        "perimeters_thresh = [x.shape[0] for x in list_of_examples]\n",
        "cum_thresh = np.cumsum(perimeters_thresh)\n",
        "j = 0\n",
        "i = 0\n",
        "while i < library.shape[0]:\n",
        "  if i > cum_thresh[j]:\n",
        "    j += 1\n",
        "  perimeter = perimeters[j]\n",
        "  fig, ax = plt.subplots(1, 1, figsize=(8, 8))\n",
        "  ax.plot(perimeter[:, 1], perimeter[:, 0], '-r', linewidth=1)\n",
        "  ax.imshow(library[i], cmap='gray')\n",
        "  i += 1\n",
        "  pdf.savefig(fig, bbox_inches='tight')\n",
        "  plt.close(fig)\n",
        "pdf.close()\n",
        "\n",
        "#library = np.stack(list_of_examples)\n",
        "#for i in range(10):\n",
        "#    interact(display, img=fixed(library[i, ...]), perimeters=fixed(perimeters[i]), slice_number=widgets.IntSlider(min=0, max=library[i, ...].shape[0] - 1, step=1, value=0))"
      ],
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "rGLa1WKZ_IFV",
        "outputId": "0cd87709-d1bd-4c2b-eed6-5d2af8c06ce6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        }
      },
      "source": [
        "print(Y_test)"
      ],
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-145-929c682a3812>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'Y_test' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "xnZyJ47Y_IFZ"
      },
      "source": [
        "from torchvision.transforms.functional import normalize\n",
        "\n",
        "correct = 0\n",
        "total = test_size\n",
        "number_of_steps = 200\n",
        "list_of_examples = []\n",
        "perimeters = []\n",
        "with torch.no_grad():\n",
        "    for data in testing_generator:\n",
        "        list_of_frames = []\n",
        "        print('***************************')\n",
        "        x = data[0].to(device)\n",
        "        y = data[1]\n",
        "        first_image = data[2]\n",
        "        #second_image = data[3].numpy()\n",
        "        #second_image = to_pil_image(data[3].int())\n",
        "        root_image = to_pil_image(data[3].int())\n",
        "        second_translation = data[4]\n",
        "        Tt = torch.zeros((number_of_steps, 3), dtype=torch.int16)\n",
        "        print(y)\n",
        "        print('/*/*/*/*/*/*/*/*/*/*/*/*/*/*')\n",
        "        j = 0\n",
        "        perimeters.append(get_perimeter(first_image[0, ...].numpy()))\n",
        "        while j < number_of_steps - 1:\n",
        "            out = policy_net(x.float())\n",
        "            action = torch.argmax(out, dim=1)\n",
        "            Tt[j + 1, :] = q_value.take_action(action, Tt[j, :])\n",
        "            if j > 0 and torch.all(Tt[j + 1, :].eq(Tt[j - 1, :])) :\n",
        "                if torch.dist(Tt[j + 1, :].float(), y).item() <= 1:\n",
        "                    correct += 1\n",
        "                break\n",
        "            translation = (Tt[j + 1, 1].item() + second_translation[0, 0].item(), Tt[j + 1, 2].item() + second_translation[0, 1].item())\n",
        "            #translated = translate_image(second_image[0, ...], Tt_new[1].item(), Tt_new[2].item())\n",
        "            #current_image = rotate_image(translated, Tt_new[0].item(), (second_position[0, 0] + Tt_new[1].item(), second_position[0, 1] + Tt_new[2].item()))\n",
        "            current_image = to_tensor(affine(root_image, angle=Tt[j + 1, 0].item(), translate=translation, scale=1, shear=0))\n",
        "            list_of_frames.append(current_image.numpy().reshape((50, 50)))\n",
        "            diff = first_image - current_image\n",
        "            diff /= 255\n",
        "            x = diff.unsqueeze(0).to(device)\n",
        "            j += 1\n",
        "        stack_of_frames = np.stack(list_of_frames)\n",
        "        stack_of_frames = np.pad(stack_of_frames, ((0, number_of_steps - stack_of_frames.shape[0]), (0, 0), (0, 0)))\n",
        "        list_of_examples.append(stack_of_frames)\n",
        "        Tt = Tt[~Tt.eq(torch.zeros((number_of_steps, 3))).all(dim=1)]\n",
        "        print(Tt)\n",
        "    print('accuracy {}%'.format(100 * correct / total))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "NF0X-zqF_IFe"
      },
      "source": [
        "library = np.stack(list_of_examples)\n",
        "for i in range(test_size):\n",
        "    interact(display, img=fixed(library[i, ...]), perimeters=fixed(perimeters[i]), slice_number=widgets.IntSlider(min=0, max=library[i, ...].shape[0] - 1, step=1, value=0))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}